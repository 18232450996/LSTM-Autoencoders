{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 5\n",
    "        self.step_num = 100\n",
    "        self.input_root =\"/data/home/bli/data/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"/data/home/bli/models/smtp/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"/data/home/bli/models/smtp/log.txt\"\n",
    "        self.training_set_size = 100*200\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 19100 normal windows and 900 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:95,vn1:57,vn2:19,tn:20,va:4,ta:5\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.112, Val_loss:0.056\n",
      "Epoch 1: Loss:0.053, Val_loss:0.038\n",
      "Epoch 2: Loss:0.036, Val_loss:0.029\n",
      "Epoch 3: Loss:0.026, Val_loss:0.024\n",
      "Epoch 4: Loss:0.020, Val_loss:0.022\n",
      "Epoch 5: Loss:0.018, Val_loss:0.021\n",
      "Epoch 6: Loss:0.017, Val_loss:0.020\n",
      "Epoch 7: Loss:0.016, Val_loss:0.020\n",
      "Epoch 8: Loss:0.016, Val_loss:0.019\n",
      "Epoch 9: Loss:0.016, Val_loss:0.019\n",
      "Epoch 10: Loss:0.016, Val_loss:0.019\n",
      "Epoch 11: Loss:0.015, Val_loss:0.018\n",
      "Epoch 12: Loss:0.015, Val_loss:0.018\n",
      "Epoch 13: Loss:0.015, Val_loss:0.018\n",
      "Epoch 14: Loss:0.015, Val_loss:0.018\n",
      "Epoch 15: Loss:0.015, Val_loss:0.018\n",
      "Epoch 16: Loss:0.015, Val_loss:0.017\n",
      "Epoch 17: Loss:0.015, Val_loss:0.017\n",
      "Epoch 18: Loss:0.015, Val_loss:0.017\n",
      "Epoch 19: Loss:0.015, Val_loss:0.017\n",
      "Epoch 20: Loss:0.015, Val_loss:0.017\n",
      "Epoch 21: Loss:0.014, Val_loss:0.017\n",
      "Epoch 22: Loss:0.014, Val_loss:0.017\n",
      "Epoch 23: Loss:0.014, Val_loss:0.017\n",
      "Epoch 24: Loss:0.014, Val_loss:0.016\n",
      "Epoch 25: Loss:0.014, Val_loss:0.016\n",
      "Epoch 26: Loss:0.014, Val_loss:0.016\n",
      "Epoch 27: Loss:0.014, Val_loss:0.016\n",
      "Epoch 28: Loss:0.014, Val_loss:0.016\n",
      "Epoch 29: Loss:0.014, Val_loss:0.016\n",
      "Epoch 30: Loss:0.014, Val_loss:0.016\n",
      "Epoch 31: Loss:0.014, Val_loss:0.016\n",
      "Epoch 32: Loss:0.013, Val_loss:0.015\n",
      "Epoch 33: Loss:0.013, Val_loss:0.015\n",
      "Epoch 34: Loss:0.013, Val_loss:0.015\n",
      "Epoch 35: Loss:0.013, Val_loss:0.015\n",
      "Epoch 36: Loss:0.013, Val_loss:0.015\n",
      "Epoch 37: Loss:0.013, Val_loss:0.015\n",
      "Epoch 38: Loss:0.013, Val_loss:0.015\n",
      "Epoch 39: Loss:0.013, Val_loss:0.015\n",
      "Epoch 40: Loss:0.013, Val_loss:0.015\n",
      "Epoch 41: Loss:0.013, Val_loss:0.015\n",
      "Epoch 42: Loss:0.013, Val_loss:0.014\n",
      "Epoch 43: Loss:0.013, Val_loss:0.014\n",
      "Epoch 44: Loss:0.013, Val_loss:0.014\n",
      "Epoch 45: Loss:0.013, Val_loss:0.014\n",
      "Epoch 46: Loss:0.012, Val_loss:0.014\n",
      "Epoch 47: Loss:0.012, Val_loss:0.014\n",
      "Epoch 48: Loss:0.012, Val_loss:0.014\n",
      "Epoch 49: Loss:0.012, Val_loss:0.013\n",
      "Epoch 50: Loss:0.012, Val_loss:0.013\n",
      "Epoch 51: Loss:0.012, Val_loss:0.013\n",
      "Epoch 52: Loss:0.012, Val_loss:0.013\n",
      "Epoch 53: Loss:0.012, Val_loss:0.013\n",
      "Epoch 54: Loss:0.012, Val_loss:0.012\n",
      "Epoch 55: Loss:0.011, Val_loss:0.012\n",
      "Epoch 56: Loss:0.011, Val_loss:0.012\n",
      "Epoch 57: Loss:0.011, Val_loss:0.012\n",
      "Epoch 58: Loss:0.011, Val_loss:0.012\n",
      "Epoch 59: Loss:0.011, Val_loss:0.012\n",
      "Epoch 60: Loss:0.011, Val_loss:0.012\n",
      "Epoch 61: Loss:0.011, Val_loss:0.011\n",
      "Epoch 62: Loss:0.011, Val_loss:0.011\n",
      "Epoch 63: Loss:0.011, Val_loss:0.011\n",
      "Epoch 64: Loss:0.011, Val_loss:0.011\n",
      "Epoch 65: Loss:0.011, Val_loss:0.011\n",
      "Epoch 66: Loss:0.011, Val_loss:0.011\n",
      "Epoch 67: Loss:0.011, Val_loss:0.011\n",
      "Epoch 68: Loss:0.011, Val_loss:0.011\n",
      "Epoch 69: Loss:0.011, Val_loss:0.011\n",
      "Epoch 70: Loss:0.011, Val_loss:0.011\n",
      "Epoch 71: Loss:0.011, Val_loss:0.011\n",
      "Epoch 72: Loss:0.011, Val_loss:0.011\n",
      "Epoch 73: Loss:0.011, Val_loss:0.011\n",
      "Epoch 74: Loss:0.011, Val_loss:0.011\n",
      "Epoch 75: Loss:0.011, Val_loss:0.011\n",
      "Epoch 76: Loss:0.011, Val_loss:0.011\n",
      "Epoch 77: Loss:0.011, Val_loss:0.011\n",
      "Epoch 78: Loss:0.011, Val_loss:0.011\n",
      "Epoch 79: Loss:0.011, Val_loss:0.011\n",
      "Epoch 80: Loss:0.011, Val_loss:0.011\n",
      "Early stopping at epoch 80\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2d6bebb898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "4\n",
      "Threshold:  0.06778919785521467\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: /data/home/bli/models/smtp/_1_5_100_para.ckpt\n",
      "--- Initialization time: 1198.876852273941 seconds ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f0cbae703b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mEncDecAD_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-66e995eb3a33>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data_source)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Early stopping at epoch %d\\n\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved accompany with parameters and threshold in file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Initialization time: %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
