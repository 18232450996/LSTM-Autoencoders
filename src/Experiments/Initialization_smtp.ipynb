{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_sfs/\"#8_15_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_sfs/log.txt\"\n",
    "        self.training_set_size = self.step_num*6000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 100:\n",
    "                    break\n",
    "\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 6290 normal windows and 260 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:310,vn1:186,vn2:62,tn:71,va:13,ta:13\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.123, Val_loss:0.059\n",
      "Epoch 1: Loss:0.058, Val_loss:0.039\n",
      "Epoch 2: Loss:0.042, Val_loss:0.029\n",
      "Epoch 3: Loss:0.032, Val_loss:0.023\n",
      "Epoch 4: Loss:0.027, Val_loss:0.020\n",
      "Epoch 5: Loss:0.023, Val_loss:0.018\n",
      "Epoch 6: Loss:0.021, Val_loss:0.017\n",
      "Epoch 7: Loss:0.020, Val_loss:0.016\n",
      "Epoch 8: Loss:0.019, Val_loss:0.015\n",
      "Epoch 9: Loss:0.018, Val_loss:0.015\n",
      "Epoch 10: Loss:0.017, Val_loss:0.014\n",
      "Epoch 11: Loss:0.017, Val_loss:0.014\n",
      "Epoch 12: Loss:0.016, Val_loss:0.014\n",
      "Epoch 13: Loss:0.016, Val_loss:0.014\n",
      "Epoch 14: Loss:0.016, Val_loss:0.014\n",
      "Epoch 15: Loss:0.015, Val_loss:0.013\n",
      "Epoch 16: Loss:0.015, Val_loss:0.013\n",
      "Epoch 17: Loss:0.015, Val_loss:0.013\n",
      "Epoch 18: Loss:0.015, Val_loss:0.013\n",
      "Epoch 19: Loss:0.014, Val_loss:0.013\n",
      "Epoch 20: Loss:0.014, Val_loss:0.013\n",
      "Epoch 21: Loss:0.014, Val_loss:0.012\n",
      "Epoch 22: Loss:0.014, Val_loss:0.012\n",
      "Epoch 23: Loss:0.014, Val_loss:0.012\n",
      "Epoch 24: Loss:0.014, Val_loss:0.012\n",
      "Epoch 25: Loss:0.013, Val_loss:0.012\n",
      "Epoch 26: Loss:0.013, Val_loss:0.012\n",
      "Epoch 27: Loss:0.013, Val_loss:0.012\n",
      "Epoch 28: Loss:0.013, Val_loss:0.012\n",
      "Epoch 29: Loss:0.013, Val_loss:0.012\n",
      "Epoch 30: Loss:0.013, Val_loss:0.012\n",
      "Epoch 31: Loss:0.013, Val_loss:0.012\n",
      "Epoch 32: Loss:0.013, Val_loss:0.011\n",
      "Epoch 33: Loss:0.012, Val_loss:0.011\n",
      "Epoch 34: Loss:0.012, Val_loss:0.011\n",
      "Epoch 35: Loss:0.012, Val_loss:0.011\n",
      "Epoch 36: Loss:0.012, Val_loss:0.011\n",
      "Epoch 37: Loss:0.012, Val_loss:0.011\n",
      "Epoch 38: Loss:0.012, Val_loss:0.011\n",
      "Epoch 39: Loss:0.012, Val_loss:0.011\n",
      "Epoch 40: Loss:0.012, Val_loss:0.011\n",
      "Epoch 41: Loss:0.012, Val_loss:0.011\n",
      "Epoch 42: Loss:0.012, Val_loss:0.011\n",
      "Epoch 43: Loss:0.011, Val_loss:0.011\n",
      "Epoch 44: Loss:0.011, Val_loss:0.011\n",
      "Epoch 45: Loss:0.011, Val_loss:0.011\n",
      "Epoch 46: Loss:0.011, Val_loss:0.011\n",
      "Epoch 47: Loss:0.011, Val_loss:0.010\n",
      "Epoch 48: Loss:0.011, Val_loss:0.010\n",
      "Epoch 49: Loss:0.011, Val_loss:0.010\n",
      "Epoch 50: Loss:0.011, Val_loss:0.010\n",
      "Epoch 51: Loss:0.011, Val_loss:0.010\n",
      "Epoch 52: Loss:0.011, Val_loss:0.010\n",
      "Epoch 53: Loss:0.011, Val_loss:0.010\n",
      "Epoch 54: Loss:0.010, Val_loss:0.010\n",
      "Epoch 55: Loss:0.010, Val_loss:0.010\n",
      "Epoch 56: Loss:0.010, Val_loss:0.010\n",
      "Epoch 57: Loss:0.010, Val_loss:0.010\n",
      "Epoch 58: Loss:0.010, Val_loss:0.010\n",
      "Epoch 59: Loss:0.010, Val_loss:0.010\n",
      "Epoch 60: Loss:0.010, Val_loss:0.010\n",
      "Epoch 61: Loss:0.010, Val_loss:0.010\n",
      "Epoch 62: Loss:0.010, Val_loss:0.010\n",
      "Epoch 63: Loss:0.010, Val_loss:0.010\n",
      "Epoch 64: Loss:0.010, Val_loss:0.010\n",
      "Epoch 65: Loss:0.010, Val_loss:0.010\n",
      "Epoch 66: Loss:0.010, Val_loss:0.010\n",
      "Epoch 67: Loss:0.010, Val_loss:0.010\n",
      "Epoch 68: Loss:0.010, Val_loss:0.010\n",
      "Epoch 69: Loss:0.010, Val_loss:0.010\n",
      "Epoch 70: Loss:0.010, Val_loss:0.010\n",
      "Epoch 71: Loss:0.010, Val_loss:0.010\n",
      "Epoch 72: Loss:0.010, Val_loss:0.010\n",
      "Epoch 73: Loss:0.010, Val_loss:0.010\n",
      "Epoch 74: Loss:0.010, Val_loss:0.010\n",
      "Epoch 75: Loss:0.010, Val_loss:0.010\n",
      "Epoch 76: Loss:0.009, Val_loss:0.010\n",
      "Epoch 77: Loss:0.009, Val_loss:0.010\n",
      "Epoch 78: Loss:0.009, Val_loss:0.010\n",
      "Epoch 79: Loss:0.009, Val_loss:0.009\n",
      "Epoch 80: Loss:0.009, Val_loss:0.009\n",
      "Epoch 81: Loss:0.009, Val_loss:0.009\n",
      "Epoch 82: Loss:0.009, Val_loss:0.009\n",
      "Epoch 83: Loss:0.009, Val_loss:0.009\n",
      "Epoch 84: Loss:0.009, Val_loss:0.009\n",
      "Epoch 85: Loss:0.009, Val_loss:0.009\n",
      "Epoch 86: Loss:0.009, Val_loss:0.009\n",
      "Epoch 87: Loss:0.009, Val_loss:0.009\n",
      "Epoch 88: Loss:0.009, Val_loss:0.009\n",
      "Epoch 89: Loss:0.009, Val_loss:0.009\n",
      "Epoch 90: Loss:0.009, Val_loss:0.009\n",
      "Epoch 91: Loss:0.009, Val_loss:0.009\n",
      "Epoch 92: Loss:0.009, Val_loss:0.009\n",
      "Epoch 93: Loss:0.009, Val_loss:0.009\n",
      "Epoch 94: Loss:0.009, Val_loss:0.009\n",
      "Epoch 95: Loss:0.009, Val_loss:0.009\n",
      "Epoch 96: Loss:0.009, Val_loss:0.009\n",
      "Epoch 97: Loss:0.009, Val_loss:0.009\n",
      "Epoch 98: Loss:0.009, Val_loss:0.009\n",
      "Epoch 99: Loss:0.009, Val_loss:0.009\n",
      "Epoch 100: Loss:0.009, Val_loss:0.009\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3959d6780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "13\n",
      "Threshold:  0.130857455543\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_sfs/_8_15_10_para.ckpt\n",
      "--- Initialization time: 40.31535983085632 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
