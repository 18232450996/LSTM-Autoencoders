{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 5\n",
    "        self.step_num = 100\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_multivariate/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_multivariate/log.txt\"\n",
    "        self.training_set_size = 100*200\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 19100 normal windows and 900 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:95,vn1:57,vn2:19,tn:20,va:4,ta:5\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.093, Val_loss:0.054\n",
      "Epoch 1: Loss:0.046, Val_loss:0.038\n",
      "Epoch 2: Loss:0.032, Val_loss:0.030\n",
      "Epoch 3: Loss:0.024, Val_loss:0.025\n",
      "Epoch 4: Loss:0.020, Val_loss:0.022\n",
      "Epoch 5: Loss:0.018, Val_loss:0.021\n",
      "Epoch 6: Loss:0.017, Val_loss:0.020\n",
      "Epoch 7: Loss:0.016, Val_loss:0.019\n",
      "Epoch 8: Loss:0.016, Val_loss:0.018\n",
      "Epoch 9: Loss:0.015, Val_loss:0.018\n",
      "Epoch 10: Loss:0.015, Val_loss:0.018\n",
      "Epoch 11: Loss:0.015, Val_loss:0.017\n",
      "Epoch 12: Loss:0.015, Val_loss:0.017\n",
      "Epoch 13: Loss:0.015, Val_loss:0.017\n",
      "Epoch 14: Loss:0.015, Val_loss:0.017\n",
      "Epoch 15: Loss:0.015, Val_loss:0.017\n",
      "Epoch 16: Loss:0.015, Val_loss:0.017\n",
      "Epoch 17: Loss:0.015, Val_loss:0.017\n",
      "Epoch 18: Loss:0.014, Val_loss:0.016\n",
      "Epoch 19: Loss:0.014, Val_loss:0.016\n",
      "Epoch 20: Loss:0.014, Val_loss:0.016\n",
      "Epoch 21: Loss:0.014, Val_loss:0.016\n",
      "Epoch 22: Loss:0.014, Val_loss:0.016\n",
      "Epoch 23: Loss:0.014, Val_loss:0.016\n",
      "Epoch 24: Loss:0.014, Val_loss:0.016\n",
      "Epoch 25: Loss:0.014, Val_loss:0.016\n",
      "Epoch 26: Loss:0.014, Val_loss:0.016\n",
      "Epoch 27: Loss:0.014, Val_loss:0.016\n",
      "Epoch 28: Loss:0.014, Val_loss:0.015\n",
      "Epoch 29: Loss:0.014, Val_loss:0.015\n",
      "Epoch 30: Loss:0.014, Val_loss:0.015\n",
      "Epoch 31: Loss:0.014, Val_loss:0.015\n",
      "Epoch 32: Loss:0.014, Val_loss:0.015\n",
      "Epoch 33: Loss:0.013, Val_loss:0.015\n",
      "Epoch 34: Loss:0.013, Val_loss:0.015\n",
      "Epoch 35: Loss:0.013, Val_loss:0.015\n",
      "Epoch 36: Loss:0.013, Val_loss:0.015\n",
      "Epoch 37: Loss:0.013, Val_loss:0.014\n",
      "Epoch 38: Loss:0.013, Val_loss:0.014\n",
      "Epoch 39: Loss:0.013, Val_loss:0.014\n",
      "Epoch 40: Loss:0.013, Val_loss:0.014\n",
      "Epoch 41: Loss:0.013, Val_loss:0.014\n",
      "Epoch 42: Loss:0.013, Val_loss:0.014\n",
      "Epoch 43: Loss:0.012, Val_loss:0.014\n",
      "Epoch 44: Loss:0.012, Val_loss:0.014\n",
      "Epoch 45: Loss:0.012, Val_loss:0.013\n",
      "Epoch 46: Loss:0.012, Val_loss:0.013\n",
      "Epoch 47: Loss:0.012, Val_loss:0.013\n",
      "Epoch 48: Loss:0.012, Val_loss:0.013\n",
      "Epoch 49: Loss:0.012, Val_loss:0.013\n",
      "Epoch 50: Loss:0.012, Val_loss:0.013\n",
      "Epoch 51: Loss:0.012, Val_loss:0.013\n",
      "Epoch 52: Loss:0.012, Val_loss:0.013\n",
      "Epoch 53: Loss:0.012, Val_loss:0.013\n",
      "Epoch 54: Loss:0.012, Val_loss:0.013\n",
      "Epoch 55: Loss:0.012, Val_loss:0.013\n",
      "Epoch 56: Loss:0.012, Val_loss:0.012\n",
      "Epoch 57: Loss:0.012, Val_loss:0.012\n",
      "Epoch 58: Loss:0.012, Val_loss:0.012\n",
      "Epoch 59: Loss:0.011, Val_loss:0.012\n",
      "Epoch 60: Loss:0.011, Val_loss:0.012\n",
      "Epoch 61: Loss:0.011, Val_loss:0.012\n",
      "Epoch 62: Loss:0.011, Val_loss:0.012\n",
      "Epoch 63: Loss:0.011, Val_loss:0.012\n",
      "Epoch 64: Loss:0.011, Val_loss:0.012\n",
      "Epoch 65: Loss:0.011, Val_loss:0.012\n",
      "Epoch 66: Loss:0.011, Val_loss:0.012\n",
      "Epoch 67: Loss:0.011, Val_loss:0.012\n",
      "Epoch 68: Loss:0.011, Val_loss:0.012\n",
      "Epoch 69: Loss:0.011, Val_loss:0.012\n",
      "Epoch 70: Loss:0.011, Val_loss:0.012\n",
      "Early stopping at epoch 70\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCWQYKoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8ccf934e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "4\n",
      "Threshold:  0.0691460227537\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_multivariate/_1_5_100_para.ckpt\n",
      "--- Initialization time: 887.6635921001434 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
