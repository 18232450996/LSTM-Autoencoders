{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from scipy.spatial.distance import mahalanobis,euclidean\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_num = 1\n",
    "hidden_num = 45#5\n",
    "step_num = 20#100\n",
    "elem_num = 34\n",
    "init_wins = 10000\n",
    "\n",
    "names = [str(x) for x in range(elem_num)] +[\"label\"]\n",
    "smtp = pd.read_csv(\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\",names=names,skiprows=step_num*init_wins)\n",
    "wins = smtp.shape[0]//step_num\n",
    "test_set = smtp.iloc[:wins*batch_num*step_num,:-1]\n",
    "labels =smtp.iloc[:wins*batch_num*step_num,-1]\n",
    "\n",
    "ts = test_set.as_matrix().reshape(wins,batch_num,step_num,elem_num)\n",
    "test_set_list = [ts[a] for a in range(wins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels[labels!=\"normal\"] = \"anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del smtp\n",
    "del test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# figure out anomaly windows\n",
    "buffer = [labels[i*step_num:(i+1)*step_num] for i in range(0,labels.size//step_num)]\n",
    "anomaly_index = []\n",
    "count = 0\n",
    "for buf in buffer:\n",
    "    if \"anomaly\" in buf.tolist():\n",
    "        anomaly_index.append(count)\n",
    "    else:\n",
    "        pass\n",
    "    count +=1\n",
    "print(anomaly_index)\n",
    "\n",
    "expert = [\"normal\"]*wins\n",
    "for x in anomaly_index:\n",
    "    expert[x] = \"anomaly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Bin/Desktop/Thesis/models/smtp_2000init/_1_45_20_para.ckpt\n"
     ]
    }
   ],
   "source": [
    "modelpath_root =\"C:/Users/Bin/Desktop/Thesis/models/smtp_2000init/\"#\"C:/Users/Bin/Desktop/Thesis/models/smtp_multivariate/\"\n",
    "modelmeta_p = modelpath_root + \"_1_45_20_para.ckpt.meta\"#\"_1_5_100_para.ckpt.meta\"\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.import_meta_graph(modelmeta_p) # load trained gragh, but without the trained parameters\n",
    "saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "p_input = graph.get_tensor_by_name(\"p_input:0\")\n",
    "p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)] \n",
    "p_is_training = graph.get_tensor_by_name(\"is_training_:0\")\n",
    "\n",
    "input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "tensor_mu = graph.get_tensor_by_name(\"mu:0\")\n",
    "tensor_sigma = graph.get_tensor_by_name(\"sigma:0\")\n",
    "tensor_threshold = graph.get_tensor_by_name(\"threshold:0\")\n",
    "\n",
    "loss_ = graph.get_tensor_by_name(\"decoder/loss:0\")\n",
    "train_ = graph.get_operation_by_name(\"cond/train_\")\n",
    "\n",
    "mu = sess.run(tensor_mu)\n",
    "sigma = sess.run(tensor_sigma)\n",
    "threshold = sess.run(tensor_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34,) (34, 34) 0.0859473\n"
     ]
    }
   ],
   "source": [
    "print(mu.shape,sigma.shape,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Phase (With expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_preprocessing(batchdata):\n",
    "    # input batchdata with shape : [batch_num, step_num, elem_num]\n",
    "    # minmax scaler on window level\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for window in batchdata:\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(window)\n",
    "        new_win = scaler.transform(window)\n",
    "        df = pd.concat((df, pd.DataFrame(new_win)),axis=0) if df.size!=0 else pd.DataFrame(new_win)\n",
    "    return df.as_matrix().reshape(batchdata.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scoring(err,mu,sigma):\n",
    "    \n",
    "    scores = []\n",
    "    for e in err:\n",
    "        scores.append(mahalanobis(e,mu,sigma))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameter(errBuffer,mu,sigma,eta,threshold,score,label):       \n",
    "        tmp_errBuffer = errBuffer[-3:] if len(errBuffer)>3 else errBuffer\n",
    "        err_vec_array = np.array(tmp_errBuffer)\n",
    "        # for univariate  data\n",
    "        __mu = np.mean(err_vec_array.ravel())\n",
    "        mu = mu*eta + __mu*(1-eta)\n",
    "\n",
    "        __sigma =np.var(err_vec_array.ravel())\n",
    "        sigma = __sigma\n",
    "        \n",
    "        __thresold = scores.min()\n",
    "        f = 0\n",
    "        for t in range(int(scores.min()*100),int(scores.max()*100),5):\n",
    "            fbeta,_, _ = evaluate(t/100,score,label)\n",
    "            __thresold = t/100 if fbeta>f else __thresold\n",
    "            \n",
    "        threshold = threshold*eta + __thresold*(1-eta)\n",
    "        return mu,sigma,threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_musigma(err_nbuf,mu,sigma):       \n",
    "    \n",
    "        err_vec_array = np.array(err_nbuf)\n",
    "        # for multivariate  data\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_threshold(normal_score, abnormal_score):\n",
    "        upper = np.median(np.array(abnormal_score))\n",
    "        lower = np.median(np.array(normal_score)) \n",
    "        scala = 20\n",
    "        delta = (upper-lower) / scala\n",
    "        candidate = lower\n",
    "        threshold = 0\n",
    "        result = 0\n",
    "\n",
    "        def evaluate(threshold,normal_score,abnormal_score):\n",
    "\n",
    "            beta = 0.5\n",
    "            tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "            fp = len(abnormal_score)-tp\n",
    "            fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "            tn = len(normal_score)- fn\n",
    "\n",
    "            if tp == 0: return 0\n",
    "\n",
    "            P = tp/(tp+fp)\n",
    "            R = tp/(tp+fn)\n",
    "            fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "            return fbeta \n",
    "\n",
    "        for _ in range(scala):\n",
    "            r = evaluate(candidate,normal_score,abnormal_score)\n",
    "            if r > result:\n",
    "                result = r \n",
    "                threshold = candidate\n",
    "            candidate += delta \n",
    "        return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(fpr,tpr,auc):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' %auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:342: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  y_true = (y_true == pos_label)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8aea17dc6fae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0merr_buf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"anomaly\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;31m#print(fpr,tpr,thresholds,auc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 534\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mdesc_score_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mergesort\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "n_buf = []\n",
    "a_buf = []\n",
    "\n",
    "y = []\n",
    "output=[]\n",
    "box = []\n",
    "err_nbuf = []\n",
    "err_abuf = []\n",
    "all_scores = []\n",
    "\n",
    "for l in labels:\n",
    "    if l == \"normal\":\n",
    "        y +=[0]\n",
    "    else: \n",
    "        y +=[1]\n",
    "for data in test_set_list:\n",
    "    \n",
    "        prediction = []\n",
    "        df = local_preprocessing(data)\n",
    "        (input_n, output_n) = sess.run([input_, output_], {p_input: df, p_is_training: False})\n",
    "\n",
    "        err = abs(input_n-output_n).reshape(-1,elem_num)\n",
    "        \n",
    "        box.append(err.reshape(-1,))\n",
    "        \n",
    "        scores = scoring(err,mu,sigma)\n",
    "        scores = pd.Series(scores)\n",
    "        \n",
    "        all_scores.append(scores)\n",
    "#        output +=  [np.array(heapq.nlargest(10,scores)).mean()]\n",
    "        output += [scores.max()]\n",
    "        pred = [scores[b*step_num:(b+1)*step_num] for b in range(batch_num)]\n",
    "        label = [expert[count*batch_num+b]for b in range(batch_num)]\n",
    "        e = [err.reshape(-1,elem_num)]\n",
    "        for index,value in enumerate(pred):\n",
    "            if value[value>threshold].size>=2: \n",
    "                if label[index] == \"anomaly\":\n",
    "                    #print(\"TP\")                 \n",
    "                    a_buf.append(df[index])\n",
    "                    err_abuf.append(e[index])\n",
    "                    #err_abuf = err_abuf + e[index] if len(err_abuf) != 0 else e[index]\n",
    "                else:\n",
    "                    #print(\"FP\")\n",
    "                    err_nbuf.append(e[index])\n",
    "                    #err_nbuf = err_nbuf + e[index] if len(err_nbuf) != 0 else e[index]\n",
    "                    n_buf.append(df[index])\n",
    "            else:               \n",
    "                if label[index] == \"anomaly\":             \n",
    "                    #print(\"FN\")\n",
    "                    a_buf.append(df[index])\n",
    "                    err_abuf.append(e[index])\n",
    "                    #err_abuf = err_abuf + e[index] if len(err_abuf) != 0 else e[index]\n",
    "                else:\n",
    "                    err_nbuf.append(e[index])\n",
    "                    #err_nbuf = err_nbuf + e[index] if len(err_nbuf) != 0 else e[index]\n",
    "                    #print(\"TN\")\n",
    "        count +=1\n",
    "        \n",
    "\n",
    "        \n",
    "        if len(n_buf)>=3000000 and len(a_buf)>=10:\n",
    "            print(\"retrain at %d batch\"%count)\n",
    "            loss_list_all=[]\n",
    "\n",
    "            datalist = np.array(n_buf[:6]).reshape(-1,batch_num,step_num,elem_num)\n",
    "            validation_list_n = np.array(n_buf[6:]).reshape(-1,batch_num,step_num,elem_num)\n",
    "            validation_list_a = np.array(a_buf).reshape(-1,batch_num,step_num,elem_num)\n",
    "            \n",
    "            for i in range(30):\n",
    "                \n",
    "                loss_list=[]\n",
    "                for data in datalist:\n",
    "                    (loss, _) = sess.run([loss_, train_], {p_input: data,p_is_training : True})\n",
    "                    loss_list.append(loss)\n",
    "                #print('Retrain-iter %d:' % (i + 1), np.array(loss_list).mean())\n",
    "                loss_list_all.append( np.array(loss_list).mean()) \n",
    "            \n",
    "\n",
    "            err_nbuf_tmp = np.array(err_nbuf).reshape(-1,elem_num)\n",
    "            mu,sigma = get_musigma(err_nbuf_tmp,mu,sigma)\n",
    "\n",
    "            normal_score = []\n",
    "            abnormal_score = []\n",
    "            \n",
    "            for val in validation_list_n:\n",
    "                (ein,aus) = sess.run([input_,output_], {p_input: val,p_is_training :False})\n",
    "                \n",
    "                err = abs(ein-aus).reshape(-1,elem_num)\n",
    "        \n",
    "                normal_score +=scoring(err,mu,sigma)\n",
    "            \n",
    "            for val_a in validation_list_a:\n",
    "                (ein,aus) = sess.run([input_,output_], {p_input: val_a,p_is_training :False})\n",
    "                \n",
    "                err = abs(ein-aus).reshape(-1,elem_num)\n",
    "        \n",
    "                abnormal_score +=scoring(err,mu,sigma)\n",
    "            \n",
    "            threshold = get_threshold(normal_score, abnormal_score)\n",
    "        \n",
    "            print(\"Parameters updated!\")\n",
    "            pd.Series(loss_list_all).plot(title=\"Loss\")\n",
    "            n_buf = []\n",
    "            a_buf = []\n",
    "            err_buf = []\n",
    "                \n",
    "fpr, tpr, thresholds = metrics.roc_curve(expert, output, pos_label=\"anomaly\")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "#print(fpr,tpr,thresholds,auc)\n",
    "plot_roc(fpr,tpr,auc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with retraining\n",
    "foo = pd.Series(np.array(all_scores).ravel())\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels[1:], foo[1:], pos_label=\"anomaly\")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "#print(fpr,tpr,thresholds,auc)\n",
    "plot_roc(fpr,tpr,auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
