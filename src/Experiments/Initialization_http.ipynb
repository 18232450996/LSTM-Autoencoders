{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 35\n",
    "        self.step_num = 30\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/http_new.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/http_8_35_30/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/http_8_35_30/log.txt\"\n",
    "        self.training_set_size = self.step_num*1000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                if i == 100:\n",
    "                    break;\n",
    "             \n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 29460 normal windows and 540 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:490,vn1:294,vn2:98,tn:100,va:9,ta:9\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.180, Val_loss:0.033\n",
      "Epoch 1: Loss:0.027, Val_loss:0.014\n",
      "Epoch 2: Loss:0.015, Val_loss:0.009\n",
      "Epoch 3: Loss:0.011, Val_loss:0.007\n",
      "Epoch 4: Loss:0.009, Val_loss:0.006\n",
      "Epoch 5: Loss:0.008, Val_loss:0.005\n",
      "Epoch 6: Loss:0.007, Val_loss:0.004\n",
      "Epoch 7: Loss:0.006, Val_loss:0.004\n",
      "Epoch 8: Loss:0.005, Val_loss:0.003\n",
      "Epoch 9: Loss:0.005, Val_loss:0.003\n",
      "Epoch 10: Loss:0.004, Val_loss:0.002\n",
      "Epoch 11: Loss:0.004, Val_loss:0.002\n",
      "Epoch 12: Loss:0.004, Val_loss:0.002\n",
      "Epoch 13: Loss:0.003, Val_loss:0.002\n",
      "Epoch 14: Loss:0.003, Val_loss:0.002\n",
      "Epoch 15: Loss:0.003, Val_loss:0.002\n",
      "Epoch 16: Loss:0.003, Val_loss:0.002\n",
      "Epoch 17: Loss:0.003, Val_loss:0.001\n",
      "Epoch 18: Loss:0.003, Val_loss:0.001\n",
      "Epoch 19: Loss:0.003, Val_loss:0.001\n",
      "Epoch 20: Loss:0.003, Val_loss:0.001\n",
      "Epoch 21: Loss:0.003, Val_loss:0.001\n",
      "Epoch 22: Loss:0.003, Val_loss:0.001\n",
      "Epoch 23: Loss:0.002, Val_loss:0.001\n",
      "Epoch 24: Loss:0.002, Val_loss:0.001\n",
      "Epoch 25: Loss:0.002, Val_loss:0.001\n",
      "Epoch 26: Loss:0.002, Val_loss:0.001\n",
      "Epoch 27: Loss:0.002, Val_loss:0.001\n",
      "Epoch 28: Loss:0.002, Val_loss:0.001\n",
      "Epoch 29: Loss:0.002, Val_loss:0.001\n",
      "Epoch 30: Loss:0.002, Val_loss:0.001\n",
      "Epoch 31: Loss:0.002, Val_loss:0.001\n",
      "Epoch 32: Loss:0.002, Val_loss:0.001\n",
      "Epoch 33: Loss:0.002, Val_loss:0.001\n",
      "Epoch 34: Loss:0.002, Val_loss:0.001\n",
      "Epoch 35: Loss:0.002, Val_loss:0.001\n",
      "Epoch 36: Loss:0.002, Val_loss:0.001\n",
      "Epoch 37: Loss:0.002, Val_loss:0.001\n",
      "Epoch 38: Loss:0.002, Val_loss:0.001\n",
      "Epoch 39: Loss:0.002, Val_loss:0.001\n",
      "Epoch 40: Loss:0.002, Val_loss:0.001\n",
      "Epoch 41: Loss:0.002, Val_loss:0.001\n",
      "Epoch 42: Loss:0.002, Val_loss:0.001\n",
      "Epoch 43: Loss:0.002, Val_loss:0.001\n",
      "Epoch 44: Loss:0.002, Val_loss:0.001\n",
      "Epoch 45: Loss:0.002, Val_loss:0.001\n",
      "Epoch 46: Loss:0.002, Val_loss:0.001\n",
      "Epoch 47: Loss:0.002, Val_loss:0.001\n",
      "Epoch 48: Loss:0.002, Val_loss:0.001\n",
      "Epoch 49: Loss:0.002, Val_loss:0.001\n",
      "Epoch 50: Loss:0.002, Val_loss:0.001\n",
      "Epoch 51: Loss:0.002, Val_loss:0.001\n",
      "Epoch 52: Loss:0.002, Val_loss:0.001\n",
      "Epoch 53: Loss:0.002, Val_loss:0.001\n",
      "Epoch 54: Loss:0.002, Val_loss:0.001\n",
      "Epoch 55: Loss:0.002, Val_loss:0.001\n",
      "Epoch 56: Loss:0.002, Val_loss:0.001\n",
      "Epoch 57: Loss:0.002, Val_loss:0.001\n",
      "Epoch 58: Loss:0.002, Val_loss:0.001\n",
      "Epoch 59: Loss:0.002, Val_loss:0.001\n",
      "Epoch 60: Loss:0.002, Val_loss:0.001\n",
      "Epoch 61: Loss:0.002, Val_loss:0.001\n",
      "Epoch 62: Loss:0.002, Val_loss:0.001\n",
      "Epoch 63: Loss:0.002, Val_loss:0.001\n",
      "Epoch 64: Loss:0.002, Val_loss:0.001\n",
      "Epoch 65: Loss:0.002, Val_loss:0.001\n",
      "Epoch 66: Loss:0.002, Val_loss:0.001\n",
      "Epoch 67: Loss:0.002, Val_loss:0.001\n",
      "Epoch 68: Loss:0.002, Val_loss:0.001\n",
      "Epoch 69: Loss:0.002, Val_loss:0.001\n",
      "Epoch 70: Loss:0.002, Val_loss:0.001\n",
      "Epoch 71: Loss:0.002, Val_loss:0.001\n",
      "Epoch 72: Loss:0.002, Val_loss:0.001\n",
      "Epoch 73: Loss:0.002, Val_loss:0.001\n",
      "Epoch 74: Loss:0.002, Val_loss:0.001\n",
      "Epoch 75: Loss:0.002, Val_loss:0.001\n",
      "Epoch 76: Loss:0.002, Val_loss:0.001\n",
      "Epoch 77: Loss:0.002, Val_loss:0.001\n",
      "Epoch 78: Loss:0.002, Val_loss:0.001\n",
      "Epoch 79: Loss:0.002, Val_loss:0.001\n",
      "Epoch 80: Loss:0.002, Val_loss:0.001\n",
      "Epoch 81: Loss:0.002, Val_loss:0.001\n",
      "Epoch 82: Loss:0.002, Val_loss:0.001\n",
      "Epoch 83: Loss:0.002, Val_loss:0.001\n",
      "Epoch 84: Loss:0.002, Val_loss:0.001\n",
      "Epoch 85: Loss:0.002, Val_loss:0.001\n",
      "Epoch 86: Loss:0.002, Val_loss:0.001\n",
      "Epoch 87: Loss:0.002, Val_loss:0.001\n",
      "Epoch 88: Loss:0.002, Val_loss:0.001\n",
      "Epoch 89: Loss:0.002, Val_loss:0.001\n",
      "Epoch 90: Loss:0.002, Val_loss:0.001\n",
      "Epoch 91: Loss:0.002, Val_loss:0.001\n",
      "Epoch 92: Loss:0.002, Val_loss:0.001\n",
      "Epoch 93: Loss:0.002, Val_loss:0.001\n",
      "Epoch 94: Loss:0.002, Val_loss:0.001\n",
      "Epoch 95: Loss:0.002, Val_loss:0.001\n",
      "Epoch 96: Loss:0.002, Val_loss:0.001\n",
      "Epoch 97: Loss:0.002, Val_loss:0.001\n",
      "Epoch 98: Loss:0.002, Val_loss:0.001\n",
      "Epoch 99: Loss:0.002, Val_loss:0.001\n",
      "Epoch 100: Loss:0.002, Val_loss:0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a3ed5f8b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "9\n",
      "Threshold:  0.0549761145839\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/http_8_35_30/_8_35_30_para.ckpt\n",
      "--- Initialization time: 195.33389258384705 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
