{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/http_new.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/http_8_15_10_/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/http_8_15_10_/log.txt\"\n",
    "        self.training_set_size = self.step_num*30000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                if i == 30:\n",
    "                    break;\n",
    "             \n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 298080 normal windows and 1920 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:14900,vn1:8940,vn2:2980,tn:2988,va:96,ta:96\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.014, Val_loss:0.003\n",
      "Epoch 1: Loss:0.005, Val_loss:0.002\n",
      "Epoch 2: Loss:0.004, Val_loss:0.001\n",
      "Epoch 3: Loss:0.004, Val_loss:0.001\n",
      "Epoch 4: Loss:0.003, Val_loss:0.001\n",
      "Epoch 5: Loss:0.003, Val_loss:0.001\n",
      "Epoch 6: Loss:0.003, Val_loss:0.001\n",
      "Epoch 7: Loss:0.003, Val_loss:0.001\n",
      "Epoch 8: Loss:0.003, Val_loss:0.001\n",
      "Epoch 9: Loss:0.003, Val_loss:0.001\n",
      "Epoch 10: Loss:0.002, Val_loss:0.001\n",
      "Epoch 11: Loss:0.002, Val_loss:0.001\n",
      "Epoch 12: Loss:0.002, Val_loss:0.001\n",
      "Epoch 13: Loss:0.002, Val_loss:0.001\n",
      "Epoch 14: Loss:0.002, Val_loss:0.001\n",
      "Epoch 15: Loss:0.002, Val_loss:0.001\n",
      "Epoch 16: Loss:0.002, Val_loss:0.001\n",
      "Epoch 17: Loss:0.002, Val_loss:0.001\n",
      "Epoch 18: Loss:0.002, Val_loss:0.000\n",
      "Epoch 19: Loss:0.002, Val_loss:0.000\n",
      "Epoch 20: Loss:0.002, Val_loss:0.000\n",
      "Epoch 21: Loss:0.002, Val_loss:0.000\n",
      "Epoch 22: Loss:0.002, Val_loss:0.000\n",
      "Epoch 23: Loss:0.002, Val_loss:0.000\n",
      "Epoch 24: Loss:0.002, Val_loss:0.000\n",
      "Epoch 25: Loss:0.002, Val_loss:0.000\n",
      "Epoch 26: Loss:0.002, Val_loss:0.000\n",
      "Epoch 27: Loss:0.002, Val_loss:0.000\n",
      "Epoch 28: Loss:0.002, Val_loss:0.000\n",
      "Epoch 29: Loss:0.002, Val_loss:0.000\n",
      "Epoch 30: Loss:0.002, Val_loss:0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nixgDZW7UfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d4c0f059b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "96\n",
      "Threshold:  0.021023363036\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/http_8_15_10_/_8_15_10_para.ckpt\n",
      "--- Initialization time: 471.4649531841278 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
