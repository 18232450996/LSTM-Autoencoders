{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp+http/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp+http/log.txt\"\n",
    "        self.training_set_size = 20*2500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            \n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 49360 normal windows and 640 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:1230,vn1:738,vn2:246,tn:254,va:16,ta:16\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.017, Val_loss:0.010\n",
      "Epoch 1: Loss:0.009, Val_loss:0.009\n",
      "Epoch 2: Loss:0.009, Val_loss:0.009\n",
      "Epoch 3: Loss:0.009, Val_loss:0.008\n",
      "Epoch 4: Loss:0.008, Val_loss:0.008\n",
      "Epoch 5: Loss:0.007, Val_loss:0.007\n",
      "Epoch 6: Loss:0.007, Val_loss:0.006\n",
      "Epoch 7: Loss:0.006, Val_loss:0.006\n",
      "Epoch 8: Loss:0.006, Val_loss:0.006\n",
      "Epoch 9: Loss:0.006, Val_loss:0.006\n",
      "Epoch 10: Loss:0.005, Val_loss:0.006\n",
      "Epoch 11: Loss:0.005, Val_loss:0.005\n",
      "Epoch 12: Loss:0.005, Val_loss:0.005\n",
      "Epoch 13: Loss:0.005, Val_loss:0.005\n",
      "Epoch 14: Loss:0.005, Val_loss:0.005\n",
      "Epoch 15: Loss:0.005, Val_loss:0.005\n",
      "Epoch 16: Loss:0.005, Val_loss:0.004\n",
      "Epoch 17: Loss:0.004, Val_loss:0.004\n",
      "Epoch 18: Loss:0.004, Val_loss:0.004\n",
      "Epoch 19: Loss:0.004, Val_loss:0.004\n",
      "Epoch 20: Loss:0.004, Val_loss:0.004\n",
      "Epoch 21: Loss:0.004, Val_loss:0.004\n",
      "Epoch 22: Loss:0.004, Val_loss:0.004\n",
      "Epoch 23: Loss:0.004, Val_loss:0.004\n",
      "Epoch 24: Loss:0.004, Val_loss:0.004\n",
      "Epoch 25: Loss:0.004, Val_loss:0.004\n",
      "Epoch 26: Loss:0.004, Val_loss:0.004\n",
      "Epoch 27: Loss:0.004, Val_loss:0.004\n",
      "Epoch 28: Loss:0.004, Val_loss:0.004\n",
      "Epoch 29: Loss:0.004, Val_loss:0.004\n",
      "Epoch 30: Loss:0.004, Val_loss:0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIwxBoD/Dy8JRAcnm2guAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1620ead5630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "16\n",
      "Threshold:  0.037954451056\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp+http/_1_45_20_para.ckpt\n",
      "--- Initialization time: 545.3994126319885 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
