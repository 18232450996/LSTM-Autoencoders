{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_10/log.txt\"\n",
    "        self.training_set_size = self.step_num*6000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 100:\n",
    "                    break\n",
    "\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 59350 normal windows and 650 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:2965,vn1:1779,vn2:593,tn:598,va:32,ta:33\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.052, Val_loss:0.020\n",
      "Epoch 1: Loss:0.015, Val_loss:0.015\n",
      "Epoch 2: Loss:0.013, Val_loss:0.014\n",
      "Epoch 3: Loss:0.012, Val_loss:0.013\n",
      "Epoch 4: Loss:0.011, Val_loss:0.012\n",
      "Epoch 5: Loss:0.011, Val_loss:0.011\n",
      "Epoch 6: Loss:0.010, Val_loss:0.011\n",
      "Epoch 7: Loss:0.010, Val_loss:0.010\n",
      "Epoch 8: Loss:0.010, Val_loss:0.010\n",
      "Epoch 9: Loss:0.009, Val_loss:0.009\n",
      "Epoch 10: Loss:0.009, Val_loss:0.009\n",
      "Epoch 11: Loss:0.008, Val_loss:0.007\n",
      "Epoch 12: Loss:0.007, Val_loss:0.007\n",
      "Epoch 13: Loss:0.007, Val_loss:0.007\n",
      "Epoch 14: Loss:0.006, Val_loss:0.007\n",
      "Epoch 15: Loss:0.006, Val_loss:0.006\n",
      "Epoch 16: Loss:0.006, Val_loss:0.007\n",
      "Epoch 17: Loss:0.006, Val_loss:0.006\n",
      "Epoch 18: Loss:0.006, Val_loss:0.006\n",
      "Epoch 19: Loss:0.006, Val_loss:0.006\n",
      "Epoch 20: Loss:0.006, Val_loss:0.005\n",
      "Epoch 21: Loss:0.005, Val_loss:0.005\n",
      "Epoch 22: Loss:0.005, Val_loss:0.005\n",
      "Epoch 23: Loss:0.005, Val_loss:0.005\n",
      "Epoch 24: Loss:0.005, Val_loss:0.005\n",
      "Epoch 25: Loss:0.005, Val_loss:0.005\n",
      "Epoch 26: Loss:0.005, Val_loss:0.005\n",
      "Epoch 27: Loss:0.004, Val_loss:0.005\n",
      "Epoch 28: Loss:0.004, Val_loss:0.005\n",
      "Epoch 29: Loss:0.004, Val_loss:0.005\n",
      "Epoch 30: Loss:0.004, Val_loss:0.005\n",
      "Epoch 31: Loss:0.004, Val_loss:0.004\n",
      "Epoch 32: Loss:0.004, Val_loss:0.005\n",
      "Epoch 33: Loss:0.004, Val_loss:0.004\n",
      "Epoch 34: Loss:0.004, Val_loss:0.004\n",
      "Epoch 35: Loss:0.004, Val_loss:0.004\n",
      "Epoch 36: Loss:0.004, Val_loss:0.004\n",
      "Epoch 37: Loss:0.004, Val_loss:0.006\n",
      "Epoch 38: Loss:0.004, Val_loss:0.004\n",
      "Epoch 39: Loss:0.004, Val_loss:0.005\n",
      "Epoch 40: Loss:0.004, Val_loss:0.004\n",
      "Epoch 41: Loss:0.004, Val_loss:0.004\n",
      "Epoch 42: Loss:0.004, Val_loss:0.004\n",
      "Epoch 43: Loss:0.003, Val_loss:0.004\n",
      "Epoch 44: Loss:0.004, Val_loss:0.004\n",
      "Epoch 45: Loss:0.003, Val_loss:0.004\n",
      "Epoch 46: Loss:0.003, Val_loss:0.005\n",
      "Epoch 47: Loss:0.003, Val_loss:0.004\n",
      "Epoch 48: Loss:0.003, Val_loss:0.004\n",
      "Epoch 49: Loss:0.003, Val_loss:0.004\n",
      "Epoch 50: Loss:0.003, Val_loss:0.004\n",
      "Epoch 51: Loss:0.003, Val_loss:0.004\n",
      "Epoch 52: Loss:0.003, Val_loss:0.005\n",
      "Epoch 53: Loss:0.003, Val_loss:0.005\n",
      "Epoch 54: Loss:0.003, Val_loss:0.003\n",
      "Epoch 55: Loss:0.003, Val_loss:0.003\n",
      "Epoch 56: Loss:0.003, Val_loss:0.003\n",
      "Epoch 57: Loss:0.003, Val_loss:0.003\n",
      "Epoch 58: Loss:0.003, Val_loss:0.005\n",
      "Epoch 59: Loss:0.003, Val_loss:0.004\n",
      "Epoch 60: Loss:0.003, Val_loss:0.003\n",
      "Epoch 61: Loss:0.003, Val_loss:0.004\n",
      "Epoch 62: Loss:0.003, Val_loss:0.004\n",
      "Epoch 63: Loss:0.003, Val_loss:0.003\n",
      "Epoch 64: Loss:0.003, Val_loss:0.003\n",
      "Epoch 65: Loss:0.003, Val_loss:0.003\n",
      "Epoch 66: Loss:0.003, Val_loss:0.004\n",
      "Epoch 67: Loss:0.003, Val_loss:0.005\n",
      "Epoch 68: Loss:0.003, Val_loss:0.003\n",
      "Epoch 69: Loss:0.003, Val_loss:0.004\n",
      "Epoch 70: Loss:0.003, Val_loss:0.003\n",
      "Epoch 71: Loss:0.003, Val_loss:0.004\n",
      "Epoch 72: Loss:0.003, Val_loss:0.003\n",
      "Epoch 73: Loss:0.003, Val_loss:0.003\n",
      "Epoch 74: Loss:0.003, Val_loss:0.004\n",
      "Epoch 75: Loss:0.003, Val_loss:0.003\n",
      "Epoch 76: Loss:0.002, Val_loss:0.004\n",
      "Epoch 77: Loss:0.003, Val_loss:0.003\n",
      "Epoch 78: Loss:0.002, Val_loss:0.004\n",
      "Epoch 79: Loss:0.003, Val_loss:0.003\n",
      "Epoch 80: Loss:0.002, Val_loss:0.003\n",
      "Epoch 81: Loss:0.002, Val_loss:0.003\n",
      "Epoch 82: Loss:0.002, Val_loss:0.003\n",
      "Epoch 83: Loss:0.002, Val_loss:0.003\n",
      "Epoch 84: Loss:0.002, Val_loss:0.003\n",
      "Epoch 85: Loss:0.002, Val_loss:0.003\n",
      "Epoch 86: Loss:0.002, Val_loss:0.003\n",
      "Epoch 87: Loss:0.002, Val_loss:0.003\n",
      "Epoch 88: Loss:0.002, Val_loss:0.003\n",
      "Epoch 89: Loss:0.002, Val_loss:0.003\n",
      "Epoch 90: Loss:0.002, Val_loss:0.003\n",
      "Epoch 91: Loss:0.002, Val_loss:0.003\n",
      "Epoch 92: Loss:0.002, Val_loss:0.003\n",
      "Epoch 93: Loss:0.002, Val_loss:0.002\n",
      "Epoch 94: Loss:0.002, Val_loss:0.004\n",
      "Epoch 95: Loss:0.002, Val_loss:0.003\n",
      "Epoch 96: Loss:0.002, Val_loss:0.003\n",
      "Epoch 97: Loss:0.002, Val_loss:0.003\n",
      "Epoch 98: Loss:0.002, Val_loss:0.003\n",
      "Epoch 99: Loss:0.002, Val_loss:0.002\n",
      "Epoch 100: Loss:0.002, Val_loss:0.002\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfRGRTkShLyLSiSj0RUQ6kf8PKfnLPZMLVLgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fab018c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "32\n",
      "Threshold:  0.0945807854154\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_10/_8_15_10_para.ckpt\n",
      "--- Initialization time: 241.73182678222656 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
