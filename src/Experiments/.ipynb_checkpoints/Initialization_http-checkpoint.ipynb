{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 5\n",
    "        self.step_num = 30\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/http_new.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/http/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/http/log.txt\"\n",
    "        self.training_set_size = 100*1000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 97800 normal windows and 2190 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:1630,vn1:978,vn2:326,tn:326,va:36,ta:37\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.033, Val_loss:0.008\n",
      "Epoch 1: Loss:0.012, Val_loss:0.006\n",
      "Epoch 2: Loss:0.007, Val_loss:0.004\n",
      "Epoch 3: Loss:0.005, Val_loss:0.003\n",
      "Epoch 4: Loss:0.005, Val_loss:0.003\n",
      "Epoch 5: Loss:0.004, Val_loss:0.003\n",
      "Epoch 6: Loss:0.004, Val_loss:0.003\n",
      "Epoch 7: Loss:0.004, Val_loss:0.003\n",
      "Epoch 8: Loss:0.004, Val_loss:0.003\n",
      "Epoch 9: Loss:0.004, Val_loss:0.003\n",
      "Epoch 10: Loss:0.004, Val_loss:0.003\n",
      "Epoch 11: Loss:0.004, Val_loss:0.003\n",
      "Epoch 12: Loss:0.003, Val_loss:0.002\n",
      "Epoch 13: Loss:0.003, Val_loss:0.002\n",
      "Epoch 14: Loss:0.003, Val_loss:0.002\n",
      "Epoch 15: Loss:0.003, Val_loss:0.002\n",
      "Epoch 16: Loss:0.003, Val_loss:0.002\n",
      "Epoch 17: Loss:0.003, Val_loss:0.002\n",
      "Epoch 18: Loss:0.003, Val_loss:0.002\n",
      "Epoch 19: Loss:0.003, Val_loss:0.002\n",
      "Epoch 20: Loss:0.003, Val_loss:0.002\n",
      "Epoch 21: Loss:0.003, Val_loss:0.002\n",
      "Epoch 22: Loss:0.003, Val_loss:0.002\n",
      "Epoch 23: Loss:0.003, Val_loss:0.002\n",
      "Epoch 24: Loss:0.003, Val_loss:0.002\n",
      "Epoch 25: Loss:0.003, Val_loss:0.002\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c183d92f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "36\n",
      "Threshold:  0.217904188468\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/http/_1_5_30_para.ckpt\n",
      "--- Initialization time: 679.1038627624512 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
