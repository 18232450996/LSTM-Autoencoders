{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 5\n",
    "        self.step_num = 100\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp_.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp/log.txt\"\n",
    "        self.training_set_size = 100*200\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 19100 normal windows and 900 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:95,vn1:57,vn2:19,tn:20,va:4,ta:5\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.090, Val_loss:0.049\n",
      "Epoch 1: Loss:0.051, Val_loss:0.035\n",
      "Epoch 2: Loss:0.037, Val_loss:0.026\n",
      "Epoch 3: Loss:0.027, Val_loss:0.021\n",
      "Epoch 4: Loss:0.021, Val_loss:0.019\n",
      "Epoch 5: Loss:0.018, Val_loss:0.019\n",
      "Epoch 6: Loss:0.017, Val_loss:0.019\n",
      "Epoch 7: Loss:0.017, Val_loss:0.018\n",
      "Epoch 8: Loss:0.016, Val_loss:0.018\n",
      "Epoch 9: Loss:0.016, Val_loss:0.018\n",
      "Epoch 10: Loss:0.016, Val_loss:0.018\n",
      "Epoch 11: Loss:0.016, Val_loss:0.018\n",
      "Epoch 12: Loss:0.016, Val_loss:0.018\n",
      "Epoch 13: Loss:0.016, Val_loss:0.018\n",
      "Epoch 14: Loss:0.015, Val_loss:0.018\n",
      "Epoch 15: Loss:0.015, Val_loss:0.018\n",
      "Epoch 16: Loss:0.015, Val_loss:0.017\n",
      "Epoch 17: Loss:0.015, Val_loss:0.017\n",
      "Epoch 18: Loss:0.015, Val_loss:0.017\n",
      "Epoch 19: Loss:0.015, Val_loss:0.017\n",
      "Epoch 20: Loss:0.015, Val_loss:0.017\n",
      "Epoch 21: Loss:0.015, Val_loss:0.017\n",
      "Epoch 22: Loss:0.015, Val_loss:0.017\n",
      "Epoch 23: Loss:0.015, Val_loss:0.016\n",
      "Epoch 24: Loss:0.015, Val_loss:0.016\n",
      "Epoch 25: Loss:0.015, Val_loss:0.016\n",
      "Epoch 26: Loss:0.014, Val_loss:0.016\n",
      "Epoch 27: Loss:0.014, Val_loss:0.016\n",
      "Epoch 28: Loss:0.014, Val_loss:0.015\n",
      "Epoch 29: Loss:0.014, Val_loss:0.015\n",
      "Epoch 30: Loss:0.014, Val_loss:0.015\n",
      "Epoch 31: Loss:0.014, Val_loss:0.015\n",
      "Epoch 32: Loss:0.014, Val_loss:0.015\n",
      "Epoch 33: Loss:0.014, Val_loss:0.015\n",
      "Epoch 34: Loss:0.014, Val_loss:0.014\n",
      "Epoch 35: Loss:0.013, Val_loss:0.014\n",
      "Epoch 36: Loss:0.013, Val_loss:0.014\n",
      "Epoch 37: Loss:0.013, Val_loss:0.014\n",
      "Epoch 38: Loss:0.013, Val_loss:0.014\n",
      "Epoch 39: Loss:0.013, Val_loss:0.014\n",
      "Epoch 40: Loss:0.013, Val_loss:0.014\n",
      "Epoch 41: Loss:0.013, Val_loss:0.014\n",
      "Epoch 42: Loss:0.013, Val_loss:0.014\n",
      "Epoch 43: Loss:0.013, Val_loss:0.013\n",
      "Epoch 44: Loss:0.013, Val_loss:0.013\n",
      "Epoch 45: Loss:0.013, Val_loss:0.013\n",
      "Epoch 46: Loss:0.013, Val_loss:0.013\n",
      "Epoch 47: Loss:0.013, Val_loss:0.013\n",
      "Epoch 48: Loss:0.012, Val_loss:0.013\n",
      "Epoch 49: Loss:0.012, Val_loss:0.012\n",
      "Epoch 50: Loss:0.012, Val_loss:0.012\n",
      "Epoch 51: Loss:0.012, Val_loss:0.012\n",
      "Epoch 52: Loss:0.012, Val_loss:0.012\n",
      "Epoch 53: Loss:0.012, Val_loss:0.012\n",
      "Epoch 54: Loss:0.012, Val_loss:0.011\n",
      "Epoch 55: Loss:0.012, Val_loss:0.011\n",
      "Epoch 56: Loss:0.012, Val_loss:0.011\n",
      "Epoch 57: Loss:0.011, Val_loss:0.011\n",
      "Epoch 58: Loss:0.011, Val_loss:0.011\n",
      "Epoch 59: Loss:0.011, Val_loss:0.011\n",
      "Epoch 60: Loss:0.011, Val_loss:0.011\n",
      "Epoch 61: Loss:0.011, Val_loss:0.011\n",
      "Epoch 62: Loss:0.011, Val_loss:0.011\n",
      "Epoch 63: Loss:0.011, Val_loss:0.011\n",
      "Epoch 64: Loss:0.011, Val_loss:0.011\n",
      "Epoch 65: Loss:0.011, Val_loss:0.011\n",
      "Epoch 66: Loss:0.011, Val_loss:0.011\n",
      "Epoch 67: Loss:0.011, Val_loss:0.011\n",
      "Epoch 68: Loss:0.011, Val_loss:0.011\n",
      "Epoch 69: Loss:0.011, Val_loss:0.011\n",
      "Epoch 70: Loss:0.011, Val_loss:0.011\n",
      "Epoch 71: Loss:0.011, Val_loss:0.011\n",
      "Epoch 72: Loss:0.011, Val_loss:0.011\n",
      "Epoch 73: Loss:0.011, Val_loss:0.011\n",
      "Epoch 74: Loss:0.011, Val_loss:0.011\n",
      "Epoch 75: Loss:0.011, Val_loss:0.011\n",
      "Early stopping at epoch 75\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX0Skivx/cbwGwQARyCkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x208a8085470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "4\n",
      "Threshold:  0.0820248089529\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp/_1_5_100_para.ckpt\n",
      "--- Initialization time: 952.6255595684052 seconds ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0ee043e758a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mEncDecAD_Train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-cb5a7f532d82>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, training_data_source)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Early stopping at epoch %d\\n\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved accompany with parameters and threshold in file: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- Initialization time: %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
