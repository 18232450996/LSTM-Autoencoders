{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 84\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/power.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/power/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/power/log.txt\"\n",
    "        self.training_set_size = 84*12\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "        \n",
    "        # for univariate data, anomaly score is squared euclidean distance\n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "        #mu = np.mean(err_vec_array,axis=0)\n",
    "        if self.conf.elem_num == 1:  # univariate\n",
    "            mu = np.mean(err_vec_array.ravel())\n",
    "            sigma = np.var(err_vec_array.ravel())\n",
    "        #sigma = pd.DataFrame(err_vec_array.reshape(-1,self.conf.step_num)).cov()\n",
    "        else:\n",
    "            mu = 0\n",
    "            sigma = 0\n",
    "        print(\"Got parameters mu(%.3f) and sigma(%.3f).\"%(mu,sigma))\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = euclidean(mu,err_n[window,t,0])\n",
    "                            normal_score.append(s)\n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "#                            temp = np.dot((err_a[window,t,:] - mu[t,:] ) , sigma[t])\n",
    "#                            s = np.dot(temp,(err_a[window,t,:] - mu[t,:] ).T)\n",
    "                            s = euclidean(mu,err_a[window,t,0])\n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 924 normal windows and 84 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:5,vn1:3,vn2:1,tn:2,va:1,ta:1\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.222, Val_loss:0.132\n",
      "Epoch 1: Loss:0.150, Val_loss:0.091\n",
      "Epoch 2: Loss:0.105, Val_loss:0.069\n",
      "Epoch 3: Loss:0.081, Val_loss:0.063\n",
      "Epoch 4: Loss:0.073, Val_loss:0.067\n",
      "Epoch 5: Loss:0.074, Val_loss:0.070\n",
      "Epoch 6: Loss:0.074, Val_loss:0.066\n",
      "Epoch 7: Loss:0.071, Val_loss:0.061\n",
      "Epoch 8: Loss:0.068, Val_loss:0.058\n",
      "Epoch 9: Loss:0.066, Val_loss:0.055\n",
      "Epoch 10: Loss:0.065, Val_loss:0.054\n",
      "Epoch 11: Loss:0.064, Val_loss:0.054\n",
      "Epoch 12: Loss:0.062, Val_loss:0.053\n",
      "Epoch 13: Loss:0.061, Val_loss:0.053\n",
      "Epoch 14: Loss:0.060, Val_loss:0.052\n",
      "Epoch 15: Loss:0.059, Val_loss:0.050\n",
      "Epoch 16: Loss:0.057, Val_loss:0.048\n",
      "Epoch 17: Loss:0.056, Val_loss:0.047\n",
      "Epoch 18: Loss:0.055, Val_loss:0.047\n",
      "Epoch 19: Loss:0.054, Val_loss:0.046\n",
      "Epoch 20: Loss:0.053, Val_loss:0.045\n",
      "Epoch 21: Loss:0.052, Val_loss:0.044\n",
      "Epoch 22: Loss:0.051, Val_loss:0.043\n",
      "Epoch 23: Loss:0.050, Val_loss:0.043\n",
      "Epoch 24: Loss:0.049, Val_loss:0.042\n",
      "Epoch 25: Loss:0.048, Val_loss:0.041\n",
      "Epoch 26: Loss:0.046, Val_loss:0.040\n",
      "Epoch 27: Loss:0.045, Val_loss:0.039\n",
      "Epoch 28: Loss:0.044, Val_loss:0.038\n",
      "Epoch 29: Loss:0.042, Val_loss:0.037\n",
      "Epoch 30: Loss:0.041, Val_loss:0.035\n",
      "Epoch 31: Loss:0.039, Val_loss:0.034\n",
      "Epoch 32: Loss:0.038, Val_loss:0.033\n",
      "Epoch 33: Loss:0.036, Val_loss:0.031\n",
      "Epoch 34: Loss:0.034, Val_loss:0.030\n",
      "Epoch 35: Loss:0.032, Val_loss:0.028\n",
      "Epoch 36: Loss:0.030, Val_loss:0.026\n",
      "Epoch 37: Loss:0.028, Val_loss:0.024\n",
      "Epoch 38: Loss:0.026, Val_loss:0.022\n",
      "Epoch 39: Loss:0.023, Val_loss:0.020\n",
      "Epoch 40: Loss:0.021, Val_loss:0.018\n",
      "Epoch 41: Loss:0.018, Val_loss:0.017\n",
      "Epoch 42: Loss:0.016, Val_loss:0.015\n",
      "Epoch 43: Loss:0.014, Val_loss:0.014\n",
      "Epoch 44: Loss:0.012, Val_loss:0.013\n",
      "Epoch 45: Loss:0.012, Val_loss:0.013\n",
      "Epoch 46: Loss:0.011, Val_loss:0.013\n",
      "Epoch 47: Loss:0.011, Val_loss:0.013\n",
      "Epoch 48: Loss:0.011, Val_loss:0.013\n",
      "Epoch 49: Loss:0.011, Val_loss:0.013\n",
      "Epoch 50: Loss:0.011, Val_loss:0.013\n",
      "Epoch 51: Loss:0.011, Val_loss:0.013\n",
      "Epoch 52: Loss:0.011, Val_loss:0.013\n",
      "Epoch 53: Loss:0.011, Val_loss:0.012\n",
      "Epoch 54: Loss:0.011, Val_loss:0.012\n",
      "Epoch 55: Loss:0.011, Val_loss:0.012\n",
      "Epoch 56: Loss:0.011, Val_loss:0.012\n",
      "Epoch 57: Loss:0.011, Val_loss:0.012\n",
      "Epoch 58: Loss:0.011, Val_loss:0.012\n",
      "Epoch 59: Loss:0.011, Val_loss:0.012\n",
      "Epoch 60: Loss:0.010, Val_loss:0.012\n",
      "Epoch 61: Loss:0.010, Val_loss:0.012\n",
      "Epoch 62: Loss:0.010, Val_loss:0.012\n",
      "Epoch 63: Loss:0.010, Val_loss:0.012\n",
      "Epoch 64: Loss:0.010, Val_loss:0.012\n",
      "Epoch 65: Loss:0.010, Val_loss:0.012\n",
      "Epoch 66: Loss:0.010, Val_loss:0.012\n",
      "Early stopping at epoch 66\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e0c245080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu(0.082) and sigma(0.004).\n",
      "1\n",
      "Threshold:  0.232607498765\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/power/_1_15_84_para.ckpt\n",
      "--- Initialization time: 290.179181098938 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
