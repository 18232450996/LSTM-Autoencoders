{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path,header=None).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 25\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest_new.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_new_8_25_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_new_8_25_10/log.txt\"\n",
    "        self.training_set_size = self.step_num*15000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 50:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 144950 normal windows and 5050 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:7245,vn1:4347,vn2:1449,tn:1454,va:252,ta:253\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.015, Val_loss:0.012\n",
      "Epoch 1: Loss:0.008, Val_loss:0.011\n",
      "Epoch 2: Loss:0.007, Val_loss:0.007\n",
      "Epoch 3: Loss:0.006, Val_loss:0.006\n",
      "Epoch 4: Loss:0.005, Val_loss:0.006\n",
      "Epoch 5: Loss:0.005, Val_loss:0.005\n",
      "Epoch 6: Loss:0.005, Val_loss:0.006\n",
      "Epoch 7: Loss:0.004, Val_loss:0.005\n",
      "Epoch 8: Loss:0.004, Val_loss:0.005\n",
      "Epoch 9: Loss:0.004, Val_loss:0.004\n",
      "Epoch 10: Loss:0.004, Val_loss:0.004\n",
      "Epoch 11: Loss:0.004, Val_loss:0.004\n",
      "Epoch 12: Loss:0.003, Val_loss:0.004\n",
      "Epoch 13: Loss:0.003, Val_loss:0.004\n",
      "Epoch 14: Loss:0.003, Val_loss:0.003\n",
      "Epoch 15: Loss:0.003, Val_loss:0.003\n",
      "Epoch 16: Loss:0.003, Val_loss:0.003\n",
      "Epoch 17: Loss:0.003, Val_loss:0.003\n",
      "Epoch 18: Loss:0.003, Val_loss:0.003\n",
      "Epoch 19: Loss:0.003, Val_loss:0.003\n",
      "Epoch 20: Loss:0.003, Val_loss:0.003\n",
      "Epoch 21: Loss:0.003, Val_loss:0.003\n",
      "Epoch 22: Loss:0.003, Val_loss:0.003\n",
      "Epoch 23: Loss:0.003, Val_loss:0.003\n",
      "Epoch 24: Loss:0.003, Val_loss:0.003\n",
      "Epoch 25: Loss:0.003, Val_loss:0.003\n",
      "Epoch 26: Loss:0.003, Val_loss:0.003\n",
      "Epoch 27: Loss:0.002, Val_loss:0.003\n",
      "Epoch 28: Loss:0.002, Val_loss:0.003\n",
      "Epoch 29: Loss:0.002, Val_loss:0.003\n",
      "Epoch 30: Loss:0.002, Val_loss:0.003\n",
      "Epoch 31: Loss:0.002, Val_loss:0.003\n",
      "Epoch 32: Loss:0.002, Val_loss:0.003\n",
      "Epoch 33: Loss:0.002, Val_loss:0.003\n",
      "Epoch 34: Loss:0.002, Val_loss:0.003\n",
      "Epoch 35: Loss:0.002, Val_loss:0.003\n",
      "Epoch 36: Loss:0.002, Val_loss:0.003\n",
      "Epoch 37: Loss:0.002, Val_loss:0.002\n",
      "Epoch 38: Loss:0.002, Val_loss:0.002\n",
      "Epoch 39: Loss:0.002, Val_loss:0.002\n",
      "Epoch 40: Loss:0.002, Val_loss:0.002\n",
      "Epoch 41: Loss:0.002, Val_loss:0.003\n",
      "Epoch 42: Loss:0.002, Val_loss:0.003\n",
      "Epoch 43: Loss:0.002, Val_loss:0.002\n",
      "Epoch 44: Loss:0.002, Val_loss:0.003\n",
      "Epoch 45: Loss:0.002, Val_loss:0.003\n",
      "Epoch 46: Loss:0.002, Val_loss:0.003\n",
      "Epoch 47: Loss:0.002, Val_loss:0.002\n",
      "Epoch 48: Loss:0.002, Val_loss:0.002\n",
      "Epoch 49: Loss:0.002, Val_loss:0.002\n",
      "Epoch 50: Loss:0.002, Val_loss:0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19418d824e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "252\n",
      "Threshold:  0.00278209399292\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_new_8_25_10/_8_25_10_para.ckpt\n",
      "--- Initialization time: 514.6935343742371 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
