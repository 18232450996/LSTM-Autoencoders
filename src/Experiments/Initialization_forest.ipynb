{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_20win/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_20win/log.txt\"\n",
    "        self.training_set_size = 100*500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 44920 normal windows and 5080 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:2245,vn1:1347,vn2:449,tn:451,va:254,ta:254\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.019, Val_loss:0.025\n",
      "Epoch 1: Loss:0.014, Val_loss:0.015\n",
      "Epoch 2: Loss:0.011, Val_loss:0.015\n",
      "Epoch 3: Loss:0.010, Val_loss:0.012\n",
      "Epoch 4: Loss:0.009, Val_loss:0.010\n",
      "Epoch 5: Loss:0.008, Val_loss:0.008\n",
      "Epoch 6: Loss:0.007, Val_loss:0.007\n",
      "Epoch 7: Loss:0.007, Val_loss:0.007\n",
      "Epoch 8: Loss:0.007, Val_loss:0.008\n",
      "Epoch 9: Loss:0.006, Val_loss:0.008\n",
      "Epoch 10: Loss:0.006, Val_loss:0.008\n",
      "Epoch 11: Loss:0.006, Val_loss:0.008\n",
      "Epoch 12: Loss:0.005, Val_loss:0.008\n",
      "Epoch 13: Loss:0.005, Val_loss:0.007\n",
      "Epoch 14: Loss:0.005, Val_loss:0.007\n",
      "Epoch 15: Loss:0.005, Val_loss:0.007\n",
      "Epoch 16: Loss:0.005, Val_loss:0.007\n",
      "Epoch 17: Loss:0.004, Val_loss:0.007\n",
      "Epoch 18: Loss:0.004, Val_loss:0.006\n",
      "Epoch 19: Loss:0.004, Val_loss:0.006\n",
      "Epoch 20: Loss:0.004, Val_loss:0.005\n",
      "Epoch 21: Loss:0.004, Val_loss:0.005\n",
      "Epoch 22: Loss:0.004, Val_loss:0.004\n",
      "Epoch 23: Loss:0.004, Val_loss:0.004\n",
      "Epoch 24: Loss:0.004, Val_loss:0.004\n",
      "Epoch 25: Loss:0.004, Val_loss:0.004\n",
      "Epoch 26: Loss:0.004, Val_loss:0.004\n",
      "Epoch 27: Loss:0.004, Val_loss:0.004\n",
      "Epoch 28: Loss:0.003, Val_loss:0.004\n",
      "Epoch 29: Loss:0.003, Val_loss:0.004\n",
      "Epoch 30: Loss:0.003, Val_loss:0.004\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a6a8f3fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "254\n",
      "Threshold:  0.00346383108214\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_10win/_1_45_10_para.ckpt\n",
      "--- Initialization time: 411.5638737678528 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
