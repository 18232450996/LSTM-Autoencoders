{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 35\n",
    "        self.step_num = 100\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest/log.txt\"\n",
    "        self.training_set_size = 100*500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 44000 normal windows and 6000 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:220,vn1:132,vn2:44,tn:44,va:30,ta:30\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.045, Val_loss:0.024\n",
      "Epoch 1: Loss:0.022, Val_loss:0.017\n",
      "Epoch 2: Loss:0.020, Val_loss:0.014\n",
      "Epoch 3: Loss:0.019, Val_loss:0.013\n",
      "Epoch 4: Loss:0.018, Val_loss:0.012\n",
      "Epoch 5: Loss:0.018, Val_loss:0.012\n",
      "Epoch 6: Loss:0.018, Val_loss:0.012\n",
      "Epoch 7: Loss:0.018, Val_loss:0.012\n",
      "Epoch 8: Loss:0.017, Val_loss:0.012\n",
      "Epoch 9: Loss:0.017, Val_loss:0.012\n",
      "Epoch 10: Loss:0.017, Val_loss:0.012\n",
      "Epoch 11: Loss:0.017, Val_loss:0.012\n",
      "Epoch 12: Loss:0.017, Val_loss:0.012\n",
      "Epoch 13: Loss:0.017, Val_loss:0.012\n",
      "Epoch 14: Loss:0.017, Val_loss:0.012\n",
      "Epoch 15: Loss:0.017, Val_loss:0.012\n",
      "Epoch 16: Loss:0.017, Val_loss:0.012\n",
      "Epoch 17: Loss:0.017, Val_loss:0.012\n",
      "Epoch 18: Loss:0.017, Val_loss:0.012\n",
      "Epoch 19: Loss:0.017, Val_loss:0.012\n",
      "Epoch 20: Loss:0.017, Val_loss:0.012\n",
      "Epoch 21: Loss:0.017, Val_loss:0.012\n",
      "Epoch 22: Loss:0.017, Val_loss:0.012\n",
      "Epoch 23: Loss:0.017, Val_loss:0.012\n",
      "Epoch 24: Loss:0.017, Val_loss:0.012\n",
      "Epoch 25: Loss:0.017, Val_loss:0.012\n",
      "Epoch 26: Loss:0.017, Val_loss:0.011\n",
      "Epoch 27: Loss:0.016, Val_loss:0.011\n",
      "Epoch 28: Loss:0.016, Val_loss:0.011\n",
      "Epoch 29: Loss:0.016, Val_loss:0.011\n",
      "Epoch 30: Loss:0.016, Val_loss:0.011\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e9487eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "30\n",
      "Threshold:  0.0162247622805\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest/_1_35_100_para.ckpt\n",
      "--- Initialization time: 598.9398469924927 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
