{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_20win/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_20win/log.txt\"\n",
    "        self.training_set_size = 100*500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 44320 normal windows and 5680 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:1105,vn1:663,vn2:221,tn:227,va:142,ta:142\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.024, Val_loss:0.020\n",
      "Epoch 1: Loss:0.018, Val_loss:0.016\n",
      "Epoch 2: Loss:0.016, Val_loss:0.015\n",
      "Epoch 3: Loss:0.015, Val_loss:0.014\n",
      "Epoch 4: Loss:0.014, Val_loss:0.013\n",
      "Epoch 5: Loss:0.013, Val_loss:0.011\n",
      "Epoch 6: Loss:0.013, Val_loss:0.010\n",
      "Epoch 7: Loss:0.012, Val_loss:0.010\n",
      "Epoch 8: Loss:0.012, Val_loss:0.009\n",
      "Epoch 9: Loss:0.011, Val_loss:0.009\n",
      "Epoch 10: Loss:0.011, Val_loss:0.009\n",
      "Epoch 11: Loss:0.011, Val_loss:0.009\n",
      "Epoch 12: Loss:0.011, Val_loss:0.008\n",
      "Epoch 13: Loss:0.011, Val_loss:0.008\n",
      "Epoch 14: Loss:0.010, Val_loss:0.008\n",
      "Epoch 15: Loss:0.010, Val_loss:0.008\n",
      "Epoch 16: Loss:0.010, Val_loss:0.008\n",
      "Epoch 17: Loss:0.010, Val_loss:0.008\n",
      "Epoch 18: Loss:0.010, Val_loss:0.008\n",
      "Epoch 19: Loss:0.010, Val_loss:0.008\n",
      "Epoch 20: Loss:0.009, Val_loss:0.008\n",
      "Epoch 21: Loss:0.009, Val_loss:0.007\n",
      "Epoch 22: Loss:0.009, Val_loss:0.007\n",
      "Epoch 23: Loss:0.009, Val_loss:0.008\n",
      "Epoch 24: Loss:0.009, Val_loss:0.008\n",
      "Epoch 25: Loss:0.009, Val_loss:0.008\n",
      "Epoch 26: Loss:0.009, Val_loss:0.008\n",
      "Epoch 27: Loss:0.008, Val_loss:0.008\n",
      "Epoch 28: Loss:0.008, Val_loss:0.009\n",
      "Epoch 29: Loss:0.008, Val_loss:0.009\n",
      "Epoch 30: Loss:0.008, Val_loss:0.009\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee856d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "142\n",
      "Threshold:  0.00811583305025\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_20win/_1_45_20_para.ckpt\n",
      "--- Initialization time: 408.0278820991516 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
