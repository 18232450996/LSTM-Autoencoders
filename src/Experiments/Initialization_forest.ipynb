{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path,header=None).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 25\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/log.txt\"\n",
    "        self.training_set_size = self.step_num*20000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 194950 normal windows and 5050 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:9745,vn1:5847,vn2:1949,tn:1954,va:252,ta:253\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.014, Val_loss:0.010\n",
      "Epoch 1: Loss:0.007, Val_loss:0.008\n",
      "Epoch 2: Loss:0.006, Val_loss:0.007\n",
      "Epoch 3: Loss:0.005, Val_loss:0.006\n",
      "Epoch 4: Loss:0.005, Val_loss:0.007\n",
      "Epoch 5: Loss:0.004, Val_loss:0.006\n",
      "Epoch 6: Loss:0.004, Val_loss:0.007\n",
      "Epoch 7: Loss:0.004, Val_loss:0.006\n",
      "Epoch 8: Loss:0.004, Val_loss:0.006\n",
      "Epoch 9: Loss:0.004, Val_loss:0.005\n",
      "Epoch 10: Loss:0.003, Val_loss:0.005\n",
      "Epoch 11: Loss:0.003, Val_loss:0.004\n",
      "Epoch 12: Loss:0.003, Val_loss:0.004\n",
      "Epoch 13: Loss:0.003, Val_loss:0.004\n",
      "Epoch 14: Loss:0.003, Val_loss:0.004\n",
      "Epoch 15: Loss:0.003, Val_loss:0.005\n",
      "Epoch 16: Loss:0.003, Val_loss:0.005\n",
      "Epoch 17: Loss:0.003, Val_loss:0.005\n",
      "Epoch 18: Loss:0.003, Val_loss:0.004\n",
      "Epoch 19: Loss:0.003, Val_loss:0.004\n",
      "Epoch 20: Loss:0.003, Val_loss:0.004\n",
      "Epoch 21: Loss:0.003, Val_loss:0.004\n",
      "Epoch 22: Loss:0.002, Val_loss:0.004\n",
      "Epoch 23: Loss:0.002, Val_loss:0.004\n",
      "Epoch 24: Loss:0.002, Val_loss:0.004\n",
      "Epoch 25: Loss:0.002, Val_loss:0.004\n",
      "Epoch 26: Loss:0.002, Val_loss:0.004\n",
      "Epoch 27: Loss:0.002, Val_loss:0.004\n",
      "Epoch 28: Loss:0.002, Val_loss:0.004\n",
      "Epoch 29: Loss:0.002, Val_loss:0.004\n",
      "Epoch 30: Loss:0.002, Val_loss:0.004\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQDDGGANYQDDGGOOygGCMMQaA/w8TOuaJ+CDSYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2198a331ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "252\n",
      "Threshold:  0.00652888151127\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/_8_25_10_para.ckpt\n",
      "--- Initialization time: 282.61270546913147 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
