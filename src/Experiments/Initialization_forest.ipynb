{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path,header=None).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 25\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest/type1train.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/log.txt\"\n",
    "        self.training_set_size = self.step_num*1100\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 10000 normal windows and 1000 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:500,vn1:300,vn2:100,tn:100,va:50,ta:50\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.085, Val_loss:0.035\n",
      "Epoch 1: Loss:0.044, Val_loss:0.029\n",
      "Epoch 2: Loss:0.035, Val_loss:0.026\n",
      "Epoch 3: Loss:0.030, Val_loss:0.025\n",
      "Epoch 4: Loss:0.028, Val_loss:0.022\n",
      "Epoch 5: Loss:0.026, Val_loss:0.019\n",
      "Epoch 6: Loss:0.025, Val_loss:0.017\n",
      "Epoch 7: Loss:0.024, Val_loss:0.016\n",
      "Epoch 8: Loss:0.023, Val_loss:0.014\n",
      "Epoch 9: Loss:0.023, Val_loss:0.013\n",
      "Epoch 10: Loss:0.022, Val_loss:0.013\n",
      "Epoch 11: Loss:0.022, Val_loss:0.012\n",
      "Epoch 12: Loss:0.021, Val_loss:0.012\n",
      "Epoch 13: Loss:0.021, Val_loss:0.011\n",
      "Epoch 14: Loss:0.021, Val_loss:0.011\n",
      "Epoch 15: Loss:0.020, Val_loss:0.010\n",
      "Epoch 16: Loss:0.020, Val_loss:0.010\n",
      "Epoch 17: Loss:0.020, Val_loss:0.010\n",
      "Epoch 18: Loss:0.019, Val_loss:0.010\n",
      "Epoch 19: Loss:0.019, Val_loss:0.009\n",
      "Epoch 20: Loss:0.018, Val_loss:0.009\n",
      "Epoch 21: Loss:0.018, Val_loss:0.009\n",
      "Epoch 22: Loss:0.018, Val_loss:0.009\n",
      "Epoch 23: Loss:0.018, Val_loss:0.009\n",
      "Epoch 24: Loss:0.018, Val_loss:0.009\n",
      "Epoch 25: Loss:0.017, Val_loss:0.008\n",
      "Epoch 26: Loss:0.017, Val_loss:0.008\n",
      "Epoch 27: Loss:0.017, Val_loss:0.008\n",
      "Epoch 28: Loss:0.017, Val_loss:0.008\n",
      "Epoch 29: Loss:0.017, Val_loss:0.008\n",
      "Epoch 30: Loss:0.017, Val_loss:0.008\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c6881e65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "50\n",
      "Threshold:  0.014811477358\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_8_25_10/_8_25_10_para.ckpt\n",
      "--- Initialization time: 35.36162853240967 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
