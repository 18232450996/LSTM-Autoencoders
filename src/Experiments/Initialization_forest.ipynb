{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path,header=None).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 25\n",
    "        self.step_num = 10\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest/forest_cbyc.csv\"#\"C:/Users/Bin/Desktop/Thesis/dataset/forest_new.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_cbyc1_8_25_10/\"#\"C:/Users/Bin/Desktop/Thesis/models/forest_new_8_25_10/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_cbyc1_8_25_10/log.txt\"#\"C:/Users/Bin/Desktop/Thesis/models/forest_new_8_25_10/log.txt\"\n",
    "        self.training_set_size = self.step_num*10000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 50:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 99390 normal windows and 610 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:4965,vn1:2979,vn2:993,tn:1002,va:30,ta:31\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.017, Val_loss:0.011\n",
      "Epoch 1: Loss:0.009, Val_loss:0.008\n",
      "Epoch 2: Loss:0.008, Val_loss:0.007\n",
      "Epoch 3: Loss:0.007, Val_loss:0.008\n",
      "Epoch 4: Loss:0.006, Val_loss:0.007\n",
      "Epoch 5: Loss:0.006, Val_loss:0.007\n",
      "Epoch 6: Loss:0.005, Val_loss:0.006\n",
      "Epoch 7: Loss:0.005, Val_loss:0.006\n",
      "Epoch 8: Loss:0.005, Val_loss:0.006\n",
      "Epoch 9: Loss:0.005, Val_loss:0.005\n",
      "Epoch 10: Loss:0.004, Val_loss:0.005\n",
      "Epoch 11: Loss:0.004, Val_loss:0.005\n",
      "Epoch 12: Loss:0.004, Val_loss:0.005\n",
      "Epoch 13: Loss:0.004, Val_loss:0.005\n",
      "Epoch 14: Loss:0.004, Val_loss:0.005\n",
      "Epoch 15: Loss:0.004, Val_loss:0.005\n",
      "Epoch 16: Loss:0.004, Val_loss:0.005\n",
      "Epoch 17: Loss:0.003, Val_loss:0.005\n",
      "Epoch 18: Loss:0.003, Val_loss:0.005\n",
      "Epoch 19: Loss:0.003, Val_loss:0.004\n",
      "Epoch 20: Loss:0.003, Val_loss:0.004\n",
      "Epoch 21: Loss:0.003, Val_loss:0.004\n",
      "Epoch 22: Loss:0.003, Val_loss:0.004\n",
      "Epoch 23: Loss:0.003, Val_loss:0.004\n",
      "Epoch 24: Loss:0.003, Val_loss:0.004\n",
      "Epoch 25: Loss:0.003, Val_loss:0.004\n",
      "Epoch 26: Loss:0.003, Val_loss:0.004\n",
      "Epoch 27: Loss:0.003, Val_loss:0.004\n",
      "Epoch 28: Loss:0.003, Val_loss:0.004\n",
      "Epoch 29: Loss:0.003, Val_loss:0.004\n",
      "Epoch 30: Loss:0.003, Val_loss:0.004\n",
      "Epoch 31: Loss:0.003, Val_loss:0.004\n",
      "Epoch 32: Loss:0.003, Val_loss:0.004\n",
      "Epoch 33: Loss:0.003, Val_loss:0.004\n",
      "Epoch 34: Loss:0.002, Val_loss:0.004\n",
      "Epoch 35: Loss:0.003, Val_loss:0.004\n",
      "Epoch 36: Loss:0.002, Val_loss:0.004\n",
      "Epoch 37: Loss:0.002, Val_loss:0.004\n",
      "Epoch 38: Loss:0.002, Val_loss:0.004\n",
      "Epoch 39: Loss:0.002, Val_loss:0.004\n",
      "Epoch 40: Loss:0.002, Val_loss:0.004\n",
      "Epoch 41: Loss:0.002, Val_loss:0.004\n",
      "Epoch 42: Loss:0.002, Val_loss:0.004\n",
      "Epoch 43: Loss:0.002, Val_loss:0.004\n",
      "Epoch 44: Loss:0.002, Val_loss:0.004\n",
      "Epoch 45: Loss:0.002, Val_loss:0.004\n",
      "Epoch 46: Loss:0.002, Val_loss:0.003\n",
      "Epoch 47: Loss:0.002, Val_loss:0.003\n",
      "Epoch 48: Loss:0.002, Val_loss:0.003\n",
      "Epoch 49: Loss:0.002, Val_loss:0.003\n",
      "Epoch 50: Loss:0.002, Val_loss:0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17409e1d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "30\n",
      "Threshold:  0.00474004425589\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_cbyc1_8_25_10/_8_25_10_para.ckpt\n",
      "--- Initialization time: 199.34094142913818 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
