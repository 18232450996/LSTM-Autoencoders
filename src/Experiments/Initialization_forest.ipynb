{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/forest.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_45_20/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/forest_8_45_20/log.txt\"\n",
    "        self.training_set_size = self.step_num*10000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 194320 normal windows and 5680 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:4855,vn1:2913,vn2:971,tn:977,va:142,ta:142\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.017, Val_loss:0.014\n",
      "Epoch 1: Loss:0.010, Val_loss:0.011\n",
      "Epoch 2: Loss:0.008, Val_loss:0.008\n",
      "Epoch 3: Loss:0.007, Val_loss:0.009\n",
      "Epoch 4: Loss:0.007, Val_loss:0.007\n",
      "Epoch 5: Loss:0.006, Val_loss:0.007\n",
      "Epoch 6: Loss:0.006, Val_loss:0.007\n",
      "Epoch 7: Loss:0.005, Val_loss:0.007\n",
      "Epoch 8: Loss:0.005, Val_loss:0.010\n",
      "Epoch 9: Loss:0.005, Val_loss:0.005\n",
      "Epoch 10: Loss:0.005, Val_loss:0.006\n",
      "Epoch 11: Loss:0.005, Val_loss:0.007\n",
      "Epoch 12: Loss:0.004, Val_loss:0.005\n",
      "Epoch 13: Loss:0.004, Val_loss:0.005\n",
      "Epoch 14: Loss:0.004, Val_loss:0.005\n",
      "Epoch 15: Loss:0.004, Val_loss:0.006\n",
      "Epoch 16: Loss:0.004, Val_loss:0.005\n",
      "Epoch 17: Loss:0.004, Val_loss:0.005\n",
      "Epoch 18: Loss:0.004, Val_loss:0.005\n",
      "Epoch 19: Loss:0.004, Val_loss:0.005\n",
      "Epoch 20: Loss:0.004, Val_loss:0.005\n",
      "Epoch 21: Loss:0.004, Val_loss:0.005\n",
      "Epoch 22: Loss:0.004, Val_loss:0.005\n",
      "Epoch 23: Loss:0.004, Val_loss:0.005\n",
      "Epoch 24: Loss:0.004, Val_loss:0.005\n",
      "Epoch 25: Loss:0.004, Val_loss:0.006\n",
      "Epoch 26: Loss:0.004, Val_loss:0.005\n",
      "Epoch 27: Loss:0.003, Val_loss:0.005\n",
      "Epoch 28: Loss:0.003, Val_loss:0.005\n",
      "Epoch 29: Loss:0.003, Val_loss:0.006\n",
      "Epoch 30: Loss:0.003, Val_loss:0.005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231a3472da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "142\n",
      "Threshold:  0.00636553645853\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/forest_8_45_20/_8_45_20_para.ckpt\n",
      "--- Initialization time: 289.83633375167847 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
