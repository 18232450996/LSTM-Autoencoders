{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 140\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/ecg.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/ecg/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/ecg/log.txt\"\n",
    "        self.training_set_size = self.step_num * 500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "        \n",
    "        # for univariate data, anomaly score is squared euclidean distance\n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array.ravel())\n",
    "        sigma = np.var(err_vec_array.ravel())\n",
    "\n",
    "        print(\"Got parameters mu(%.3f) and sigma(%.3f).\"%(mu,sigma))\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_n[window,t],mu,sigma)\n",
    "                            normal_score.append(s)\n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "#                            temp = np.dot((err_a[window,t,:] - mu[t,:] ) , sigma[t])\n",
    "#                            s = np.dot(temp,(err_a[window,t,:] - mu[t,:] ).T)\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "                  \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 25:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 69580 normal windows and 420 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:245,vn1:147,vn2:49,tn:56,va:1,ta:2\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.003, Val_loss:0.003\n",
      "Epoch 1: Loss:0.002, Val_loss:0.002\n",
      "Epoch 2: Loss:0.001, Val_loss:0.002\n",
      "Epoch 3: Loss:0.001, Val_loss:0.001\n",
      "Epoch 4: Loss:0.001, Val_loss:0.001\n",
      "Epoch 5: Loss:0.001, Val_loss:0.001\n",
      "Epoch 6: Loss:0.001, Val_loss:0.001\n",
      "Epoch 7: Loss:0.001, Val_loss:0.001\n",
      "Epoch 8: Loss:0.001, Val_loss:0.001\n",
      "Epoch 9: Loss:0.000, Val_loss:0.001\n",
      "Epoch 10: Loss:0.000, Val_loss:0.000\n",
      "Epoch 11: Loss:0.000, Val_loss:0.000\n",
      "Epoch 12: Loss:0.000, Val_loss:0.000\n",
      "Epoch 13: Loss:0.000, Val_loss:0.000\n",
      "Epoch 14: Loss:0.000, Val_loss:0.000\n",
      "Epoch 15: Loss:0.000, Val_loss:0.000\n",
      "Epoch 16: Loss:0.000, Val_loss:0.000\n",
      "Epoch 17: Loss:0.000, Val_loss:0.000\n",
      "Epoch 18: Loss:0.000, Val_loss:0.000\n",
      "Epoch 19: Loss:0.000, Val_loss:0.000\n",
      "Epoch 20: Loss:0.000, Val_loss:0.000\n",
      "Epoch 21: Loss:0.000, Val_loss:0.000\n",
      "Epoch 22: Loss:0.000, Val_loss:0.000\n",
      "Epoch 23: Loss:0.000, Val_loss:0.000\n",
      "Epoch 24: Loss:0.000, Val_loss:0.000\n",
      "Epoch 25: Loss:0.000, Val_loss:0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+8eFQs969oMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184cd28d320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu(0.013) and sigma(0.000).\n",
      "1\n",
      "Threshold:  0.000661553183454\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/ecg/_1_45_140_para.ckpt\n",
      "--- Initialization time: 787.4569752216339 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
