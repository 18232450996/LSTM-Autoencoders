{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 3\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_3/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_3/log.txt\"\n",
    "        self.training_set_size = self.step_num*100000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 100:\n",
    "                    break\n",
    "\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 95304 normal windows and 1248 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:15880,vn1:9528,vn2:3176,tn:3184,va:208,ta:208\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.011, Val_loss:0.006\n",
      "Epoch 1: Loss:0.003, Val_loss:0.003\n",
      "Epoch 2: Loss:0.002, Val_loss:0.002\n",
      "Epoch 3: Loss:0.002, Val_loss:0.002\n",
      "Epoch 4: Loss:0.001, Val_loss:0.002\n",
      "Epoch 5: Loss:0.001, Val_loss:0.002\n",
      "Epoch 6: Loss:0.001, Val_loss:0.001\n",
      "Epoch 7: Loss:0.001, Val_loss:0.001\n",
      "Epoch 8: Loss:0.001, Val_loss:0.001\n",
      "Epoch 9: Loss:0.001, Val_loss:0.001\n",
      "Epoch 10: Loss:0.001, Val_loss:0.001\n",
      "Epoch 11: Loss:0.001, Val_loss:0.001\n",
      "Epoch 12: Loss:0.001, Val_loss:0.001\n",
      "Epoch 13: Loss:0.001, Val_loss:0.001\n",
      "Epoch 14: Loss:0.001, Val_loss:0.001\n",
      "Epoch 15: Loss:0.000, Val_loss:0.001\n",
      "Epoch 16: Loss:0.000, Val_loss:0.001\n",
      "Epoch 17: Loss:0.000, Val_loss:0.001\n",
      "Epoch 18: Loss:0.000, Val_loss:0.001\n",
      "Epoch 19: Loss:0.000, Val_loss:0.001\n",
      "Epoch 20: Loss:0.000, Val_loss:0.001\n",
      "Epoch 21: Loss:0.000, Val_loss:0.001\n",
      "Epoch 22: Loss:0.000, Val_loss:0.001\n",
      "Epoch 23: Loss:0.000, Val_loss:0.001\n",
      "Epoch 24: Loss:0.000, Val_loss:0.001\n",
      "Epoch 25: Loss:0.000, Val_loss:0.001\n",
      "Epoch 26: Loss:0.000, Val_loss:0.001\n",
      "Epoch 27: Loss:0.000, Val_loss:0.001\n",
      "Epoch 28: Loss:0.000, Val_loss:0.001\n",
      "Epoch 29: Loss:0.000, Val_loss:0.001\n",
      "Epoch 30: Loss:0.000, Val_loss:0.001\n",
      "Epoch 31: Loss:0.000, Val_loss:0.001\n",
      "Epoch 32: Loss:0.000, Val_loss:0.001\n",
      "Epoch 33: Loss:0.000, Val_loss:0.001\n",
      "Epoch 34: Loss:0.000, Val_loss:0.001\n",
      "Epoch 35: Loss:0.000, Val_loss:0.001\n",
      "Epoch 36: Loss:0.000, Val_loss:0.001\n",
      "Epoch 37: Loss:0.000, Val_loss:0.001\n",
      "Epoch 38: Loss:0.000, Val_loss:0.001\n",
      "Epoch 39: Loss:0.000, Val_loss:0.001\n",
      "Epoch 40: Loss:0.000, Val_loss:0.001\n",
      "Epoch 41: Loss:0.000, Val_loss:0.001\n",
      "Epoch 42: Loss:0.000, Val_loss:0.001\n",
      "Epoch 43: Loss:0.000, Val_loss:0.001\n",
      "Epoch 44: Loss:0.000, Val_loss:0.001\n",
      "Epoch 45: Loss:0.000, Val_loss:0.001\n",
      "Epoch 46: Loss:0.000, Val_loss:0.001\n",
      "Epoch 47: Loss:0.000, Val_loss:0.001\n",
      "Epoch 48: Loss:0.000, Val_loss:0.001\n",
      "Epoch 49: Loss:0.000, Val_loss:0.001\n",
      "Epoch 50: Loss:0.000, Val_loss:0.001\n",
      "Epoch 51: Loss:0.000, Val_loss:0.001\n",
      "Epoch 52: Loss:0.000, Val_loss:0.001\n",
      "Epoch 53: Loss:0.000, Val_loss:0.001\n",
      "Epoch 54: Loss:0.000, Val_loss:0.001\n",
      "Epoch 55: Loss:0.000, Val_loss:0.001\n",
      "Epoch 56: Loss:0.000, Val_loss:0.000\n",
      "Epoch 57: Loss:0.000, Val_loss:0.001\n",
      "Epoch 58: Loss:0.000, Val_loss:0.001\n",
      "Epoch 59: Loss:0.000, Val_loss:0.000\n",
      "Epoch 60: Loss:0.000, Val_loss:0.000\n",
      "Epoch 61: Loss:0.000, Val_loss:0.000\n",
      "Epoch 62: Loss:0.000, Val_loss:0.000\n",
      "Epoch 63: Loss:0.000, Val_loss:0.000\n",
      "Epoch 64: Loss:0.000, Val_loss:0.001\n",
      "Epoch 65: Loss:0.000, Val_loss:0.000\n",
      "Epoch 66: Loss:0.000, Val_loss:0.000\n",
      "Epoch 67: Loss:0.000, Val_loss:0.001\n",
      "Epoch 68: Loss:0.000, Val_loss:0.001\n",
      "Epoch 69: Loss:0.000, Val_loss:0.001\n",
      "Epoch 70: Loss:0.000, Val_loss:0.001\n",
      "Epoch 71: Loss:0.000, Val_loss:0.001\n",
      "Epoch 72: Loss:0.000, Val_loss:0.001\n",
      "Epoch 73: Loss:0.000, Val_loss:0.001\n",
      "Epoch 74: Loss:0.000, Val_loss:0.001\n",
      "Epoch 75: Loss:0.000, Val_loss:0.001\n",
      "Epoch 76: Loss:0.000, Val_loss:0.001\n",
      "Epoch 77: Loss:0.000, Val_loss:0.001\n",
      "Epoch 78: Loss:0.000, Val_loss:0.001\n",
      "Epoch 79: Loss:0.000, Val_loss:0.001\n",
      "Epoch 80: Loss:0.000, Val_loss:0.001\n",
      "Epoch 81: Loss:0.000, Val_loss:0.001\n",
      "Epoch 82: Loss:0.000, Val_loss:0.001\n",
      "Epoch 83: Loss:0.000, Val_loss:0.001\n",
      "Epoch 84: Loss:0.000, Val_loss:0.001\n",
      "Epoch 85: Loss:0.000, Val_loss:0.001\n",
      "Epoch 86: Loss:0.000, Val_loss:0.001\n",
      "Epoch 87: Loss:0.000, Val_loss:0.001\n",
      "Epoch 88: Loss:0.000, Val_loss:0.001\n",
      "Epoch 89: Loss:0.000, Val_loss:0.001\n",
      "Epoch 90: Loss:0.000, Val_loss:0.000\n",
      "Epoch 91: Loss:0.000, Val_loss:0.000\n",
      "Epoch 92: Loss:0.000, Val_loss:0.000\n",
      "Epoch 93: Loss:0.000, Val_loss:0.000\n",
      "Epoch 94: Loss:0.000, Val_loss:0.000\n",
      "Epoch 95: Loss:0.000, Val_loss:0.000\n",
      "Epoch 96: Loss:0.000, Val_loss:0.000\n",
      "Epoch 97: Loss:0.000, Val_loss:0.000\n",
      "Epoch 98: Loss:0.000, Val_loss:0.000\n",
      "Epoch 99: Loss:0.000, Val_loss:0.000\n",
      "Epoch 100: Loss:0.000, Val_loss:0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cad32a0908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "208\n",
      "Threshold:  0.0235452085682\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_8_15_3/_8_15_3_para.ckpt\n",
      "--- Initialization time: 550.6756811141968 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
