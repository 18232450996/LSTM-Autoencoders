{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 8\n",
    "        self.hidden_num = 35\n",
    "        self.step_num = 30\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_35_30/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_8_35_30/log.txt\"\n",
    "        self.training_set_size = self.step_num*2000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 100:\n",
    "                    break\n",
    "\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 58920 normal windows and 1080 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:980,vn1:588,vn2:196,tn:200,va:18,ta:18\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.078, Val_loss:0.024\n",
      "Epoch 1: Loss:0.018, Val_loss:0.017\n",
      "Epoch 2: Loss:0.015, Val_loss:0.015\n",
      "Epoch 3: Loss:0.013, Val_loss:0.013\n",
      "Epoch 4: Loss:0.012, Val_loss:0.012\n",
      "Epoch 5: Loss:0.011, Val_loss:0.012\n",
      "Epoch 6: Loss:0.011, Val_loss:0.011\n",
      "Epoch 7: Loss:0.010, Val_loss:0.011\n",
      "Epoch 8: Loss:0.010, Val_loss:0.010\n",
      "Epoch 9: Loss:0.010, Val_loss:0.010\n",
      "Epoch 10: Loss:0.010, Val_loss:0.010\n",
      "Epoch 11: Loss:0.010, Val_loss:0.010\n",
      "Epoch 12: Loss:0.010, Val_loss:0.010\n",
      "Epoch 13: Loss:0.009, Val_loss:0.010\n",
      "Epoch 14: Loss:0.009, Val_loss:0.010\n",
      "Epoch 15: Loss:0.009, Val_loss:0.010\n",
      "Epoch 16: Loss:0.009, Val_loss:0.010\n",
      "Epoch 17: Loss:0.009, Val_loss:0.010\n",
      "Epoch 18: Loss:0.009, Val_loss:0.009\n",
      "Epoch 19: Loss:0.009, Val_loss:0.009\n",
      "Epoch 20: Loss:0.009, Val_loss:0.009\n",
      "Epoch 21: Loss:0.009, Val_loss:0.009\n",
      "Epoch 22: Loss:0.009, Val_loss:0.009\n",
      "Epoch 23: Loss:0.009, Val_loss:0.009\n",
      "Epoch 24: Loss:0.009, Val_loss:0.009\n",
      "Epoch 25: Loss:0.009, Val_loss:0.009\n",
      "Epoch 26: Loss:0.009, Val_loss:0.009\n",
      "Epoch 27: Loss:0.009, Val_loss:0.009\n",
      "Epoch 28: Loss:0.009, Val_loss:0.009\n",
      "Epoch 29: Loss:0.009, Val_loss:0.009\n",
      "Epoch 30: Loss:0.009, Val_loss:0.009\n",
      "Epoch 31: Loss:0.009, Val_loss:0.009\n",
      "Epoch 32: Loss:0.009, Val_loss:0.009\n",
      "Epoch 33: Loss:0.009, Val_loss:0.009\n",
      "Epoch 34: Loss:0.009, Val_loss:0.009\n",
      "Epoch 35: Loss:0.009, Val_loss:0.009\n",
      "Epoch 36: Loss:0.008, Val_loss:0.009\n",
      "Epoch 37: Loss:0.008, Val_loss:0.009\n",
      "Epoch 38: Loss:0.008, Val_loss:0.009\n",
      "Epoch 39: Loss:0.008, Val_loss:0.009\n",
      "Epoch 40: Loss:0.008, Val_loss:0.009\n",
      "Epoch 41: Loss:0.008, Val_loss:0.009\n",
      "Epoch 42: Loss:0.008, Val_loss:0.009\n",
      "Epoch 43: Loss:0.008, Val_loss:0.009\n",
      "Epoch 44: Loss:0.008, Val_loss:0.009\n",
      "Epoch 45: Loss:0.008, Val_loss:0.009\n",
      "Epoch 46: Loss:0.008, Val_loss:0.009\n",
      "Epoch 47: Loss:0.008, Val_loss:0.008\n",
      "Epoch 48: Loss:0.008, Val_loss:0.008\n",
      "Epoch 49: Loss:0.008, Val_loss:0.008\n",
      "Epoch 50: Loss:0.008, Val_loss:0.010\n",
      "Epoch 51: Loss:0.008, Val_loss:0.008\n",
      "Epoch 52: Loss:0.008, Val_loss:0.008\n",
      "Epoch 53: Loss:0.008, Val_loss:0.008\n",
      "Epoch 54: Loss:0.008, Val_loss:0.008\n",
      "Epoch 55: Loss:0.007, Val_loss:0.008\n",
      "Epoch 56: Loss:0.008, Val_loss:0.012\n",
      "Epoch 57: Loss:0.008, Val_loss:0.008\n",
      "Epoch 58: Loss:0.007, Val_loss:0.008\n",
      "Epoch 59: Loss:0.007, Val_loss:0.008\n",
      "Epoch 60: Loss:0.007, Val_loss:0.008\n",
      "Epoch 61: Loss:0.007, Val_loss:0.008\n",
      "Epoch 62: Loss:0.007, Val_loss:0.009\n",
      "Epoch 63: Loss:0.007, Val_loss:0.007\n",
      "Epoch 64: Loss:0.007, Val_loss:0.007\n",
      "Epoch 65: Loss:0.007, Val_loss:0.007\n",
      "Epoch 66: Loss:0.007, Val_loss:0.007\n",
      "Epoch 67: Loss:0.007, Val_loss:0.007\n",
      "Epoch 68: Loss:0.007, Val_loss:0.009\n",
      "Epoch 69: Loss:0.007, Val_loss:0.008\n",
      "Epoch 70: Loss:0.006, Val_loss:0.007\n",
      "Epoch 71: Loss:0.007, Val_loss:0.009\n",
      "Epoch 72: Loss:0.007, Val_loss:0.008\n",
      "Epoch 73: Loss:0.006, Val_loss:0.007\n",
      "Epoch 74: Loss:0.007, Val_loss:0.010\n",
      "Epoch 75: Loss:0.006, Val_loss:0.009\n",
      "Epoch 76: Loss:0.006, Val_loss:0.008\n",
      "Epoch 77: Loss:0.006, Val_loss:0.009\n",
      "Epoch 78: Loss:0.006, Val_loss:0.007\n",
      "Epoch 79: Loss:0.006, Val_loss:0.007\n",
      "Epoch 80: Loss:0.006, Val_loss:0.007\n",
      "Epoch 81: Loss:0.006, Val_loss:0.007\n",
      "Epoch 82: Loss:0.006, Val_loss:0.007\n",
      "Epoch 83: Loss:0.006, Val_loss:0.006\n",
      "Epoch 84: Loss:0.006, Val_loss:0.007\n",
      "Epoch 85: Loss:0.006, Val_loss:0.006\n",
      "Epoch 86: Loss:0.006, Val_loss:0.007\n",
      "Epoch 87: Loss:0.006, Val_loss:0.006\n",
      "Epoch 88: Loss:0.006, Val_loss:0.006\n",
      "Epoch 89: Loss:0.005, Val_loss:0.006\n",
      "Epoch 90: Loss:0.005, Val_loss:0.006\n",
      "Epoch 91: Loss:0.005, Val_loss:0.006\n",
      "Epoch 92: Loss:0.005, Val_loss:0.007\n",
      "Epoch 93: Loss:0.005, Val_loss:0.007\n",
      "Epoch 94: Loss:0.006, Val_loss:0.007\n",
      "Epoch 95: Loss:0.005, Val_loss:0.006\n",
      "Epoch 96: Loss:0.005, Val_loss:0.006\n",
      "Epoch 97: Loss:0.005, Val_loss:0.006\n",
      "Epoch 98: Loss:0.005, Val_loss:0.006\n",
      "Epoch 99: Loss:0.005, Val_loss:0.006\n",
      "Epoch 100: Loss:0.005, Val_loss:0.006\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBHNtUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b4920166a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "18\n",
      "Threshold:  0.0517835758487\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_8_35_30/_8_35_30_para.ckpt\n",
      "--- Initialization time: 418.06263065338135 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
