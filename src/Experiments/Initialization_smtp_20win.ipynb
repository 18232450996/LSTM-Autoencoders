{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_2000init/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_2000init/log.txt\"\n",
    "        self.training_set_size = 100*2000\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 94680 normal windows and 1860 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:2365,vn1:1419,vn2:473,tn:477,va:46,ta:47\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.015, Val_loss:0.010\n",
      "Epoch 1: Loss:0.009, Val_loss:0.009\n",
      "Epoch 2: Loss:0.008, Val_loss:0.008\n",
      "Epoch 3: Loss:0.006, Val_loss:0.007\n",
      "Epoch 4: Loss:0.005, Val_loss:0.007\n",
      "Epoch 5: Loss:0.005, Val_loss:0.005\n",
      "Epoch 6: Loss:0.005, Val_loss:0.005\n",
      "Epoch 7: Loss:0.004, Val_loss:0.005\n",
      "Epoch 8: Loss:0.004, Val_loss:0.004\n",
      "Epoch 9: Loss:0.004, Val_loss:0.004\n",
      "Epoch 10: Loss:0.004, Val_loss:0.004\n",
      "Epoch 11: Loss:0.004, Val_loss:0.004\n",
      "Epoch 12: Loss:0.003, Val_loss:0.004\n",
      "Epoch 13: Loss:0.003, Val_loss:0.004\n",
      "Epoch 14: Loss:0.003, Val_loss:0.003\n",
      "Epoch 15: Loss:0.003, Val_loss:0.003\n",
      "Epoch 16: Loss:0.003, Val_loss:0.003\n",
      "Epoch 17: Loss:0.003, Val_loss:0.003\n",
      "Epoch 18: Loss:0.002, Val_loss:0.003\n",
      "Epoch 19: Loss:0.002, Val_loss:0.003\n",
      "Epoch 20: Loss:0.002, Val_loss:0.003\n",
      "Epoch 21: Loss:0.002, Val_loss:0.002\n",
      "Epoch 22: Loss:0.002, Val_loss:0.002\n",
      "Epoch 23: Loss:0.002, Val_loss:0.002\n",
      "Epoch 24: Loss:0.002, Val_loss:0.003\n",
      "Epoch 25: Loss:0.002, Val_loss:0.003\n",
      "Epoch 26: Loss:0.002, Val_loss:0.002\n",
      "Epoch 27: Loss:0.002, Val_loss:0.002\n",
      "Epoch 28: Loss:0.002, Val_loss:0.002\n",
      "Epoch 29: Loss:0.002, Val_loss:0.002\n",
      "Epoch 30: Loss:0.002, Val_loss:0.002\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD9CAYAAAC85wBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDEGgP8HnOeWQpz9GTQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2932c02dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
