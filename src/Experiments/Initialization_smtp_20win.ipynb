{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp_20win/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp_20win/log.txt\"\n",
    "        self.training_set_size = 100*200\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 19720 normal windows and 280 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:490,vn1:294,vn2:98,tn:104,va:7,ta:7\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.031, Val_loss:0.018\n",
      "Epoch 1: Loss:0.013, Val_loss:0.014\n",
      "Epoch 2: Loss:0.011, Val_loss:0.012\n",
      "Epoch 3: Loss:0.011, Val_loss:0.011\n",
      "Epoch 4: Loss:0.010, Val_loss:0.010\n",
      "Epoch 5: Loss:0.010, Val_loss:0.010\n",
      "Epoch 6: Loss:0.010, Val_loss:0.010\n",
      "Epoch 7: Loss:0.010, Val_loss:0.010\n",
      "Epoch 8: Loss:0.010, Val_loss:0.009\n",
      "Epoch 9: Loss:0.010, Val_loss:0.009\n",
      "Epoch 10: Loss:0.009, Val_loss:0.009\n",
      "Epoch 11: Loss:0.009, Val_loss:0.008\n",
      "Epoch 12: Loss:0.009, Val_loss:0.008\n",
      "Epoch 13: Loss:0.009, Val_loss:0.008\n",
      "Epoch 14: Loss:0.008, Val_loss:0.008\n",
      "Epoch 15: Loss:0.008, Val_loss:0.008\n",
      "Epoch 16: Loss:0.008, Val_loss:0.007\n",
      "Epoch 17: Loss:0.007, Val_loss:0.007\n",
      "Epoch 18: Loss:0.007, Val_loss:0.007\n",
      "Epoch 19: Loss:0.007, Val_loss:0.007\n",
      "Epoch 20: Loss:0.007, Val_loss:0.007\n",
      "Epoch 21: Loss:0.007, Val_loss:0.006\n",
      "Epoch 22: Loss:0.006, Val_loss:0.006\n",
      "Epoch 23: Loss:0.006, Val_loss:0.006\n",
      "Epoch 24: Loss:0.006, Val_loss:0.006\n",
      "Epoch 25: Loss:0.006, Val_loss:0.006\n",
      "Epoch 26: Loss:0.006, Val_loss:0.006\n",
      "Epoch 27: Loss:0.006, Val_loss:0.006\n",
      "Epoch 28: Loss:0.006, Val_loss:0.006\n",
      "Epoch 29: Loss:0.006, Val_loss:0.006\n",
      "Epoch 30: Loss:0.006, Val_loss:0.006\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2478946bdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "7\n",
      "Threshold:  0.136231982618\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp_20win/_1_45_20_para.ckpt\n",
      "--- Initialization time: 211.3914270401001 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
