{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 15\n",
    "        self.step_num = 84\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/power.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/power/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/power/log.txt\"\n",
    "        self.training_set_size = 84*12\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "        \n",
    "        # for univariate data, anomaly score is squared euclidean distance\n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array.ravel())\n",
    "        sigma = np.var(err_vec_array.ravel())\n",
    "\n",
    "        print(\"Got parameters mu(%.3f) and sigma(%.3f).\"%(mu,sigma))\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_n[window,t],mu,sigma)\n",
    "                            normal_score.append(s)\n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "#                            temp = np.dot((err_a[window,t,:] - mu[t,:] ) , sigma[t])\n",
    "#                            s = np.dot(temp,(err_a[window,t,:] - mu[t,:] ).T)\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "                  \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 924 normal windows and 84 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:5,vn1:3,vn2:1,tn:2,va:1,ta:1\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.075, Val_loss:0.053\n",
      "Epoch 1: Loss:0.062, Val_loss:0.055\n",
      "Epoch 2: Loss:0.062, Val_loss:0.054\n",
      "Epoch 3: Loss:0.059, Val_loss:0.050\n",
      "Epoch 4: Loss:0.057, Val_loss:0.048\n",
      "Epoch 5: Loss:0.056, Val_loss:0.047\n",
      "Epoch 6: Loss:0.054, Val_loss:0.046\n",
      "Epoch 7: Loss:0.053, Val_loss:0.046\n",
      "Epoch 8: Loss:0.052, Val_loss:0.044\n",
      "Epoch 9: Loss:0.050, Val_loss:0.042\n",
      "Epoch 10: Loss:0.049, Val_loss:0.041\n",
      "Epoch 11: Loss:0.047, Val_loss:0.040\n",
      "Epoch 12: Loss:0.046, Val_loss:0.039\n",
      "Epoch 13: Loss:0.044, Val_loss:0.037\n",
      "Epoch 14: Loss:0.042, Val_loss:0.035\n",
      "Epoch 15: Loss:0.039, Val_loss:0.034\n",
      "Epoch 16: Loss:0.037, Val_loss:0.031\n",
      "Epoch 17: Loss:0.034, Val_loss:0.029\n",
      "Epoch 18: Loss:0.031, Val_loss:0.026\n",
      "Epoch 19: Loss:0.028, Val_loss:0.024\n",
      "Epoch 20: Loss:0.025, Val_loss:0.022\n",
      "Epoch 21: Loss:0.023, Val_loss:0.020\n",
      "Epoch 22: Loss:0.020, Val_loss:0.019\n",
      "Epoch 23: Loss:0.018, Val_loss:0.018\n",
      "Epoch 24: Loss:0.016, Val_loss:0.017\n",
      "Epoch 25: Loss:0.015, Val_loss:0.017\n",
      "Epoch 26: Loss:0.014, Val_loss:0.016\n",
      "Epoch 27: Loss:0.014, Val_loss:0.016\n",
      "Epoch 28: Loss:0.013, Val_loss:0.016\n",
      "Epoch 29: Loss:0.013, Val_loss:0.016\n",
      "Epoch 30: Loss:0.013, Val_loss:0.016\n",
      "Epoch 31: Loss:0.013, Val_loss:0.016\n",
      "Epoch 32: Loss:0.013, Val_loss:0.016\n",
      "Epoch 33: Loss:0.012, Val_loss:0.016\n",
      "Epoch 34: Loss:0.012, Val_loss:0.016\n",
      "Epoch 35: Loss:0.012, Val_loss:0.016\n",
      "Epoch 36: Loss:0.012, Val_loss:0.016\n",
      "Epoch 37: Loss:0.012, Val_loss:0.016\n",
      "Epoch 38: Loss:0.012, Val_loss:0.016\n",
      "Epoch 39: Loss:0.012, Val_loss:0.016\n",
      "Epoch 40: Loss:0.012, Val_loss:0.015\n",
      "Epoch 41: Loss:0.012, Val_loss:0.015\n",
      "Epoch 42: Loss:0.012, Val_loss:0.015\n",
      "Epoch 43: Loss:0.012, Val_loss:0.015\n",
      "Epoch 44: Loss:0.012, Val_loss:0.015\n",
      "Epoch 45: Loss:0.012, Val_loss:0.015\n",
      "Epoch 46: Loss:0.012, Val_loss:0.015\n",
      "Epoch 47: Loss:0.012, Val_loss:0.015\n",
      "Epoch 48: Loss:0.012, Val_loss:0.015\n",
      "Epoch 49: Loss:0.012, Val_loss:0.015\n",
      "Epoch 50: Loss:0.012, Val_loss:0.015\n",
      "Epoch 51: Loss:0.012, Val_loss:0.015\n",
      "Epoch 52: Loss:0.012, Val_loss:0.015\n",
      "Epoch 53: Loss:0.011, Val_loss:0.015\n",
      "Epoch 54: Loss:0.011, Val_loss:0.015\n",
      "Epoch 55: Loss:0.011, Val_loss:0.015\n",
      "Epoch 56: Loss:0.011, Val_loss:0.015\n",
      "Epoch 57: Loss:0.011, Val_loss:0.015\n",
      "Epoch 58: Loss:0.011, Val_loss:0.015\n",
      "Early stopping at epoch 58\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD6ipyN9J7lYzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2acbdbc8a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu(0.085) and sigma(0.005).\n",
      "1\n",
      "Threshold:  0.00671047344804\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/power/_1_15_84_para.ckpt\n",
      "--- Initialization time: 133.71724200248718 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
