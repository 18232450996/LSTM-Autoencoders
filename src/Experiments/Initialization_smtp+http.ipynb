{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from scipy.spatial.distance import mahalanobis,euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a initialization dataset, split it into normal lists and abnormal lists of different subsets.\n",
    "\n",
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self, path,training_set_size,step_num,batch_num,training_data_source,log_path):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "        self.training_data_source = training_data_source\n",
    "        self.training_set_size = training_set_size\n",
    "        \n",
    "        \n",
    "\n",
    "        self.df = pd.read_csv(self.path).iloc[:self.training_set_size,:]\n",
    "            \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.sn,self.vn1,self.vn2,self.tn,self.va,self.ta,self.va_labels = self.preprocessing(self.df,log_path)\n",
    "        assert min(self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size) > 0, \"Not enough continuous data in file for training, ended.\"+str((self.sn.size,self.vn1.size,self.vn2.size,self.tn.size,self.va.size,self.ta.size))\n",
    "           \n",
    "        # data seriealization\n",
    "        t1 = self.sn.shape[0]//step_num\n",
    "        t2 = self.va.shape[0]//step_num\n",
    "        t3 = self.vn1.shape[0]//step_num\n",
    "        t4 = self.vn2.shape[0]//step_num\n",
    "        t5 = self.tn.shape[0]//step_num\n",
    "        t6 = self.ta.shape[0]//step_num\n",
    "        \n",
    "        self.sn_list = [self.sn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.va_list = [self.va[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        self.vn1_list = [self.vn1[step_num*i:step_num*(i+1)].as_matrix() for i in range(t3)]\n",
    "        self.vn2_list = [self.vn2[step_num*i:step_num*(i+1)].as_matrix() for i in range(t4)]\n",
    "        \n",
    "        self.tn_list = [self.tn[step_num*i:step_num*(i+1)].as_matrix() for i in range(t5)]\n",
    "        self.ta_list = [self.ta[step_num*i:step_num*(i+1)].as_matrix() for i in range(t6)]\n",
    "        \n",
    "        self.va_label_list =  [self.va_labels[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        \n",
    "        print(\"Ready for training.\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def preprocessing(self,df,log_path):\n",
    "        \n",
    "        #scaling\n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = pd.DataFrame(scaler.transform(df.iloc[:,:-1]))\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        # split dataframe into segments of length L, if a window contains mindestens one anomaly, then this window is anomaly wondow\n",
    "        L = self.step_num \n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        temp = []\n",
    "        a_pos= []\n",
    "        \n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "\n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "        print(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\"%(normal.shape[0],anomaly.shape[0]))\n",
    "\n",
    "        a_labels = data.iloc[np.array(a_list),-1]\n",
    "        \n",
    "        # split into subsets\n",
    "        tmp = normal.index.size//self.step_num//10 \n",
    "        assert tmp > 0 ,\"Too small normal set %d rows\"%normal.index.size\n",
    "        sn = normal.iloc[:tmp*5*self.step_num,:]\n",
    "        vn1 = normal.iloc[tmp*5*self.step_num:tmp*8*self.step_num,:]\n",
    "        vn2 = normal.iloc[tmp*8*self.step_num:tmp*9*self.step_num,:]\n",
    "        tn = normal.iloc[tmp*9*self.step_num:,:]\n",
    "        \n",
    "        tmp_a = anomaly.index.size//self.step_num//2 \n",
    "        va = anomaly.iloc[:tmp_a*self.step_num,:] if tmp_a !=0 else anomaly\n",
    "        ta = anomaly.iloc[tmp_a*self.step_num:,:] if tmp_a !=0 else anomaly\n",
    "        a_labels = a_labels[:va.index.size]\n",
    "        \n",
    "        print(\"Local preprocessing finished.\")\n",
    "        print(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        \n",
    "        f = open(log_path,'a')\n",
    "        \n",
    "        f.write(\"Info: Initialization set contains %d normal windows and %d abnormal windows.\\n\"%(normal.shape[0],anomaly.shape[0]))\n",
    "        f.write(\"Subsets contain windows: sn:%d,vn1:%d,vn2:%d,tn:%d,va:%d,ta:%d\\n\"%(sn.shape[0]/self.step_num,vn1.shape[0]/self.step_num,vn2.shape[0]/self.step_num,tn.shape[0]/self.step_num,va.shape[0]/self.step_num,ta.shape[0]/self.step_num))\n",
    "        f.close()\n",
    "        \n",
    "        return sn,vn1,vn2,tn,va,ta,a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf_EncDecAD_KDD99(object):\n",
    "    \n",
    "    def __init__(self, training_data_source = \"file\", optimizer=None, decode_without_input=False):\n",
    "        \n",
    "        self.batch_num = 1\n",
    "        self.hidden_num = 45\n",
    "        self.step_num = 20\n",
    "        self.input_root =\"C:/Users/Bin/Desktop/Thesis/dataset/smtp.csv\"\n",
    "        self.iteration = 300\n",
    "        self.modelpath_root = \"C:/Users/Bin/Desktop/Thesis/models/smtp+http/\"\n",
    "        self.modelmeta = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_.ckpt.meta\"\n",
    "        self.modelpath_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt\"\n",
    "        self.modelmeta_p = self.modelpath_root + \"_\"+str(self.batch_num)+\"_\"+str(self.hidden_num)+\"_\"+str(self.step_num)+\"_para.ckpt.meta\"\n",
    "        self.decode_without_input =  False\n",
    "        \n",
    "        self.log_path = \"C:/Users/Bin/Desktop/Thesis/models/smtp+http/log.txt\"\n",
    "        self.training_set_size = 20*2500\n",
    "        # import dataset\n",
    "        # The dataset is divided into 6 parts, namely training_normal, validation_1,\n",
    "        # validation_2, test_normal, validation_anomaly, test_anomaly.\n",
    "       \n",
    "        self.training_data_source = training_data_source\n",
    "        data_helper = Data_Helper(self.input_root,self.training_set_size,self.step_num,self.batch_num,self.training_data_source,self.log_path)\n",
    "        \n",
    "        self.sn_list = data_helper.sn_list\n",
    "        self.va_list = data_helper.va_list\n",
    "        self.vn1_list = data_helper.vn1_list\n",
    "        self.vn2_list = data_helper.vn2_list\n",
    "        self.tn_list = data_helper.tn_list\n",
    "        self.ta_list = data_helper.ta_list\n",
    "        self.data_list = [self.sn_list, self.va_list, self.vn1_list, self.vn2_list, self.tn_list, self.ta_list]\n",
    "        \n",
    "        self.elem_num = data_helper.sn.shape[1]\n",
    "        self.va_label_list = data_helper.va_label_list \n",
    "        \n",
    "        \n",
    "        f = open(self.log_path,'a')\n",
    "        f.write(\"Batch_num=%d\\nHidden_num=%d\\nwindow_length=%d\\ntraining_used_#windows=%d\\n\"%(self.batch_num,self.hidden_num,self.step_num,self.training_set_size//self.step_num))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "\n",
    "    def __init__(self, hidden_num, inputs, is_training, optimizer=None, reverse=True, decode_without_input=False):\n",
    "\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.elem_num = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self._enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        self._dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "        if is_training == True:\n",
    "            self._enc_cell = tf.nn.rnn_cell.DropoutWrapper(self._enc_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "            self._dec_cell = tf.nn.rnn_cell.DropoutWrapper(self._dec_cell, input_keep_prob=0.8, output_keep_prob=0.8)\n",
    "        \n",
    "        self.is_training = is_training\n",
    "        \n",
    "        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "        \n",
    "        with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "         \n",
    "            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,self.elem_num], dtype=tf.float32))\n",
    " \n",
    "            dec_bias_ = tf.Variable(tf.constant(0.1,shape=[self.elem_num],dtype=tf.float32))\n",
    "\n",
    "            dec_state = self.enc_state\n",
    "            dec_input_ = tf.ones(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            \n",
    "            for step in range(len(inputs)):\n",
    "                if step > 0:\n",
    "                    vs.reuse_variables()\n",
    "                (dec_input_, dec_state) =self._dec_cell(dec_input_, dec_state)\n",
    "                dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "                dec_outputs.append(dec_input_)\n",
    "                # use real input as as input of decoder ***********************************\n",
    "                tmp = -(step+1) \n",
    "                dec_input_ = inputs[tmp]\n",
    "                \n",
    "            if reverse:\n",
    "                dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "            self.output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "            self.loss = tf.reduce_mean(tf.square(self.input_ - self.output_),name=\"loss\")\n",
    "        \n",
    "        def check_is_train(is_training):\n",
    "            def t_ (): return tf.train.AdamOptimizer().minimize(self.loss,name=\"train_\")\n",
    "            def f_ (): return tf.train.AdamOptimizer(1/math.inf).minimize(self.loss)\n",
    "            is_train = tf.cond(is_training, t_, f_)\n",
    "            return is_train\n",
    "        self.train = check_is_train(is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameter_Helper(object):\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "       \n",
    "        \n",
    "    def mu_and_sigma(self,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "        err_vec_list = []\n",
    "        \n",
    "        ind = list(np.random.permutation(len(self.conf.vn1_list)))\n",
    "        \n",
    "        while len(ind)>=self.conf.batch_num:\n",
    "            data = []\n",
    "            for _ in range(self.conf.batch_num):\n",
    "                data += [self.conf.vn1_list[ind.pop()]]\n",
    "            data = np.array(data,dtype=float)\n",
    "            data = data.reshape((self.conf.batch_num,self.conf.step_num,self.conf.elem_num))\n",
    "\n",
    "            (_input_, _output_) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "            abs_err = abs(_input_ - _output_)\n",
    "            err_vec_list += [abs_err[i] for i in range(abs_err.shape[0])]\n",
    "            \n",
    "\n",
    "        # new metric\n",
    "        err_vec_array = np.array(err_vec_list).reshape(-1,self.conf.elem_num)\n",
    "        \n",
    "        # for multivariate data, anomaly score is squared mahalanobis distance\n",
    "\n",
    "        mu = np.mean(err_vec_array,axis=0)\n",
    "        sigma = np.cov(err_vec_array.T)\n",
    "\n",
    "        print(\"Got parameters mu and sigma.\")\n",
    "        \n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_threshold(self,mu,sigma,sess,input_, output_,p_input, p_is_training):\n",
    "\n",
    "            normal_score = []\n",
    "            for count in range(len(self.conf.vn2_list)//self.conf.batch_num):\n",
    "                normal_sub = np.array(self.conf.vn2_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                (input_n, output_n) = sess.run([input_, output_], {p_input: normal_sub,p_is_training : False})\n",
    "\n",
    "                err_n = abs(input_n-output_n).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            normal_score.append(mahalanobis(err_n[window,t],mu,sigma))\n",
    "                            \n",
    "\n",
    "                    \n",
    "            abnormal_score = []\n",
    "            '''\n",
    "            if have enough anomaly data, then calculate anomaly score, and the \n",
    "            threshold that achives best f1 score as divide boundary.\n",
    "            otherwise estimate threshold through normal scores\n",
    "            '''\n",
    "            print(len(self.conf.va_list))\n",
    "            \n",
    "            if len(self.conf.va_list) < self.conf.batch_num: # not enough anomaly data for a single batch\n",
    "                threshold = max(normal_score) * 2\n",
    "                print(\"Not enough large va set, estimated threshold by normal data.\")\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                for count in range(len(self.conf.va_list)//self.conf.batch_num):\n",
    "                    abnormal_sub = np.array(self.conf.va_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = np.array(self.conf.va_label_list[count*self.conf.batch_num:(count+1)*self.conf.batch_num]) \n",
    "                    va_lable_list = va_lable_list.reshape(self.conf.batch_num,self.conf.step_num)\n",
    "                    \n",
    "                    (input_a, output_a) = sess.run([input_, output_], {p_input: abnormal_sub,p_is_training : False})\n",
    "                    err_a = abs(input_a-output_a).reshape(-1,self.conf.step_num,self.conf.elem_num)\n",
    "                    for window in range(self.conf.batch_num):\n",
    "                        for t in range(self.conf.step_num):\n",
    "                            s = mahalanobis(err_a[window,t],mu,sigma)\n",
    "                            \n",
    "                            if va_lable_list[window,t] == \"normal\":\n",
    "                                normal_score.append(s)\n",
    "                            else:\n",
    "                                abnormal_score.append(s)\n",
    "                \n",
    "                upper = np.median(np.array(abnormal_score))\n",
    "                lower = np.median(np.array(normal_score)) \n",
    "                scala = 20\n",
    "                delta = (upper-lower) / scala\n",
    "                candidate = lower\n",
    "                threshold = 0\n",
    "                result = 0\n",
    "                \n",
    "                def evaluate(threshold,normal_score,abnormal_score):\n",
    "#                    pd.Series(normal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/normal.csv\",index=None)\n",
    "#                    pd.Series(abnormal_score).to_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/abnormal.csv\",index=None)\n",
    "                    \n",
    "                    beta = 0.5\n",
    "                    tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "                    fp = len(abnormal_score)-tp\n",
    "                    fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "                    tn = len(normal_score)- fn\n",
    "                    \n",
    "                    if tp == 0: return 0\n",
    "                    \n",
    "                    P = tp/(tp+fp)\n",
    "                    R = tp/(tp+fn)\n",
    "                    fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "                    return fbeta \n",
    "                \n",
    "                for _ in range(scala):\n",
    "                    r = evaluate(candidate,normal_score,abnormal_score)\n",
    "                    if r > result:\n",
    "                        result = r \n",
    "                        threshold = candidate\n",
    "                    candidate += delta \n",
    "            \n",
    "            print(\"Threshold: \",threshold)\n",
    "\n",
    "            return threshold\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncDecAD_Train(object):\n",
    "    \n",
    "    def __init__(self,training_data_source='file'):\n",
    "        start_time = time.time()\n",
    "        conf = Conf_EncDecAD_KDD99(training_data_source=training_data_source)\n",
    "        \n",
    "\n",
    "        batch_num = conf.batch_num\n",
    "        hidden_num = conf.hidden_num\n",
    "        step_num = conf.step_num\n",
    "        elem_num = conf.elem_num\n",
    "        \n",
    "        iteration = conf.iteration\n",
    "        modelpath_root = conf.modelpath_root\n",
    "        modelpath = conf.modelpath_p\n",
    "        decode_without_input = conf.decode_without_input\n",
    "        \n",
    "        patience = 20\n",
    "        patience_cnt = 0\n",
    "        min_delta = 0.0001\n",
    "        \n",
    "        \n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "        \n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "        \n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "        \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        \n",
    "        graph = tf.get_default_graph()\n",
    "        gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        assign_ops = [graph.get_operation_by_name(v.op.name + \"/Assign\") for v in gvars]\n",
    "        init_values = [assign_op.inputs[1] for assign_op in assign_ops]    \n",
    "            \n",
    "        \n",
    "        print(\"Training start.\")\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "\n",
    "            loss = []\n",
    "            val_loss = []\n",
    "            sn_list_length = len(conf.sn_list)\n",
    "            tn_list_length = len(conf.tn_list)\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                #training set\n",
    "                snlist = conf.sn_list[:]\n",
    "                tmp_loss = 0\n",
    "                for t in range(sn_list_length//batch_num):\n",
    "                    data =[]\n",
    "                    for _ in range(batch_num):\n",
    "                        data.append(snlist.pop())\n",
    "                    data = np.array(data)\n",
    "                    (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                    tmp_loss += loss_val\n",
    "                l = tmp_loss/(sn_list_length//batch_num)\n",
    "                loss.append(l)\n",
    "                \n",
    "                #validation set\n",
    "                tnlist = conf.tn_list[:]\n",
    "                tmp_loss_ = 0\n",
    "                for t in range(tn_list_length//batch_num):\n",
    "                    testdata = []\n",
    "                    for _ in range(batch_num):\n",
    "                        testdata.append(tnlist.pop())\n",
    "                    testdata = np.array(testdata)\n",
    "                    (loss_val,ein,aus) = sess.run([ae.loss,input_,output_], {p_input: testdata,p_is_training :False})\n",
    "                    tmp_loss_ += loss_val\n",
    "                tl = tmp_loss_/(tn_list_length//batch_num)\n",
    "                val_loss.append(tl)\n",
    "                print('Epoch %d: Loss:%.3f, Val_loss:%.3f' %(i, l,tl))\n",
    "                \n",
    "                if i == 30:\n",
    "                    break\n",
    "                #Early stopping\n",
    "                if i > 50 and  val_loss[i] < np.array(val_loss[:i]).min():\n",
    "                    #save_path = saver.save(sess, conf.modelpath_p)\n",
    "                    gvars_state = sess.run(gvars)\n",
    "                    \n",
    "                if i > 0 and val_loss[i-1] - val_loss[i] > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "                if i>50 and patience_cnt > patience:\n",
    "                    print(\"Early stopping at epoch %d\\n\"%i)\n",
    "                    feed_dict = {init_value: val for init_value, val in zip(init_values, gvars_state)}\n",
    "                    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "                    #saver.restore(sess,tf.train.latest_checkpoint(modelpath_root))\n",
    "                    #graph = tf.get_default_graph()\n",
    "                    break\n",
    "        \n",
    "            plt.plot(loss,label=\"Train\")\n",
    "            plt.plot(val_loss,label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # mu & sigma & threshold\n",
    "\n",
    "            para = Parameter_Helper(conf)\n",
    "            mu, sigma = para.mu_and_sigma(sess,input_, output_,p_input, p_is_training)\n",
    "            threshold = para.get_threshold(mu,sigma,sess,input_, output_,p_input, p_is_training)\n",
    "            \n",
    "#            test = EncDecAD_Test(conf)\n",
    "#            test.test_encdecad(sess,input_,output_,p_input,p_is_training,mu,sigma,threshold,beta = 0.5)\n",
    "            \n",
    "            c_mu = tf.constant(mu,dtype=tf.float32,name = \"mu\")\n",
    "            c_sigma = tf.constant(sigma,dtype=tf.float32,name = \"sigma\")\n",
    "            c_threshold = tf.constant(threshold,dtype=tf.float32,name = \"threshold\")\n",
    "            print(\"Saving model to disk...\")\n",
    "            save_path = saver.save(sess, conf.modelpath_p)\n",
    "            print(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            f = open(conf.log_path,'a')\n",
    "            f.write(\"Early stopping at epoch %d\\n\"%i)\n",
    "            #f.write(\"Paras: mu=%.3f,sigma=%.3f,threshold=%.3f\\n\"%(mu,sigma,threshold))\n",
    "            f.write(\"Model saved accompany with parameters and threshold in file: %s\" % save_path)\n",
    "            f.write(\"--- Initialization time: %s seconds ---\" % (time.time() - start_time))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Info: Initialization set contains 49360 normal windows and 640 abnormal windows.\n",
      "Local preprocessing finished.\n",
      "Subsets contain windows: sn:1230,vn1:738,vn2:246,tn:254,va:16,ta:16\n",
      "\n",
      "Ready for training.\n",
      "Training start.\n",
      "Epoch 0: Loss:0.018, Val_loss:0.011\n",
      "Epoch 1: Loss:0.010, Val_loss:0.010\n",
      "Epoch 2: Loss:0.009, Val_loss:0.009\n",
      "Epoch 3: Loss:0.009, Val_loss:0.008\n",
      "Epoch 4: Loss:0.008, Val_loss:0.008\n",
      "Epoch 5: Loss:0.008, Val_loss:0.007\n",
      "Epoch 6: Loss:0.007, Val_loss:0.006\n",
      "Epoch 7: Loss:0.006, Val_loss:0.006\n",
      "Epoch 8: Loss:0.006, Val_loss:0.006\n",
      "Epoch 9: Loss:0.006, Val_loss:0.005\n",
      "Epoch 10: Loss:0.005, Val_loss:0.005\n",
      "Epoch 11: Loss:0.005, Val_loss:0.005\n",
      "Epoch 12: Loss:0.005, Val_loss:0.005\n",
      "Epoch 13: Loss:0.005, Val_loss:0.005\n",
      "Epoch 14: Loss:0.005, Val_loss:0.005\n",
      "Epoch 15: Loss:0.004, Val_loss:0.004\n",
      "Epoch 16: Loss:0.004, Val_loss:0.004\n",
      "Epoch 17: Loss:0.004, Val_loss:0.004\n",
      "Epoch 18: Loss:0.004, Val_loss:0.004\n",
      "Epoch 19: Loss:0.004, Val_loss:0.004\n",
      "Epoch 20: Loss:0.004, Val_loss:0.004\n",
      "Epoch 21: Loss:0.004, Val_loss:0.004\n",
      "Epoch 22: Loss:0.004, Val_loss:0.004\n",
      "Epoch 23: Loss:0.004, Val_loss:0.004\n",
      "Epoch 24: Loss:0.004, Val_loss:0.004\n",
      "Epoch 25: Loss:0.004, Val_loss:0.004\n",
      "Epoch 26: Loss:0.004, Val_loss:0.004\n",
      "Epoch 27: Loss:0.004, Val_loss:0.004\n",
      "Epoch 28: Loss:0.004, Val_loss:0.004\n",
      "Epoch 29: Loss:0.004, Val_loss:0.003\n",
      "Epoch 30: Loss:0.003, Val_loss:0.004\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a258d5a5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got parameters mu and sigma.\n",
      "16\n",
      "Threshold:  0.0443464643491\n",
      "Saving model to disk...\n",
      "Model saved accompany with parameters and threshold in file: C:/Users/Bin/Desktop/Thesis/models/smtp+http/_1_45_20_para.ckpt\n",
      "--- Initialization time: 425.9028196334839 seconds ---\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    EncDecAD_Train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
