{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import norm\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/Bin/Desktop/Thesis/code/src_3sigma/Initialization')\n",
    "from EncDecAD import EncDecAD\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data_Helper(object):\n",
    "    \n",
    "    def __init__(self,path,step_num,batch_num):\n",
    "        self.path = path\n",
    "        self.step_num = step_num\n",
    "        self.batch_num = batch_num\n",
    "       \n",
    "        self.df = pd.read_csv(self.path[0])\n",
    "        test_n_data = pd.read_csv(self.path[1])\n",
    "        test_a_data = pd.read_csv(self.path[2])\n",
    "        \n",
    "        print(\"Preprocessing...\")\n",
    "        \n",
    "        self.trainingset,_ = self.preprocessing(self.df)\n",
    "        self.test_n,_ = self.preprocessing(test_n_data)\n",
    "        _,self.test_a = self.preprocessing(test_a_data)\n",
    "    \n",
    "        tt = self.trainingset.shape[0] // step_num\n",
    "        t1 = self.test_n.shape[0] // step_num\n",
    "        t2 = self.test_a.shape[0] // step_num\n",
    "        self.traininglist = [self.trainingset[step_num*i:step_num*(i+1)].as_matrix() for i in range(tt)]\n",
    "        self.test_n_list = [self.test_n[step_num*i:step_num*(i+1)].as_matrix() for i in range(t1)]\n",
    "        self.test_a_list = [self.test_a[step_num*i:step_num*(i+1)].as_matrix() for i in range(t2)]\n",
    "        print(\"Ready for training.\")\n",
    "  \n",
    "    \n",
    "    def preprocessing(self,df):\n",
    "        \n",
    "        label = df.iloc[:,-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.iloc[:,:-1])\n",
    "        cont = scaler.transform(df.iloc[:,:-1])\n",
    "            \n",
    "        cont = pd.DataFrame(cont)\n",
    "#        cont.columns = continuous.columns.values\n",
    "        data = pd.concat((cont,label),axis=1)\n",
    "        \n",
    "        # split data according to window length\n",
    "        n_list = []\n",
    "        a_list = []\n",
    "        #new version: iter windows\n",
    "        windows = [data.iloc[w*self.step_num:(w+1)*self.step_num,:] for w in range(data.index.size//self.step_num)]\n",
    "        for win in windows:\n",
    "            label = win.iloc[:,-1]\n",
    "            if label[label!=\"normal\"].size == 0:\n",
    "                n_list += [i for i in win.index]\n",
    "            else:\n",
    "                a_list += [i for i in win.index]\n",
    "                \n",
    "        normal = data.iloc[np.array(n_list),:-1]\n",
    "        anomaly = data.iloc[np.array(a_list),:-1]\n",
    "\n",
    "        return normal,anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hidden_num in range(5,55,5):\n",
    "    for step_num in range(10,110,10):\n",
    "        tf.reset_default_graph()\n",
    "        training_data_source='file'\n",
    "\n",
    "        batch_num = 1\n",
    "        elem_num = 34\n",
    "\n",
    "        iteration = 300\n",
    "        path = [\"C:/Users/Bin/Desktop/Thesis/dataset/http_gridsearch_train.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/http_gridsearch_test_n.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/http_gridsearch_test_a.csv\"]\n",
    "        data_helper = Data_Helper(path,step_num,batch_num)\n",
    "        train_list = data_helper.traininglist \n",
    "        test_n_list = data_helper.test_n_list\n",
    "        test_a_list = data_helper.test_a_list\n",
    "\n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "\n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "\n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "\n",
    "    \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        print(\"Training start.\")\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            errorBuf = []\n",
    "            \n",
    "            graph = tf.get_default_graph()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "            print(\"Training...\")\n",
    "            for i in range(iteration):\n",
    "                data =[]\n",
    "                for temp in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(train_list))\n",
    "                    sub = train_list[ind]\n",
    "                    data.append(sub)\n",
    "                data = np.array(data)\n",
    "                (ein,aus,loss_val, _) = sess.run([input_,output_,ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                abs_err = abs(ein-aus)\n",
    "                for e in abs_err:\n",
    "                    errorBuf.append(e)\n",
    "            train_mean = np.array(errorBuf).ravel().mean()\n",
    "            \n",
    "            print(\"testing...\")\n",
    "            error_n_buf = []\n",
    "            for _ in range(len(test_n_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_n_list)-1)\n",
    "                    sub = test_n_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_n_buf.append(e)\n",
    "            test_n_mean = np.array(error_n_buf).ravel().mean()\n",
    "            \n",
    "            error_a_buf = []\n",
    "            for _ in range(len(test_a_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_a_list))\n",
    "                    sub = test_a_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_a_buf.append(e)\n",
    "            test_a_mean = np.array(error_a_buf).ravel().mean()\n",
    "            results.append([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])\n",
    "            f= open(b\"C:\\Users\\Bin\\Desktop\\Thesis\\code\\src\\GridSearch\\http_results.txt\",\"a\")\n",
    "            f.write(str([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])+\"\\n\")\n",
    "            f.close()\n",
    "            sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hidden_num in range(5,55,5):\n",
    "    for step_num in range(10,110,10):\n",
    "        tf.reset_default_graph()\n",
    "        training_data_source='file'\n",
    "\n",
    "        batch_num = 1\n",
    "        elem_num = 34\n",
    "\n",
    "        iteration = 300\n",
    "        path = [\"C:/Users/Bin/Desktop/Thesis/dataset/smtp_gridsearch_train.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/smtp_gridsearch_test_n.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/smtp_gridsearch_test_a.csv\"]\n",
    "        data_helper = Data_Helper(path,step_num,batch_num)\n",
    "        train_list = data_helper.traininglist \n",
    "        test_n_list = data_helper.test_n_list\n",
    "        test_a_list = data_helper.test_a_list\n",
    "\n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "\n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "\n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "\n",
    "    \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        print(\"Training start.\")\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            errorBuf = []\n",
    "            \n",
    "            graph = tf.get_default_graph()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "            print(\"Training...\")\n",
    "            for i in range(iteration):\n",
    "                data =[]\n",
    "                for temp in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(train_list))\n",
    "                    sub = train_list[ind]\n",
    "                    data.append(sub)\n",
    "                data = np.array(data)\n",
    "                (ein,aus,loss_val, _) = sess.run([input_,output_,ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                abs_err = abs(ein-aus)\n",
    "                for e in abs_err:\n",
    "                    errorBuf.append(e)\n",
    "            train_mean = np.array(errorBuf).ravel().mean()\n",
    "            \n",
    "            print(\"testing...\")\n",
    "            error_n_buf = []\n",
    "            for _ in range(len(test_n_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_n_list))\n",
    "                    sub = test_n_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_n_buf.append(e)\n",
    "            test_n_mean = np.array(error_n_buf).ravel().mean()\n",
    "            \n",
    "            error_a_buf = []\n",
    "            for _ in range(len(test_a_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_a_list))\n",
    "                    sub = test_a_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_a_buf.append(e)\n",
    "            test_a_mean = np.array(error_a_buf).ravel().mean()\n",
    "            results.append([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])\n",
    "            f= open(b\"C:\\Users\\Bin\\Desktop\\Thesis\\code\\src\\GridSearch\\smtp_results.txt\",\"a\")\n",
    "            f.write(str([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])+\"\\n\")\n",
    "            f.close()\n",
    "            sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for hidden_num in range(5,55,5):\n",
    "    for step_num in range(10,110,10):\n",
    "        tf.reset_default_graph()\n",
    "        training_data_source='file'\n",
    "\n",
    "        batch_num = 1\n",
    "        elem_num = 7\n",
    "\n",
    "        iteration = 300\n",
    "        path = [\"C:/Users/Bin/Desktop/Thesis/dataset/forest_gridsearch_train.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/forest_gridsearch_test_n.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/forest_gridsearch_test_a.csv\"]\n",
    "        data_helper = Data_Helper(path,step_num,batch_num)\n",
    "        train_list = data_helper.traininglist \n",
    "        test_n_list = data_helper.test_n_list\n",
    "        test_a_list = data_helper.test_a_list\n",
    "\n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "\n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "\n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "\n",
    "    \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        print(\"Training start.\")\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            errorBuf = []\n",
    "            \n",
    "            graph = tf.get_default_graph()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "            print(\"Training...\")\n",
    "            for i in range(iteration):\n",
    "                data =[]\n",
    "                for temp in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(train_list))\n",
    "                    sub = train_list[ind]\n",
    "                    data.append(sub)\n",
    "                data = np.array(data)\n",
    "                (ein,aus,loss_val, _) = sess.run([input_,output_,ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                abs_err = abs(ein-aus)\n",
    "                for e in abs_err:\n",
    "                    errorBuf.append(e)\n",
    "            train_mean = np.array(errorBuf).ravel().mean()\n",
    "            \n",
    "            print(\"testing...\")\n",
    "            error_n_buf = []\n",
    "            for _ in range(len(test_n_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_n_list))\n",
    "                    sub = test_n_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_n_buf.append(e)\n",
    "            test_n_mean = np.array(error_n_buf).ravel().mean()\n",
    "            \n",
    "            error_a_buf = []\n",
    "            for _ in range(len(test_a_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_a_list))\n",
    "                    sub = test_a_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_a_buf.append(e)\n",
    "            test_a_mean = np.array(error_a_buf).ravel().mean()\n",
    "            results.append([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])\n",
    "            f= open(\"C:/Users/Bin/Desktop/Thesis/code/src/GridSearch/forest_results.txt\",\"a\")\n",
    "            f.write(str([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])+\"\\n\")\n",
    "            f.close()\n",
    "            sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n",
      "Training start.\n",
      "Training...\n",
      "testing...\n",
      "Preprocessing...\n",
      "Ready for training.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for hidden_num in range(5,55,5):\n",
    "    for step_num in range(10,110,10):\n",
    "        tf.reset_default_graph()\n",
    "        training_data_source='file'\n",
    "\n",
    "        batch_num = 1\n",
    "        elem_num = 1\n",
    "\n",
    "        iteration = 300\n",
    "        path = [\"C:/Users/Bin/Desktop/Thesis/dataset/power_gridsearch_train.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/power_gridsearch_test_n.csv\",\"C:/Users/Bin/Desktop/Thesis/dataset/power_gridsearch_test_a.csv\"]\n",
    "        data_helper = Data_Helper(path,step_num,batch_num)\n",
    "        train_list = data_helper.traininglist \n",
    "        test_n_list = data_helper.test_n_list\n",
    "        test_a_list = data_helper.test_a_list\n",
    "\n",
    "        #************#\n",
    "        # Training\n",
    "        #************#\n",
    "\n",
    "        p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "        p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "\n",
    "        p_is_training = tf.placeholder(tf.bool,name= \"is_training_\")\n",
    "\n",
    "    \n",
    "        ae = EncDecAD(hidden_num, p_inputs, p_is_training , decode_without_input=False)\n",
    "        print(\"Training start.\")\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            errorBuf = []\n",
    "            \n",
    "            graph = tf.get_default_graph()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            input_= tf.transpose(tf.stack(p_inputs), [1, 0, 2])    \n",
    "            output_ = graph.get_tensor_by_name(\"decoder/output_:0\")\n",
    "            print(\"Training...\")\n",
    "            for i in range(iteration):\n",
    "                data =[]\n",
    "                for temp in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(train_list)-1)\n",
    "                    sub = train_list[ind]\n",
    "                    data.append(sub)\n",
    "                data = np.array(data)\n",
    "                (ein,aus,loss_val, _) = sess.run([input_,output_,ae.loss, ae.train], {p_input: data,p_is_training : True})\n",
    "                abs_err = abs(ein-aus)\n",
    "                for e in abs_err:\n",
    "                    errorBuf.append(e)\n",
    "            train_mean = np.array(errorBuf).ravel().mean()\n",
    "            \n",
    "            print(\"testing...\")\n",
    "            error_n_buf = []\n",
    "            for _ in range(len(test_n_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_n_list))\n",
    "                    sub = test_n_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_n_buf.append(e)\n",
    "            test_n_mean = np.array(error_n_buf).ravel().mean()\n",
    "            \n",
    "            error_a_buf = []\n",
    "            for _ in range(len(test_a_list)):\n",
    "                data = []\n",
    "                for _ in range(batch_num):\n",
    "                    ind = np.random.randint(0,len(test_a_list))\n",
    "                    sub = test_a_list[ind]\n",
    "                    data.append(sub)\n",
    "                    data = np.array(data)\n",
    "                    (input_n, output_n) = sess.run([input_, output_], {p_input: data, p_is_training: False})\n",
    "                    err_n = abs(input_n-output_n)\n",
    "                    for e in err_n:\n",
    "                            error_a_buf.append(e)\n",
    "            test_a_mean = np.array(error_a_buf).ravel().mean()\n",
    "            results.append([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])\n",
    "            f= open(\"C:/Users/Bin/Desktop/Thesis/code/src/GridSearch/power_results.txt\",\"a\")\n",
    "            f.write(str([hidden_num,step_num,train_mean,test_n_mean,test_a_mean,test_a_mean/test_n_mean])+\"\\n\")\n",
    "            f.close()\n",
    "            sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
