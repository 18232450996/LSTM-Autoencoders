{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/Bin/Desktop/Thesis/code')\n",
    "from Conf_EncDecAD_KDD99 import Conf_EncDecAD_KDD99\n",
    "from EncDecAD import EncDecAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration \n",
    "\n",
    "data_root = \"C:/Users/Bin/Documents/Datasets/KDD99/6_subsets_win/\"\n",
    "conf = Conf_EncDecAD_KDD99(data_root)\n",
    "#[sn_list, va_list, vn1_list, vn2_list, tn_list, ta_list] = conf.data_list\n",
    "\n",
    "#p_input = conf.p_input\n",
    "#p_inputs = conf.p_inputs\n",
    "\n",
    "\n",
    "\n",
    "batch_num = conf.batch_num\n",
    "hidden_num = conf.hidden_num\n",
    "step_num = conf.step_num\n",
    "elem_num = conf.elem_num\n",
    "\n",
    "iteration = conf.iteration\n",
    "modelpath_root = conf.modelpath_root\n",
    "modelpath = conf.modelpath\n",
    "decode_without_input = conf.decode_without_input\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: 5.18361\n",
      "iter 2: 5.86233\n",
      "iter 3: 3.78134\n",
      "iter 4: 3.73872\n",
      "iter 5: 3.81944\n",
      "iter 6: 3.32764\n",
      "iter 7: 2.52159\n",
      "iter 8: 3.28384\n",
      "iter 9: 2.72645\n",
      "iter 10: 2.16944\n",
      "iter 11: 2.11586\n",
      "iter 12: 1.97249\n",
      "iter 13: 1.53761\n",
      "iter 14: 1.33796\n",
      "iter 15: 1.20542\n",
      "iter 16: 1.02981\n",
      "iter 17: 0.960701\n",
      "iter 18: 0.842137\n",
      "iter 19: 0.766566\n",
      "iter 20: 0.763983\n",
      "iter 21: 0.716064\n",
      "iter 22: 0.646767\n",
      "iter 23: 0.572744\n",
      "iter 24: 0.513957\n",
      "iter 25: 0.488271\n",
      "iter 26: 0.436969\n",
      "iter 27: 0.431574\n",
      "iter 28: 0.404603\n",
      "iter 29: 0.400701\n",
      "iter 30: 0.358389\n",
      "iter 31: 0.323485\n",
      "iter 32: 0.315652\n",
      "iter 33: 0.28882\n",
      "iter 34: 0.295219\n",
      "iter 35: 0.267925\n",
      "iter 36: 0.262069\n",
      "iter 37: 0.246708\n",
      "iter 38: 0.259378\n",
      "iter 39: 0.234649\n",
      "iter 40: 0.218839\n",
      "iter 41: 0.205176\n",
      "iter 42: 0.213667\n",
      "iter 43: 0.203183\n",
      "iter 44: 0.200519\n",
      "iter 45: 0.176602\n",
      "iter 46: 0.186246\n",
      "iter 47: 0.165514\n",
      "iter 48: 0.206119\n",
      "iter 49: 0.158497\n",
      "iter 50: 0.159212\n",
      "iter 51: 0.168552\n",
      "iter 52: 0.149896\n",
      "iter 53: 0.149947\n",
      "iter 54: 0.167606\n",
      "iter 55: 0.146471\n",
      "iter 56: 0.151842\n",
      "iter 57: 0.133846\n",
      "iter 58: 0.134038\n",
      "iter 59: 0.135731\n",
      "iter 60: 0.155831\n",
      "iter 61: 0.128244\n",
      "iter 62: 0.140515\n",
      "iter 63: 0.125693\n",
      "iter 64: 0.139322\n",
      "iter 65: 0.117178\n",
      "iter 66: 0.13502\n",
      "iter 67: 0.138518\n",
      "iter 68: 0.116281\n",
      "iter 69: 0.107341\n",
      "iter 70: 0.118075\n",
      "iter 71: 0.138382\n",
      "iter 72: 0.108406\n",
      "iter 73: 0.116339\n",
      "iter 74: 0.106821\n",
      "iter 75: 0.113094\n",
      "iter 76: 0.107447\n",
      "iter 77: 0.101713\n",
      "iter 78: 0.101402\n",
      "iter 79: 0.101011\n",
      "iter 80: 0.0973553\n",
      "iter 81: 0.0957245\n",
      "iter 82: 0.0995814\n",
      "iter 83: 0.0985341\n",
      "iter 84: 0.0903437\n",
      "iter 85: 0.0985811\n",
      "iter 86: 0.0856328\n",
      "iter 87: 0.0906031\n",
      "iter 88: 0.0925519\n",
      "iter 89: 0.09509\n",
      "iter 90: 0.0906455\n",
      "iter 91: 0.0997721\n",
      "iter 92: 0.0951337\n",
      "iter 93: 0.0873918\n",
      "iter 94: 0.0974422\n",
      "iter 95: 0.0838951\n",
      "iter 96: 0.0881814\n",
      "iter 97: 0.0905629\n",
      "iter 98: 0.0812015\n",
      "iter 99: 0.091278\n",
      "iter 100: 0.0905999\n",
      "Model saved in file: C:/Users/Bin/Desktop/Thesis/tmp/52test/LSTMAutoencoder_kdd99_v1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\n    err_vec_list = []   \\n    for _ in range(len(conf.vn1_list)//batch_num):\\n        data =[]\\n        for temp in range(batch_num):\\n            ind = np.random.randint(0,len(conf.vn1_list)-1)\\n            sub = conf.vn1_list[ind]\\n            data.append(sub)\\n        data = np.array(data)\\n        (_input_, _output_) = sess.run([input_, output_], {p_input: data})\\n        err_vec_list.append(abs(_input_ - _output_))\\n    \\n    err_vec = np.mean(np.array(err_vec_list),axis=0).reshape(batch_num,-1)\\n    mu = np.mean(err_vec,axis=0)\\n    sigma = np.cov(err_vec.T)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f19d277780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "    p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "    \n",
    "    _enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "    _dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "    #inputs = conf.p_inputs\n",
    "    inputs = p_inputs #...\n",
    "    \n",
    "    reverse = True\n",
    "    decode_without_input = False\n",
    "    is_training = True\n",
    "    with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "        (z_codes, enc_state) = tf.contrib.rnn.static_rnn(_enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('decoder',reuse =True) as vs:\n",
    "\n",
    "        dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,elem_num], dtype=tf.float32),name=\"dec_weight_\")\n",
    "\n",
    "        dec_bias_ = tf.Variable(tf.constant(0.1,shape=[elem_num],dtype=tf.float32),name=\"dec_bias_\")\n",
    "\n",
    "\n",
    "        dec_state = enc_state\n",
    "        dec_input_ = tf.zeros(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "        dec_outputs = []\n",
    "\n",
    "        for step in range(len(inputs)):\n",
    "            if step > 0:\n",
    "                vs.reuse_variables()\n",
    "            (dec_input_, dec_state) =_dec_cell(dec_input_, dec_state)\n",
    "            dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "            dec_outputs.append(dec_input_)\n",
    "    \n",
    "        if reverse:\n",
    "            dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "        output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2])\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    input_= tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "    output_ = tf.transpose(output_, [0,1,2],name=\"output_\")\n",
    "    loss_ = tf.reduce_mean(tf.square(input_ - output_),name=\"loss_\")\n",
    "\n",
    "   \n",
    "    train_ = tf.train.AdamOptimizer().minimize(loss_)\n",
    "    #train_ = tf.train.GradientDescentOptimizer(0.01).minimize(loss_)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    loss = []\n",
    "    for i in range(iteration):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.sn_list)-1)\n",
    "            sub = conf.sn_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "\n",
    "        (loss_val, _) = sess.run([loss_, train_], {p_input: data})\n",
    "        loss.append(loss_val)\n",
    "        print('iter %d:' % (i + 1), loss_val)\n",
    "    pd.Series(loss).plot(title=\"Loss\")\n",
    "    \n",
    "\n",
    "\n",
    "    save_path = saver.save(sess, modelpath)\n",
    "    print(\"Model saved in file: %s\" % save_path) \n",
    "\n",
    "\n",
    "\"\"\"    \n",
    "    err_vec_list = []   \n",
    "    for _ in range(len(conf.vn1_list)//batch_num):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.vn1_list)-1)\n",
    "            sub = conf.vn1_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "        (_input_, _output_) = sess.run([input_, output_], {p_input: data})\n",
    "        err_vec_list.append(abs(_input_ - _output_))\n",
    "    \n",
    "    err_vec = np.mean(np.array(err_vec_list),axis=0).reshape(batch_num,-1)\n",
    "    mu = np.mean(err_vec,axis=0)\n",
    "    sigma = np.cov(err_vec.T)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae = EncDecAD(hidden_num, p_inputs, is_training = True, decode_without_input=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss = []\n",
    "    for i in range(iteration):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.sn_list)-1)\n",
    "            sub = conf.sn_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "        \n",
    "        (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data})\n",
    "        loss.append(loss_val)\n",
    "        print('iter %d:' % (i + 1), loss_val)\n",
    "    pd.Series(loss).plot(title=\"Loss\")\n",
    "\n",
    "    save_path = saver.save(sess, modelpath)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate parameters using Vn1 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Bin/Desktop/Thesis/tmp/52test/LSTMAutoencoder_kdd99_v1.ckpt\n",
      "Model restored.\n",
      "Initialized\n",
      "Got parameters mu and sigma.\n",
      "[ 2.47900534  2.65072298  1.92521286  2.28814459  2.61893225  1.64392376\n",
      "  1.65296149  3.38620949  2.63580346  1.36862111  0.77583998  1.4508338\n",
      "  1.46497953  4.20760441  1.06591856  0.90033042  2.40265799  2.78535223\n",
      "  1.45963299  0.93910503  1.53346193  4.04377556  0.77164221  1.38760459\n",
      "  0.93642557  1.9093492   1.73797488  3.55401564  2.77352738  2.59809399\n",
      "  1.58653104  2.28719878  2.74313807  2.71970558  2.04446077  1.91078532\n",
      "  1.36285734  2.60908341  2.43268967  1.60982823  1.89660001  2.82109928\n",
      "  3.07074809  1.38175988  0.62283087  1.08731341  1.30417895  3.23656464\n",
      "  0.59649086  1.21448803  2.65691829  2.38575554  1.99026322  0.75105131\n",
      "  1.59650135  3.7172184   0.82149935  1.6191628   1.50066483  1.62990415\n",
      "  1.91305232  2.61611128  2.63212967  2.09391522  2.57241273  2.06606412\n",
      "  2.89572048  2.60031962  1.5353272   1.35842359  1.05100644  3.11040616\n",
      "  2.27452898  1.76393759  1.90384769  2.3195312   3.25409937  1.50702286\n",
      "  0.85089004  1.26474071  1.00295532  2.11084175  1.09645295  1.74883592\n",
      "  2.35845447  1.92483747  2.44314003  1.03799331  1.5843066   3.31545186\n",
      "  1.05545819  1.57727611  1.97933614  1.3723917   1.2574991   2.94042158\n",
      "  2.51406741  2.45768785  2.8880825   2.03431821  2.91624832  2.65744472\n",
      "  1.53691792  2.02385879  0.90618068  2.94404936  1.97681081  1.739815\n",
      "  1.99881685  2.24928999  3.12304211  1.8650682   1.01101232  1.16281772\n",
      "  1.2062943   2.37492156  1.47585392  1.99748385  1.68774962  2.29339695\n",
      "  2.43956161  1.51912892  1.83988702  2.98946691  1.61249733  1.17545152\n",
      "  2.31165791  1.43019378  1.11072886  2.99085999  2.33867097  3.10834551\n",
      "  2.15370226  2.22419477  2.73255372  2.73251295  2.35416269  2.43499851\n",
      "  0.90208626  2.16445661  2.00745726  1.76343465  2.15180731  2.29328179\n",
      "  2.09552526  1.80846369  0.9429487   1.76780856  1.48709655  2.71667624\n",
      "  1.75981736  2.19300032  0.91756821  2.45946741  1.94745505  2.46280265\n",
      "  2.71873283  2.69259     1.65526581  1.51507211  2.66370535  1.46513176\n",
      "  1.54277718  2.35099006  1.85653555  3.1968739   1.59349787  1.86076796\n",
      "  2.50099516  2.66816473  3.34202266  2.29301453  0.79829967  1.42925549\n",
      "  2.0441823   1.82419837  1.72783411  2.16802359  1.34797072  1.29281878\n",
      "  1.3298322   2.59473014  1.79928839  2.63988352  2.40025187  2.23817158\n",
      "  0.82751477  2.39228678  1.60489309  3.36039615  2.24839568  2.49407053\n",
      "  1.37708199  2.20523286  2.91241264  1.18775356  1.29933202  1.64815962\n",
      "  0.94098437  2.75924206  1.36667717  1.15701306  2.18199873  1.79146957\n",
      "  3.46034551  2.40139842  0.71317577  1.08306944  1.89137268  1.92365432\n",
      "  1.51456952  1.7200197   2.05059791  0.77395147  1.01550221  3.15550661\n",
      "  1.63720679  2.13525915  2.59120893  1.85713315  0.85729361  2.15719295\n",
      "  1.54641438  3.86777306  1.61414838  2.37116623  1.40634859  3.54273272\n",
      "  2.92670774  2.17882419  0.90974075  1.58270192  1.06585872  2.91980767\n",
      "  1.6998148   1.70482886  1.82724023  1.92413461  2.96528482  2.06625605\n",
      "  0.79074514  1.06321239  2.09597325  2.14834833  2.14887214  1.2616626\n",
      "  2.99614429  0.74711043  0.64734852  2.94068861  1.25109982  1.14914703\n",
      "  2.35117483  1.44950008  0.60524476  1.71220088  1.32540083  3.52527118\n",
      "  1.63973069  2.28276753  1.66895795  4.33358192  2.25647521  3.04457259\n",
      "  0.91371965  1.69896793  1.70593989  2.79559278  2.08034325  1.8082478\n",
      "  2.44874525  2.39491296  2.21821451  1.43812215  0.81457633  1.21864271\n",
      "  2.23852396  2.26285529  2.26733971  1.01565552  3.41721416  1.63347697\n",
      "  1.1263895   2.13799477  1.31334364  1.18451905  2.07321882  1.39219761\n",
      "  0.65214956  1.31404507  1.19079947  2.18300772  2.03848886  2.46739578\n",
      "  1.66198087  4.16304588  1.97409558  3.55245733  1.1593976   1.75529861\n",
      "  2.09688616  2.09933901  2.29591513  1.14774859  2.62695003  2.34617496\n",
      "  1.6948452   1.76240122  0.9812659   1.16070545  1.85931718  2.28034115\n",
      "  2.43384719  1.15871453  2.93233824  2.43848515  1.56056845  1.87289262\n",
      "  1.34331119  2.82745457  1.53862071  1.37881565  0.84695548  1.13438666\n",
      "  1.40223014  1.6309036   1.86003745  2.71940088  1.35526729  3.43279696\n",
      "  1.99659312  3.89630818  1.03833079  1.81125867  1.46814466  1.18894398\n",
      "  2.15911484  0.63540244  2.44666147  2.20252419  1.17194963  1.79105818\n",
      "  1.13329446  0.7892288   1.1904254   2.11248207  2.90727973  1.23653102\n",
      "  2.40791893  2.78326178  1.3874464   1.73664355  1.54350924  3.7737689\n",
      "  1.4991014   0.98212928  1.39508986  0.9861573   1.89362395  1.66936207\n",
      "  2.17757797  2.71796274  0.91549331  2.44187975  1.95917201  3.98142862\n",
      "  0.84234393  1.75965345  0.93330795  0.64156449  1.64303529  0.67304814\n",
      "  2.32775569  2.0538938   0.89809382  1.60917723  1.2600584   1.03721428\n",
      "  0.92544878  1.74799597  2.60965586  1.02497697  1.99527168  2.45132804\n",
      "  0.83820218  1.52981305  1.31224585  3.85311127  1.47204089  0.52061456\n",
      "  1.48585713  0.85404599  2.19821572  2.1689117   2.49307704  2.29731059\n",
      "  0.5565263   1.70310557  2.37032366  3.66558194  1.43274033  2.05526853\n",
      "  1.17178845  0.56668097  1.04883981  0.7710036   1.81535304  1.87100959\n",
      "  1.05831122  1.5116837   1.36403775  1.81275439  1.22590697  1.34203005\n",
      "  1.8251524   0.92173576  1.50926685  1.89096332  0.46835962  1.7558645\n",
      "  0.85431993  3.3901341   1.13873732  0.61960661  1.10098052  0.63222367\n",
      "  2.27470875  2.36033678  2.44022441  1.65482402  0.61393625  1.47546518\n",
      "  2.52570486  2.90638566  1.86419201  2.61829042  1.41662979  0.71018487\n",
      "  0.68274748  0.62521994  1.33220637  1.55889332  1.45370841  1.59881377\n",
      "  1.40157723  2.20161772  1.48276341  0.86368769  1.16348159  0.85237753\n",
      "  1.149068    1.43374157  0.482117    1.65769386  0.90704584  2.58614016\n",
      "  1.28802347  0.71390992  0.55179596  0.49504057  2.48602033  2.00151443\n",
      "  2.05157065  1.27815175  0.73489726  1.26967514  2.54293203  1.83751357\n",
      "  1.76337492  2.97728848  1.518538    0.71489549  0.76236886  0.36663896\n",
      "  1.06211722  1.26275992  1.65000701  1.6473043   0.95341873  1.88201201\n",
      "  1.66449702  0.35966247  0.84283704  0.47422829  0.87523639  0.98849773\n",
      "  0.39407289  1.32905173  0.92823678  1.53698039  1.76005387  0.95874768\n",
      "  0.62314731  0.42511907  2.17637491  1.51337481  1.38287759  1.09142709\n",
      "  0.6978237   1.05425119  2.39588594  0.87711859  1.30683982  2.74785471\n",
      "  1.47690141  0.79564989  0.78283668  0.3943485   0.79944342  1.03210998\n",
      "  1.82301879  1.431144    0.24850245  1.1854018   1.51420832  0.60645014\n",
      "  0.56734455  0.22591195  0.58253235  0.48787874  0.57440633  1.09173298\n",
      "  0.98795396  0.56287068  1.82827151  1.17561412  1.07790482  0.2517603\n",
      "  1.36639428  0.98927498  0.69223887  0.90642804  0.53626734  0.87496263\n",
      "  2.05161142  0.36576051  0.79002702  2.09826922  1.3024559   0.87098992\n",
      "  0.56241167  0.69388914  0.39897829  0.90306652  1.84445822  0.95049077\n",
      "  0.91144609  0.57198471  1.00187302  0.84305799  0.29280409  0.44747099\n",
      "  0.58348155  0.11005834  0.67091805  0.8338688   1.11506248  0.31437591\n",
      "  1.40030742  1.2730664   1.17206836  0.18445016  0.6016385   0.53433895\n",
      "  0.39270782  0.79605383  0.42112488  0.67909682  1.66503847  0.21363799\n",
      "  0.44381389  1.33503544  0.9772827   1.03204322  0.29843444  0.86687613\n",
      "  0.10995796  0.90140134  1.65405273  0.51027107  1.39366817  0.16302758\n",
      "  0.5202117   0.71246612  0.21540049  0.55304408  0.62503326  0.30108434\n",
      "  0.65443701  0.47493464  1.00664485  0.60651189  0.67430514  1.32060826\n",
      "  0.77734697  0.45785195  0.15152487  0.38934204  0.28686672  0.95574313\n",
      "  0.46311969  0.46809086  1.19971681  0.32465279  0.35558859  0.57069105\n",
      "  0.51494241  1.02481794  0.22177486  1.03012776  0.47371945  0.97322023\n",
      "  1.38378489  0.39782828  1.48343146  0.10987286  0.28841135  0.41742644\n",
      "  0.37787789  0.63391423  0.49374777  0.62150878  0.32252476  0.15835048\n",
      "  0.70868236  0.71137297  0.11364911  1.32986116  0.26136231  0.50545251\n",
      "  0.19435135  0.59026212  0.20639019  0.98183173  0.55315566  0.30093238\n",
      "  0.64462352  0.51051807  0.58897829  0.15749505  0.1960974   0.69660985\n",
      "  0.31174654  1.10656428  0.62760127  0.89657372  1.0148766   0.50799024\n",
      "  1.39221585  0.25702637  0.24957284  0.08774756  0.60976356  0.4673214\n",
      "  0.27019376  0.87449867  0.59985834  0.18301314  0.4441708   0.85569286\n",
      "  0.35645938  0.9172042   0.22839686  0.3328976   0.22399473  0.75932497\n",
      "  0.15390639  0.70967859  0.78302211  0.24537139  0.33317316  1.1586138\n",
      "  1.15203214  0.39556155  0.75092065  0.28968123  0.21997234  0.86287737\n",
      "  0.72475773  0.19584166]\n"
     ]
    }
   ],
   "source": [
    "from Parameter_helper import Parameter_Helper\n",
    "\n",
    "para = Parameter_Helper(conf)\n",
    "\n",
    "mu, sigma = para.mu_and_sigma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate anomaly score, get threshold t using Vn2 and Va dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = para.get_threshold(mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, modelpath)  # decode_without_input=True, iter=5000\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    \n",
    "    normal_score = []\n",
    "    n_in = []\n",
    "    n_out = []\n",
    "    a_in = []\n",
    "    a_out = []\n",
    "    \n",
    "    for count in range(len(tn_list)//batch_num):\n",
    "        normal_sub = np.array(tn_list[count*batch_num:(count+1)*batch_num]) \n",
    "        (input_n, output_n) = sess.run([ae.input_, ae.output_], {p_input: normal_sub})\n",
    "        n_in.append(input_n)\n",
    "        n_out.append(output_n)\n",
    "        err_n = abs(input_n-output_n).reshape(-1,step_num)\n",
    "        err_n = err_n.reshape(batch_num,-1)\n",
    "        for batch in range(batch_num):\n",
    "           temp = np.dot( (err_n[batch] - mu ).reshape(1,-1)  , sigma.T)\n",
    "           s = np.dot(temp,(err_n[batch] - mu ))\n",
    "           normal_score.append(s[0])\n",
    "           \n",
    "    abnormal_score = []\n",
    "    for count in range(len(ta_list)//batch_num):\n",
    "        abnormal_sub = np.array(ta_list[count*batch_num:(count+1)*batch_num]) \n",
    "        (input_a, output_a) = sess.run([ae.input_, ae.output_], {p_input: abnormal_sub})\n",
    "        a_in.append(input_a)\n",
    "        a_out.append(output_a)\n",
    "        err_a = abs(input_a-output_a).reshape(-1,step_num)\n",
    "        err_a = err_a.reshape(batch_num,-1)\n",
    "        for batch in range(batch_num):\n",
    "           temp = np.dot( (err_a[batch] - mu ).reshape(1,-1)  , sigma.T)\n",
    "           s = np.dot(temp,(err_a[batch] - mu ))\n",
    "           abnormal_score.append(s[0])\n",
    "             \n",
    "\n",
    "    print('Predict result :')\n",
    "\n",
    "    pd.Series(normal_score).plot(label=\"normal_score\",figsize=(18,5))\n",
    "    pd.Series(abnormal_score).plot(label=\"abnormal_score\")\n",
    "    bar = threshold*np.ones(len(normal_score)+len(abnormal_score))\n",
    "    pd.Series(bar).plot(label=\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "fp = len(abnormal_score)-tp\n",
    "fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "tn = len(normal_score)- fn\n",
    "P = tp/(tp+fp)\n",
    "R = tp/(tp+fn)\n",
    "fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tp,fp,tn,fn,P,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp/fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
