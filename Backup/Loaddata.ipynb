{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loaddata(object):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.folder = \"C:/Users/Bin/Documents/Datasets/\"\n",
    "        self.kdd99_col_name_suffix = \"KDD99/columns.txt\"\n",
    "        self.kdd99_dataset_suffix = \"KDD99/kddcup.data_10_percent_corrected\"\n",
    "        self.power_demand_suffix = \"EncDec-AD dataset/power_data.txt\"\n",
    "        \n",
    "    def read(self):\n",
    "        if self.dataset == \"kdd99\":\n",
    "            with open(self.folder+self.kdd99_col_name_suffix) as col_file:\n",
    "                line = col_file.readline()\n",
    "            columns = line.split('.')\n",
    "            col_names = []\n",
    "            col_types = []\n",
    "            for col in columns:\n",
    "                col_names.append(col.split(': ')[0].strip())\n",
    "                col_types.append(col.split(': ')[1])\n",
    "            col_names.append(\"label\")\n",
    "            df = pd.read_csv(self.folder+self.kdd99_dataset_suffix,names=col_names)\n",
    "            data = df.iloc[:,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()   #Select only numeric features\n",
    "            label = df.iloc[:,-1]\n",
    "\n",
    "            # Scaling\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(data)\n",
    "            data = scaler.transform(data) \n",
    "            \n",
    "            return data\n",
    "        \n",
    "        elif self.dataset == \"power_demand\":\n",
    "            \n",
    "            power = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/power_data.txt\",names=[\"power_demand\"])\n",
    "            # downsample the dataset by 8 to obtain non-overlapping sequences with L=84 such that each window corresponds to one week\n",
    "            sub_power = pd.Series(power[490:].reset_index(drop=True)[\"power_demand\"])\n",
    "            index = [8*t for t in range(sub_power.shape[0]//8 +1)]\n",
    "            sub_power = sub_power[index].reset_index(drop=True)  # shape(4319,)\n",
    "            #Scaling\n",
    "            sub_power = sub_power.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(sub_power)\n",
    "            sub_power = scaler.transform(sub_power) \n",
    "            \n",
    "            # split the normal dataset into training_normal(15), validation_1(9), validation_2(9), test_normal(12), \n",
    "            # split the anomalous dataset into validation_anomaly(3), test_anomaly(3)\n",
    "            # here the anomalous data are manually labeled\n",
    "            sub_power_ = np.array([sub_power[t][0] for t in range(sub_power.shape[0])])\n",
    "            sub_power_list = [sub_power_[t*84:(t+1)*84] for t in range(51)]\n",
    "            \n",
    "            anomalous_indices = [11,12,16,17,19,50]\n",
    "            normal_indices = list(set(range(51))-set(anomalous_indices))\n",
    "            dataset_normal = [sub_power_list[i] for i in normal_indices]\n",
    "            dataset_anomalous = [sub_power_list[j] for j in anomalous_indices]\n",
    "            \n",
    "            training_normal = np.concatenate(dataset_normal[:15])\n",
    "            validation_1 = np.concatenate(dataset_normal[15:24])\n",
    "            validation_2 = np.concatenate(dataset_normal[24:33])\n",
    "            test_normal = np.concatenate(dataset_normal[33:45])\n",
    "            whole_normal = np.concatenate((training_normal, validation_1, validation_2, test_normal))\n",
    "            \n",
    "            validation_anomaly = np.concatenate(dataset_anomalous[:3])\n",
    "            test_anomaly = np.concatenate(dataset_anomalous[3:])\n",
    "            whole_anormaly = np.concatenate((validation_anomaly,test_anomaly))\n",
    "            \n",
    "            return [training_normal, validation_1, validation_2, test_normal, validation_anomaly, test_anomaly,whole_normal,whole_anormaly]\n",
    "        \n",
    "        elif self.dataset == \"space_shuttle\":\n",
    "            tek17 = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/TEK17.txt\",header=None)\n",
    "            tek16 = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/TEK16.txt\",header=None)\n",
    "            tek14 = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/TEK14.txt\",header=None)\n",
    "            tek = pd.concat([tek14,tek16,tek17],axis=0).reset_index(drop=True)\n",
    "            # downsample the dataset by 3 \n",
    "            sub_tek = pd.Series(tek[0])\n",
    "            index = [3*t for t in range(tek.shape[0]//3)]\n",
    "            sub_tek = sub_tek[index].reset_index(drop=True)\n",
    "            #Scaling\n",
    "            sub_tek = sub_tek.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(sub_tek)\n",
    "            sub_tek = scaler.transform(sub_tek) \n",
    "            #Applying sliding window, window length 1500/3=500, step_size 500/3= 166\n",
    "            STEP_SIZE = 500//3 # downsampled by 3\n",
    "            WINDOW_LENGTH = 1500//3   # downsampled by 3\n",
    "            t = 1\n",
    "            sequence = sub_tek[:WINDOW_LENGTH]\n",
    "            while t*STEP_SIZE+WINDOW_LENGTH <=sub_tek.size:\n",
    "                sequence = np.concatenate((sequence,sub_tek[t*STEP_SIZE:t*STEP_SIZE+WINDOW_LENGTH]))\n",
    "                t = t+1\n",
    "            #return a timeseries after appling sliding window\n",
    "            return sequence\n",
    "        \n",
    "        elif self.dataset == \"ecg\":\n",
    "            # use the first channel of the qtdb/sel102 dataset\n",
    "            ecg = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/EncDec-AD dataset/qtdbsel102.txt\",header=None,usecols=[1],sep=\"\\t\")\n",
    "            ecg = pd.Series(ecg[1])\n",
    "            \n",
    "            #Scaling\n",
    "            ecg = ecg.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(ecg)\n",
    "            ecg = scaler.transform(ecg) \n",
    "            \n",
    "            return ecg\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(\"Wrong dataset name\")\n",
    "            return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
