\chapter{Experimental results}
\label{chap:results}


\section{Anomaly detection performance}
\label{sec:performance}

With parameters learned from \Fref{sec:parametertuning}, autoencoders are trained for each dataset with the beginning of streaming data. The anomaly detection performance is indicated by AUC. For each dataset, we compare the AUC of online phase that without and with continuously model and parameter updating (\Fref{tab:performance}). For the PowerDemand dataset, retrain trigger depends on the batch performance. And for the rest datasets, retraining only triggered when retrain buffers are full. The retraining brings overall performance improvement on all datasets comparing to stationary models. Especially in the SMTP+HTTP dataset, the stationary without learning concept drifted knowledge performs clearly worth than the model with updating.

\begin{table}[h] 
\caption{Performance} 
\centering      
\begin{tabular}{c | c | c | c}  
\hline  
Dataset & AUC(without retraining) & AUC(with retraining) & \#retrain \\ 
\hline 
PowerDemand & 0.91 & 0.97 & 2  \\  
\hline 
SMTP & 0.94 &  0.98 &  2 \\ 
\hline 
HTTP & 0.76 &  0.86 &  2 \\ 
\hline
SMTP+HTTP & 0.64 & 0.85 & 4 \\
\hline 
ForestCover &0.67&0.74 & 3\\   
\hline    
\end{tabular}
\label{tab:performance}  
\end{table} 
In order to compare the performance with and without retraining, and after each retraining, another example is to calculate the AUC value for each specified time period. As Shown in \Fref{fig:auc_retrain}, the x-axis is the periods before first retraining(shown as P1 in each subplot), between first and second retraining, and so on. For each dataset, we compare the AUC value of stationary model (trained with only initialization set) and adaptive model (online updated). For most cases, the adaptive models outperform stationary models, which shows the models profits from the knowledge updating over streaming data. To be notice that the SMTP+HTTP set contains sudden concept drift around P3, which leads to a sharp decline of the stationary model. In the meantime, the adaptive model is slightly influenced by the mixed knowledge at P3 but keeps outstanding performance when the stream switched to HTTP side.\\
\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=15cm]{auc_retrain}
\caption[AUC comparation between stationary and adaptive models over stream]{AUC comparation between stationary and adaptive models over stream: The x-axes represent specific periods over stream. For example, P1 is the period from beginning to the first retraining, and P2 is the the period between first and second retraining etc.}
\label{fig:auc_retrain}
\end{figure}

\Fref{fig:runtime} shows the run time used for both stationary models and adaptive models for each data set. The updating takes more time for HTTP, SMTP+HTTP and FORESR than PowerDemand and SMTP is due to that larger datasets take more time for prediction, and trigger more updating events. And for each retraining, the retrain buffers also contains larger retraining sets.



\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=7cm]{runtime}
\caption[Run time statistics]{Run time statistics}
\label{fig:runtime}
\end{figure}


\section{Synthetic example}
\label{sec:synthetic}

In order to show the benefit of model retraining along the stream, we demonstrate the online learning process of the small set Power demand in this section. The PowerDemand dataset does not contain clear incremental or sudden concept drift, but the normal pattern still different slightly to each other. Lack of overall impression during the model initialization phase can lead to failures during the online phase. 

\subsection{Reaction of concept drift}
\label{sec:reaction}

\Fref{fig:power_retraining} shows 3 continual days power demand in normal state. Due to the lack of knowledge for current pattern, the autoencoder reconstructs the input time series higher than desired on day 1 (left diagram). This could be caused by seasonal changes on the power demand, which is slightly, gradually, and not able to cause misclassify directly. However, the increase of normal data reconstruction error makes the margin between two classification classes smaller, and harder to make decision. As a consequence, the model retraining process is triggered after the second day with last seen data in the retrain buffer, and the model performs well again on the third day.

\begin{figure}[h]
\centering
\includegraphics[width=15cm, height=4cm]{power_retraining}
\caption[Retraining effect on Power Demand dataset]{Retraining effect on Power Demand dataset}
\label{fig:power_retraining}
\end{figure}

\subsection{Retaining}
\label{sec:retrainig}

During the online phase, the model is retrained two times, before batch No.10 and No. 27. After retraining, the normal data reconstruction error becomes lower while for abnormal data becomes higher, so that the classification becomes easier.

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{power_online_score}
\caption[Power Demand dataset online learning scores]{Power Demand dataset online learning scores: For each window, the anomaly score is the highest pointwise scores within the window}
\label{fig:power_online}
\end{figure}

After each retraining process, the parameters mu, sigma and threshold of anomaly scores are also updated. \Fref{fig:parachanges} shows the parameter changes over the stream. As there is no clear concept drift during the power demand stream, the parameters change just slightly in oder to learn latest knowledge from the retrain buffer. 

\begin{figure}[h]
\centering
\includegraphics[width=6cm, height=4cm]{para_update}
\caption[Online parameter updating]{Online parameter updating}
\label{fig:parachanges}
\end{figure}



\section{Model retraining}
\label{sec:retraining}

\subsection{Reaction of sudden and drastic concept drift}
\label{sec:reaction}
The main advantage of online model is its ability to take reaction against sudden data distributional changes in time. The SMTP+HTTP data set is composed by directly connect HTTP set after SMTP, so there is a sudden concept drift in between. The model is initialized with only SMTP data, so HTTP is completely unknown knowledge for the model. \Fref{fig:smtp+http} is a box plot of anomaly scores of normal instances from different part of the stream. The block B1 is a statistic of normal instances’ anomaly scores between the last model updating on the SMTP side and the concept drift happening, which is relative lower due to the good grasp of SMTP data. Once the concept drift takes place, namely, HTTP data arrives along with the stream, more normal instances with higher anomaly score appears in B2. Although a retraining process is triggered soon after the concept drift, the normal instances’ anomaly scores still increase due to lack of HTTP instance. Gradually, with the increasing amount seen HTTP data, the model gives normal data lower anomaly score again during B4 to B6. As a result, we can observe that, when a sudden concept drift happened in the stream, our model needs only 3 to 4 times retraining with totally 3500 instances for retraining to master the new data distribution again.

\begin{figure}[h]
\centering
\includegraphics[width=13cm, height=6cm]{smtp+http}
\caption[Anomaly score box chart of SMTP+HTTP]{Anomaly score box chart of SMTP+HTTP}
\label{fig:smtp+http}
\end{figure}

\subsection{Reaction of serried and slight concept drift}
\label{sec:reaction}

Sometimes concept drift over the stream are slight, periodically, and potentially repeated. A single slight concept drift may not be able to trigger the retraining, but new knowledge should be saved into retraining buffer, so that once the model retrained with the fresh knowledge, the model should perform well when the same concept drift happens. We experiment with the ForestCover dataset. There are 7 kinds of forest cover types as labels. We take the least type No.4 as anomaly while the rest 6 kinds as normal. Cover types appears alternately over the stream, so that it could be treated as slight concept drift.\\

In the beginning, 3000 windows are used for initialization, and 26050 windows comes as stream.  Every normal window contains more than 10 scores over threshold is treated as hard window and appended to retraining buffer. Also, every abnormal window is saved in anomaly buffer for threshold updating.  When normal buffer size reaches 750 and anomaly buffer is not empty, a updating process will be triggered. If there is not anomaly data avaliable, the normal buffer is maintained as a queue, where oldest windows are discarded, and latest windows are appended. The model retraining is triggered 3 times over the ForestCover stream. 

\begin{figure}[t]
\centering
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[height=4cm]{pie1}
	\caption{Initialization dataset}
	\label{fig:init}
	\end{subfigure}
	~
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[height=4cm]{pie2}
	\caption{Retrain Buffer 1}
	\label{fig:buf1}
	\end{subfigure}
	~
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[ height=4cm]{pie3}
	\caption{Retrain Buffer 2}
	\label{fig:buf2}
	\end{subfigure}
	~
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[height=4cm]{pie4}
	\caption{Retrain buffer 3}
	\label{fig:buf3}
	\end{subfigure}

	\caption[FOREST data initializaiton and retrain buffer data distribution]{FOREST data initializaiton and retrain buffer data distribution: N1 to N6 represent normal states from the 6 different cover type classes, and A represents anomaly, namely type4. (a) is the initialization data distribution and (b), (c), (d) are the three retrain buffers' data distribution. Outer rings are all seen data since last retraining and inner rings are the portions added to retrain buffer.}
\label{fig:bufs}
\end{figure}








