\chapter{Preliminaries}
\label{Preliminaries}

\section{Definition of a stream}
\label{sec:Definition of a stream}

Definition of a stream(time series, dimensionality, volume, velocity, label)
Assuming that there are some devices or data warehouse that generate data continuously with a velocity V (here we only taking about numerical data). The data stream from 1st timestamp until ith timestamp is descripted as:
	                             ********
	
Where *** represent the instance at timestamp t in the data stream. And we assume the volume of data stream is infinity, which means, there are always available data instances generated by the data source.
To be more generally, we consider *** as either univariate or multivariate, *** is defined as 
				************

Where <f*1,…,f*N> is the feature space of the data stream with size N. For each instance ***, the label y*t = {0,1} tells either the instance is normal or abnormal.


\section{Method of processing the stream}
\label{sec:Method of processing the stream}

For further online processing and detection, we generate mini-batches upon the data stream. The streaming data is accumulated as window W, and a mini-batch consist of one or more windows.
	                             ********

			**********
Where W*t is a window with length WN start from instance at timestamp t, B*t is a mini-batch consists of BN windows starting from window  W*t.

\section{Defination of outlier}
\label{sec:Defination of outlier}

Pointwise
A data point (instance) is anomalous if this point is distant from other observations according to some specific measurement metrics. This is used in fine-grained anomaly detection tasks, that need to find out every single anomalous instance, e.g. credit card fraud detection, spam email detection.

Window-based
A window is anomalous if the window contains one or more anomalous data points. For most of the window-based anomaly detection algorithm, they only calculate the anomaly score of a given window, it’s hard and sometimes not necessary to find out which data points of this window are anomalous.

\section{LSTMs and autoencoder}
\label{sec:LSTMs and autoencoder}

Recurrent neural networks(RNNs) are widely used for speech, video recognition and prediction due to its recurrent property that captures the temporal dependency between data in compare with feed forward networks. However, the volume of RNN’s memory is limited, and vanishing gradient is also a difficulty by training RNNs. Therefore, the long short-term memory networks (LSTMs) are a kind of reinforced RNN that is able to remember meaningful information in arbitrary time interval. A LSTM network is a recurrent neural network with neurons being LSTM units.


A single LSTM unit can be unfolded over time. The LSTM unit take a data window as input, one data point at a specific time point for each step. Therefore, the LSTM unit extracts useful and drop useless temporal information for the window of data.
	
Deep LSTM RNNs are built by stacking multiple LSTM layers. Note that LSTM RNNs are already deep architectures in the sense that they can be considered as a feed-forward neural network unrolled in time where each layer shares the same model parameters. It has been argued that deep layers in RNNs allow the network to learn at different time scales over the input. ( TrainingandAnalyzingDeepRecurrentNeural Networks)

An autoencoder is an artificial neural network with symmetrical structure. Normally an autoencoder has at least one hidden layer that consists of less neurons than input and output layers. And the basic aim of autoencoders is to reconstruct its own input and learn a lower dimensional representation (encoding) of input data in the hidden layer. Moreover, the autoencoders are also used for anomaly detection by measuring the reconstruction error between inputs and predictions.
Normally the component between input layer and hidden layer is called encoder***of the autoencoder, and the symmetrical component between hidden layer and output layer is called decoder ***. For input X, the objective function is to find weight vectors for encoder and decoder to minimize the reconstruction error.
	 
 

