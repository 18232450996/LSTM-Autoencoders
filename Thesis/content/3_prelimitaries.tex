\chapter{Preliminaries}
\label{Preliminaries}

\section{Definition of stream}
\label{sec:Definition of stream}

Assume that there are some devices or data warehouse that provide data continuously with a specific velocity V (here we only take about numerical data). An input window is defined as
\begin{equation} \label{eq:timeseries}
X^{(i)}=\{x_1^{(i)},x_2^{(i)}, ... , x_T^{(i)}\}
\end{equation}
	                        
	
where \textbf{$X^{(i)}$} is the \textbf{$i^{th}$} input window with length T, and \textbf{$x_t^{(i)}$} is the \textbf{$t^{th}$} instance in the window, which is a multi-dimensional vector.\\

In each single time, a mini-batch B consists of multipe windows is used for model training or testing, and we assume that the data stream is infinity, so that mini-batches can always be accumulated from data stream. The equation \ref{eq:minibatch} is a mini-batch start from window \textbf{$X^{(i)}$} with batch size MB. And equation \ref{eq:stream} describes the data stream.
\begin{equation} \label{eq:minibatch}
B^{(i)}=\{X^{(i)},x^{(i+1)}, ... , x^{(i+MB-1)}\}
\end{equation}

\begin{equation} \label{eq:stream}
S=\{..., B^{(i)},B^{(i+MB)}, B^{(i+2MB)},...\}
\end{equation}



\section{Defination of anomlaies}
\label{sec:Defination of anomalies}

\textbf{Pointwise}: A data point (instance) is anomalous if this point is distant from other observations according to some specific measurement metrics. This is used in fine-grained anomaly detection tasks, that need to find out every single anomalous instance, e.g. credit card fraud detection, spam email detection.\\

\textbf{Window-based}: A window is anomalous if the window contains one or more anomalous data points. For most of the window-based anomaly detection algorithm, they only calculate the anomaly score of a given window, it’s hard and sometimes not necessary to find out which data points in this window are those anomalies.\\

In the anomlay detection experiments, normal data is treated as positive class and anomalies as negative class. The confusion matrix is shown in Table~\ref{tab:confusion}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{Actual value}\\
\cline{3-4}
\multicolumn{2}{c|}{}&Normal&Abnormal\\
\cline{2-4}
\multirow{2}{*}{Prediction}& Normal & $TP$ & $FP$\\
\cline{2-4}
& Abnormal & $FN$ & $TN$\\
\cline{2-4}
\end{tabular}
\end{center}
\label{tab:confusion}
\caption{Confusion matrix}
\end{table}

The target is to achieve higher true positive rate (equation \ref{eq:tpr}, predict normal data correctly) and while remain lower false positive rate (equation \ref{eq:fpr},miss classify anomalies as normal). The evaluation metric is Area Under the Curve (AUC), where curve represents the receiver operating characteristic curve, and is created by plotting the true positive rate against the false positive rate at various threshold settings.
\begin{equation} \label{eq:tpr}
TPR =\dfrac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:fpr}
FPR = \dfrac{FP}{FP+TN}
\end{equation}



\section{LSTMs}
\label{sec:LSTMs}

Recurrent neural networks(RNNs) are widely used for speech, video recognition and prediction due to its recurrent property that captures the temporal dependency between data in compare with other feed forward networks. However, the volume of RNN’s memory is limited, and vanishing gradient is also a difficulty by training RNNs. Therefore, the long short-term memory networks (LSTMs) are a kind of reinforced RNN that is able to remember meaningful information in arbitrary time interval. A LSTM network is a recurrent neural network with neurons being LSTM units. \Fref{fig:lstmunit} shows a classical structure of a LSTM unit. LSTMs are able to capture long-term memory while there are a forget gate and a update gate in the LSTM unit, that select necessary previous information and new coming information according to the input data at each time step. The information is transferred to the next step within the cell state. Besides, each LSTM units also output its value by going through a softmax function.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm, height=8cm]{lstmunit}
\caption[LSTM unit]{The LSTM unit}
\label{fig:lstmunit}
\end{figure}


A LSTM unit can be unfolded over time, as shown in \Fref{fig:unfolded}. The LSTM unit takes a data window as input, namyly takes one instance at a time. Therefore, the LSTM unit extracts useful and drop useless temporal information from the window of data.\\


Deep LSTM RNNs are built by stacking multiple LSTM layers. Note that LSTM RNNs are already deep architectures in the sense that they can be considered as a feed-forward neural network unrolled in time where each layer shares the same model parameters. It has been argued that deep layers in RNNs allow the network to learn at different time scales over the input\cite{deep}. \Fref{fig:deeplstm} is a example of stacked deep LSTM neural network, there are 3 LSTM layers, each can be unfolded into 5 time steps, so the LSTMs take a window in length 5 as input and the output is in same size.

\begin{figure}[h]
\centering
\includegraphics[width=12cm, height=5cm]{unfold}
\caption[Unfolded LSTM unit]{Unfolded LSTM unit}
\label{fig:unfolded}
\end{figure}

\begin{figure}[hb]
\centering
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[height=6cm]{deepfolderedlstm}
	\caption{Deep folded LSTMs}
	\label{fig:deeplstm1}
	\end{subfigure}
	~
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[ height=6cm]{deeplstm}
	\caption{Deep unfolded LSTMs. Each horizontal dark dot chain is an unfoldered LSTM unit over time, hollow dots and grey dots are windows of inputs and outputs.}
	\label{fig:deeplstm2}
	\end{subfigure}
	\caption[Deep LSTMs]{Deep LSTMs}
\label{fig:deeplstm}

\end{figure}

\section{Autoencoders}
\label{sec:Autoencoders}

An autoencoder is an artificial neural network with symmetrical structure. Normally an autoencoder has at least one hidden layer that consists of less neurons than input and output layers. And the basic aim of autoencoders is to reconstruct its own input and learn a lower dimensional representation (encoding) of input data in the hidden layer. Moreover, the autoencoders are also used for anomaly detection by measuring the reconstruction error between inputs and predictions.
Normally the component between input layer and hidden layer is called encoder, and the symmetrical component between hidden layer and output layer is called decoder. For input $\chi$, the objective function is to find weight vectors for encoder and decoder to minimize the reconstruction error (\ref{eq:autoencoder}).
\begin{equation} \label{eq:autoencoder}
\begin{aligned}
\Phi : &\chi \rightarrow H \\
\Psi : &H \rightarrow \chi \\
\phi, \Psi = &argmin \left \| \chi-(\Psi \circ \Phi)\chi \right \| 
\end{aligned}
\end{equation}

LSTMs-autoencoder has the same encoder-decoder architecture, while the neurons are LSTM units and connected in the way described in section \ref{sec:LSTMs}. \Fref{fig:encdecad} is a basic LSTMs-based autoencoder architecture with single LSTM layer on both encoder and decoder side. Our incremental LSTMs-autoencoder is based on this structure. The model takes window with length T as input (one instance in each step). The cell state carries sequence information and is passed through LSTM unit over time. When the encoder reaches the last encoder state, namely ET in \Fref{fig:encdecad2}, the cell state is actually the fix length embedding of the input window, and copied to the decoder as initial cell state of decoder, so that the input information is also transferred to the decoder. And the decoder predict the window in reversed order in order to make the optimization problem easier. To be notice is, different from aforementioned deep LSTMs in section \ref{sec:LSTMs}, the encoder outputs at each time step are not directly used as inputs of decoder, while between the encoder and decoder is actually not the same logical connection as stacked LSTMs. Here, the outputs of encoder are ignored, and there are different works contributes to the research of decoder inputs. Cho et al. \cite{phraserepresentation} feeds the input sequence to the decoder for a learning phrase representation task, Malhotra et al. \cite{encdecad} feed to decoder LSTM unit at each time step the prediction of last time step as input, and in a extended work \cite{timenet} they feed the decoder always a constant vector for an anomaly detection task, because the finial cell state already carries all relevant information to represent the input window. In our model, we feed the decoder a constant vector.
 
\begin{figure}[hb]
\centering
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[height=6cm]{encdecad1}
	\caption{Folded LSTMs-Autoencoder}
	\label{fig:encdecad1}
	\end{subfigure}
	~
	\begin {subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[ height=6cm]{encdecad2}
	\caption{Unfolded LSTMs-Autoencoder}
	\label{fig:encdecad2}
	\end{subfigure}
	\caption[LSTMs-Autoencoder]{LSTMs-Autoencoder}
\label{fig:encdecad}

\end{figure}

\section{Streaming data generator: Apache Kafka}
\label{sec:Streaming data generator: Apache Kafka}

We utilize Apache Kafka as the streaming platform. Kafka is a widely used Publish/Subscribe architecture streaming system. It different from classical message queue technique with its fault tolerant, durable and large capacity properties. In the experimental setting, our data source is static databases, Kafka generate real-time data stream pipeline as data source publish records to the specific topic (topic is the data category mechanisms used in Kafka), and furthermore the stream of records will be consumed by different consumers like our analysis model, visualization model etc. This configuration can be easily scaled up to more complicated and demanding real world use cases. Each record in the Kafka stream pipeline is in the form of [Key, Value, Timestamp], where keys are used for positioning and values carry the data record.
