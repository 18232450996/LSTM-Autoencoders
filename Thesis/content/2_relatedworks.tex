\chapter{Related works}
\label{chap:relatedworks}

In this section, we present a survey on previous works in anomaly detection. Both batch models and online models are listed with respectively classical machine learning approaches and neural network as well as autoencoder based approaches. And for the online case, we also survey works with neural network updating approaches.

\section{Batch model}
\label{sec:batch}

\subsection{Classical machine learning based approaches}
\label{sec:classical}

As an important component of data mining and machine learning, anomaly detection has been investigated using plenty of efficient models. When talking about anomaly detection, the most intuitive solutions are detection of outliers from a dense cluster, or to find data points that have obvious different property as their neighbors. Considering the lack of label and extremely imbalanced dataset, unsupervised approaches are more widely used in practice, for example Local Outlier Factor (LOF).\\

In anomaly detection, the LOF is a common density-based approach. LOF shares some concepts with DBSCAN such as ‘core distance’ and ‘reachability distance’, in order to estimate local density. Here, points with substantially lower local density than their neighbors are considered as anomalies. LOF shows competitive performance in many anomaly detection tasks, especially when dealing with data with unevenly density distribution. However, when getting a numerical factor from LOF model as result, it is actually hard to define a threshold automatically for the judgement of anomaly.\\

Because the anomalies appear rarely in the dataset, and occur usually in novel ways, it is expensive to label and hard to learn all kinds of anomalies during the training phase, so unsupervised models are commonly used. There are also a batch of semi-supervised or one-class anomaly detection models. The intuitive difference between anomaly detection and binary classification problem is the obvious fewer positive class data (anomalies). A typical one-class model is the One-class Support Vector Machine (OCSVM).\\

As an semi-supervised one-class classifier, OCSVM takes only normal data as input, and generates a decision surface to separate them from the anomaly data. By analyzing anomalies, the datasets are always bias to the normal part, and anomaly appear only rarely. So, this kind of one-class classifiers avoid making balance between the two classes. Besides, they also take advantage of classical support vector machine by using kernel methods, they can also deal with linearly not separable data. However, in the meantime, choosing a proper kernel becomes a hard problem for OCSVM. A suboptimal kernel function can seriously impact the performance.\\

Although classical machine learning approaches can handle most of the normal anomaly detection, there is still a lack of pervasive models that fit different kinds of data characters. Moreover, only few of those approaches could be directly or after some modification used for time series and streaming data, while they ignore the temporal dependency between samples.



\subsection{Autoencoder-based batch approaches}
\label{sec:Autoencoder-based batch approaches}

LSTMs-Autoencoders are originally widely used for text generation because the LSTMs are able to capture the contextual dependency between words and sentences. Text data are usually embedded into vector as input data of autoencoder. And the tasks are either to generate temporal relevant text on the decoder side or to learn text representation from the hidden layer \cite{phraserepresentation}. Similar problem appears also in time series mining due to the temporal dependency of values between timestamps. So, in later works, LSTMs-Autoencoder are also used for time series data.\\

Sutskever et al. \cite{seq2seq} proposed a deep LSTMs-based sequence-to-sequence model for language translation. In their work, the deep LSTMs encoder takes single sentence as input, and learn a hidden vector with fixed length, then a different LSTMs decoder decodes the hidden vector to the target sentence.  As a translation task, they found that this encoder-decoder architecture can capture long sentences and sensible phrases, especially they achieved better performance with deep LSTMs in compare with shallow LSTMs. In addition, a valuable found is, reversing the order of words in the input sentence makes the optimization problem much easier and achieved better performance. The LSTMs based model outperforms non-LSTMs model on the long input sentence cases (more than 35 words) since its long-term memory ability.\\

Li et al. \cite{hierarchicalseq2seq} did similar research on long paragraph text and even entire document generation using LSTMs-autoencoders. Their main contribution is the hierarchical sentences representation. The model learns word-level, sentence-level and paragraph-level information with each respectively a LSTMs layer, so that the model captures very long-term temporal information. Moreover, they introduced an attention based hierarchical sequence-to-sequence model that connect the most relavant parts between encoder and decoder, for example, the words around a final punctuation. They experiment with documents over 100 words, the results show that hierarchical and attention-based hierarchical LSTMs learns even better long-term temporal information than standard LSTMs-based encoder-decoder models.\\

As autoencoders achieve great successes in text data and speech processing, they are also used for time series anomaly detection. These models train autoencoders with only normal data while take anomalies as unknown patterns. Then the autoencoders are only able to reconstruct normal patterns, then large reconstruction error indicates anomaly. An early work \cite{eps} used vanilla autoencoder to detect abnormal status of the electric power system. In order to capture temporal information, they applied sliding window on the raw data as input. As anomaly scoring method, they evaluated each sliding window with respect to their reconstruction error. As some measures in the autoencoder output vectors that are more sensible to anomalies than others, they use the average absolute deviation of reconstruction error as anomaly score. And the anomaly threshold is chosen by large amount of experiments over normal data.\\

Another important reason of using autoencoder for anomaly detection is its ability of dealing with high-dimensional data. Sakurada et al. \cite{ dimensionalityreduction} experimented with time series data that consist of 10-100 variables without linear correlation. Comparing with reconstruction using PCA or Kernel PCA techniques, the autoencoder reconstruction error can easily recognize anomalies.\\

In further researches, Malhotra et al. \cite{lstmad}\cite{encdecad} developed the application of LSTMs-autoencoder from sequence learning into anomaly detection problem. They proposed a stacked LSTMs model to learn high level temporal patterns. They show that LSTMs outperform normal RNNs-based anomaly detection model and avoid the gradient vanishing problem thanks to the gate architecture inside the LSTM unit. They also detect anomaly based on the reconstruction error. The scoring function is based on the parameters of an estimated normal distribution of a validation set. Their experiments show that the model performs good in variety kinds of datasets. A variation of this model \cite{timenet} has been proved that achieves better performance in the anomaly detection tasks, while they tell that using a constant as input of decoder instead of read time series value improves the performance of model.

\section{Online anomaly detection over data stream}
\label{sec: Online anomaly detection over data stream }


\subsection{Classical online approaches}
\label{sec:classical online approaches}

Recently, streaming data mining attracts more and more attention, and many efficient classical models are modified to fit the online learning property. Cui et al. \cite{cui} proposed online anomaly detection approach with grid-based summarization and clustering algorithm, which is able to detect anomaly immediatly when data arrives. For the passed streaming data, they used summarization algorithm to reduce the run time and memory consumption over stream. And they give anomaly scores to instances to describe concrete anomaly degree. The model shows good performance in KDD dataset for network attack detection. However, they consider only isolated data point without any temporal information measuring. Another streaming data anomaly detection framework by Tang et al. \cite{onlinelof} used sliding window to involve the contextual dependency and temporal changes in the stream. The anomaly detection algorithm is a modified version of LOF while LOF’s high time complexity can not be directly employed to streaming data. They use comentropy to filter out most normal windows, and feed the rest to LOF.

\subsection{Autoencoder-based online approaches}
\label{sec:Autoencoder-based online approaches }

Zhou et al. \cite{online} proposed an online updating approach for denoising autoencoders by modifying the hidden layer neurons in order to deal with the non-stationary streaming data properties. The basic idea consists two steps, adding hidden layer neurons to capture new knowledge, and merging hidden layer neurons if information is redundant. Their experimental result shows comparable or better reconstruction result than non-incremental approaches with only few data used during initialization. And they show that their incremental feature learning methods performs more adaptively and robustly to highly non-stationary input data distribution.\\

Dong et al. \cite{threaded} proposed a 2-step anomaly detection mechanism with incremental autoencoders. They implemented the model with ensembled autoencoders in multithreads in order to leverage parallel computing when large volumes of data arrive. In the 2-step mechanism, they check anomaly in the first step and verify anomaly data with previous and subsequent data (to distinguish between anomalous state and concept drift) in the second step to reduce false-positive rate in anomaly detection. In the experimental results, they show that their model outperforms commonly used tree-based streaming anomaly detection models especially when concept drift presents. And they speed up the online processing with mini-batch learning and online learning in multithreads.\\

Ghazikhani et al. \cite{onlinenn} introduced an online neural network model for streaming data towards to the two major problems of online learning, concept drift and imbalanced classes. In term of concept drift, they applied a forgetting function that weights recent instances to navigate the model to the drifted model, so that the model always learns pattern from latest data. Besides, for class imbalance, they proposed an error function for two-class imbalance problem with the basic idea that the error function generating higher error signals for instances in the minority class.\\

Kochurov el at. \cite{bayesian} designed incremental learning framework for deep neural networks based on Bayesian inference. They argued that, naïve deep learning approaches for incremental learning applies Stochastic Gradient Descent (SGD), which intent to keep previous learned model remembered, and enhanced with current batch of new data. However, by SGD, the neural network model is likely to converge to the local optimal of the latest batch of data without preserving the previous knowledge. Their Bayesian framework estimates the posterior distribution over the weights of the model in the condition of previous knowledge and use the Bayesian rule to sequentially update the posterior distribution in the incremental learning.\\

In this work, we implement a LSTMs-autoencoder based incremental streaming data anomaly detection model. The LSTMs-autoencoder is close to the model introduced by Malhotra et al. \cite{encdecad}, and we design an online model updating strategy as well as the updating data buffers used for model updating, which collect fresh knowledge from the last seen instances. 








