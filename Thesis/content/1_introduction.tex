\chapter{Introduction}
\label{chap:Introduction}

Anomaly detection attracts more and more attention in the data mining domain, and is widely used in the different applications, e.g. manufacture, e-commerce, internet etc. Successful anomaly detection in time avoids inconvenient and reduces maintenance expenditure in many scenarios like credit card fraud detecting, spam email recognition, machine condition monitoring, and could also be used as a preprocessing step to remove anomalies from datasets before machine learning tasks. There are already plenty of anomaly detection techniques proposed in previous literatures that solve this problem from variety perspectives, e.g. distance-based methods, clustering analysis, density-based methods etc.\\

There is no lack of anomaly detection approaches that perform good with respect to different kinds of data. Supervised approaches take anomaly detection as a binary classification problem of “normal” and “abnormal” instances, and all instance labels should be available in advance. The key difference to other classification problem is here the class labes are extremely biased to the normal class while anomaly only appears rarely. Instead of doing data augmentation on anomaly data for supervised approaches, unsupervised approaches are more direct solutions to this problem, which treat the instances that fit least to the majority as the anomalies. Furthermore, under many real-world situations, only partial labels are available, and therefore semi-supervised as well as one-class models are more efficient. 
Those models learn pattern from the partially labeled normal data, and 
scale anomaly likelihood according to the difference between an unseen pattern and the learned normal pattern.\\

However, most of the existing anomaly detection models are designed as batch models, which means, the entire training set should be available in advance. This becomes a shortcoming under today’s big data background. With the rapid development of hardware in the last decade, the situation of data acquisition and analysis has significantly been changed. Specifically, in the IoT applications, data are acquired from sensors attached to IoT devices, and arrive continuously and everlasting. In the beginning, no static entire training set is available for model initialization in the batch fashion. Besides, during data analysis, we should always consider the volume and velocity of data, which means, on one hand, with traditional batch classifiers, the infinity data stream will lead to out of memory problem, on the other hand, streaming data usually comes with a high velocity that leaving the system few processing time. Optimally, the model should have only single look at each data point in the stream. In addition, the statistical property of data may also change over time, which is formally called ‘concept drift’. The model should always learn latest knowledge from the stream and update its identification of anomaly automatically, while anomalies could be temporally. After a data distribution change, an anomaly could possibly become normal in the new streaming context. Data distribution changes should not be classified as anomaly, in the meanwhile, though anomalies show up rarely over the stream, they should not be oversighted. To this end, an anomaly detection system for streaming data should be able to 1) be initialized with only a small subset, 2) process streaming data and make prediction in real-time, 3) adapt data evolution over the stream. 4) model should be able to deal with the biased data.\\

LSTMs are a kind of recurrent neural network that exhibit dynamic temporal behaviour for time series. As a neural network-based model, LSTMs are able to deal with high-dimensional and non-linear data directly. In the last decade, LSTM are used widely in time series prediction and text prediction. LSTMs-based autoencoders are also applied to sequence to sequence problems, e.g. language translation, time series data embedding. Deep LSTMs have been shown good performance in capturing hierarchical information of time series like separation of sentences. Recently, LSTMs-based autoencoders are also used for time series anomaly detection in order to capture the temporal information between data points. For example, Malhotra et al. introduced an autoencoder based anomaly detection approaches in [1],[2], and achieved good performance in multiple time series dataset. However, in those researches, they still assume that the whole datasets are available beforehand and work on static data. Also, the aforementioned online learning difficulties are not taken into consideration. Hence, we enhanced this kind of LSTMs-Autoencoder based static anomaly detection approaches with the online learning ability by appending incremental model updating strategies.\\

Neural networks, including autoencoders, are normally used in batch fashion, namely the whole training set is available, and trained with backpropagation. Under online setting, only small subset accumulated data from stream are available for model initial training, which may lead to getting a suboptimal model. The precondition of the online LSTMs-autoencoder is that the accumulated initialization set is enough to train a convergent model. The difficulty is to detect when model should be updated and what data should be used for model updating. The short-term changes of data distribution should not cause model variation, while permanent concept drifts should trigger model updating as soon as possible.\\

In this paper, we introduce a novel and robust incremental LSTMs-Autoencoder anomaly detection model, which designed specifically for time series data in a streaming fashion using Long Short-Term memory (LSTM) units, with also online learning ability for model updating. For each accumulated mini-batch of streaming data, the autoencoder reconstructs it with previous knowledge learned from normal data. Anomalies (never used for training) are supposed to cause significant larger reconstruction error than normal data. In addition, the model is able to update itself when data distribution changes are detected. The LSTMs-Autoencoder is experimented with different data streams, and the experiment results show its ability in detecting anomaly from streaming data and is able to adjust itself with different kinds of concept drifts by model online updating.\\


The rest of this paper is organized as follow. In \autoref{chap:relatedworks}, we collected previous works on anomaly detection and their shortcomings. We also refer previous works on incremental neural network. In \autoref{chap:Preliminaries}, define the problem formally. In \autoref{chap:Proposedmodel}, we propose our method for streaming data anomaly detection and discuss the advantage over previous works. In \autoref{chap:Experimentalsetup}, we describe the datasets used for experiments and the experimental set-up. In \autoref{chap:results}, we present our experimental results. And in \autoref{chap:conclusion}, we summarize the work and discuss of success and deficiency.

