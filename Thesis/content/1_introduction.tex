\chapter{Introduction}
\label{chap:Introduction}

Anomaly detection is a core component of data mining, and widely used in the manufacturing industry, e-commerce, internet applications etc. It could avoid inconvenient and reduce lose in many scenarios like machine health monitoring, credit card fraud detecting and spam email recognition, and could also be used as a preprocessing step to remove anomalies for datasets in many machine learning tasks. There are already plenty of anomaly detection techniques proposed in previous literatures, that solve this problem from variety perspectives, e.g. distance-based methods, clustering analysis, density-based methods etc.\\

There is no lack of anomaly detection approaches that perform good with respect to different kinds of data. Supervised approaches take anomaly detection as a binary classification problem of “normal” instances and “abnormal” instances, and all instance labels should be available in advance. The key difference to other classification problem is the amount of class label is extremely biased to the normal class. In order to avoid doing data augmentation or down sampling, unsupervised approaches are more direct solutions to this problem, which find out the instances that fit least to the majority as the anomalies. Furthermore, in most situations, partial labels are available, and semi-supervised and one-class models are more efficient. They learn the pattern from labeled normal data, test data that not fit the learned pattern perfectly is likely to be the anomalies. Different kinds of anomaly detection approaches fit different use cases and data character. However, majority of them are batch model, which means, all data should be available in advance. This becomes a shortcoming under today’s big data background. With the rapid development of hardware in the last decade, the situation of data acquisition and analysis has significantly been changed. Specifically, the IoT application. Assume that we collect data from sensors attached to IoT devices, the data comes continuously and everlasting. In the beginning, no static full set of data is available for model initialization in the traditional way. Besides, during data analysis, we should always consider the volume and velocity of data, which means, on one hand, with traditional batch classifiers, the infinity data stream will lead to out of memory, on the other hand, streaming data usually comes in a high speed that leaving the system few processing time, the model should work with only single look at each data point in the stream. In addition, the statistical property of data may also change over time, which is formally called ‘concept drift’. The model should always learn new knowledge from the stream and update its identification of anomaly automatically, while anomalies could be temporally. After a data distribution change, an anomaly possibly becomes normal in the new data environment. Data distribution changes should not be classified as anomaly, and anomaly show up rarely in over the stream, they should also not be oversighted. To this end, an anomaly detection system for streaming data should be able to 1) be initialized with only a small subset, 2) process streaming data and make prediction in real-time, 3) adapt data evolution over time. 4) model should be able to deal with the biased class problem.\\


LSTMs are a kind of recurrent neural network and proposed for temporal dependently data. As a neural network based model, LSTMs can deal with high-dimentional and non-linear data, with arbitary expansion of the model structure. In the last decade, LSTM are used widely in time series prediction, text prediction. And LSTMs-based autoencoder is a good choice for sequence to sequence problem, e.g. language translation, time series data embedding. Deep LSTMs have also shown good performance of capturing hierarchical information from time series like seperation of sentances. Recently, LSTMs-based autoencoders are also used for time series anomaly detection in order to capture the temporal information between data points. For example, Malhotra et al. introduced an autoencoder based anomaly detection approaches in [1],[2], and achieved good performance in multiple time series dataset. This kind of anomaly deteciton approaches overcomes reconstruction error based models using traditional vanille autoencoders (VAE). However, in those approaches, they still assume that the whole datasets are available beforehand and work on static data. Also, they didn’t consider the aforementioned online learning difficulties. Hence, we enhanced this kind of LSTMs-Autoencoder based static anomaly detection approaches with the online learning ability by implementing incremental model updating strategies with streaming data.\\

Neural networks, including autoencoders, are normally used in batch fashion, namely the whole training set is available, and trained by backpropagation. When come to online setting, only small subset accumulated data from stream are available for model initial training, which may be suboptimal. Assume that the initialization set is enough to train a convergent model, the further streaming data are used for further model updating to adjust latest streaming data changes and the patterns never seen ever. Unlike batch models, instead of aiming at best overall performance, online neural networks are learned to achieve best sequential performance for current streaming data. The difficulty is to detect when model should be updated according to latest data and updating with which part of data. The short-term changes of data distribution should not cause model variation, while permanent concept drifts should trigger model updating as soon as possible.\\

In this paper, we introduce a novel and robust incremental LSTMs-Autoencoder anomaly detection model, which designed specifically for time series data in a streaming fashion using Long Short-Term memory (LSTM) units, with also online learning ability for model updating. For each accumulated mini-batch of streaming data, the autoencoder reconstructs it with previous knowledge learned from normal data. Anomaly data (never used for training) is expected to cause significant larger reconstruction error than normal data. In addition, the model is able to update itself when detected data distribution changes.\\

Todo: Summary of experiment results…\\

The rest of this paper is organized as follow. In \autoref{chap:relatedworks}, we collected previous works on anomaly detection and their shortcomings. We also refer some works on incremental neural network. In \autoref{chap:Preliminaries}, define the problem formally. In \autoref{chap:Proposedmodel}, we propose our method for streaming data anomaly detection and discuss the advantage over previous works. In \autoref{chap:Experimentalsetup}, we describe the datasets used for experiments and the experimental set-up. In \autoref{chap:results}, we present our experimental results. And in \autoref{chap:conclusion}, we summarize the work and discuss of success and deficiency.
