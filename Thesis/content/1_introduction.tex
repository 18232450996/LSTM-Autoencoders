\chapter{Introduction}
\label{chap:Introduction}

Anomaly detection is an core component of data mining, and widely used in the manufacturing industry, e-commerce, internet applications etc. It could avoid or reduce lose in many scenarios like machine health monitoring, credit card fraud detecting and spam email recognition, and could also be used as a preprocessing step to remove anomalies for datasets in many machine learning tasks. There are already plenty of anomaly detection techniques proposed in previous literatures, that solve this problem from variety perspectives, e.g. distance-based methods, clustering analysis, density-based methods etc.\\

There is no lack of anomaly detection approaches that perform good with respect to different kinds of data, however, majority of them are batch model, which means, all data should be available in advance. This becomes a shortcoming under today’s big data background. With the rapid development of hardware in the last decade, the situation of data acquisition and analysis has significantly been changed. Specifically, the IoT application. Assume that we collect data from sensors attached to IoT devices, the data comes continuously and everlasting. In the beginning, no static full set of data is available for model initialization. Besides, during data analysis, we should always consider the volume and velocity of data, which means, on one hand, with traditional batch classifiers, the infinity data stream will lead to out of memory, on the other hand, streaming data usually comes in a high speed that leaving the system few processing time, the model should work with only single look at each data point in the stream. In addition, the statistical property of data may also change over time, which is formally called ‘concept drift’. The model should always learn new knowledge from the stream and update its identification of anomaly automatically, while anomalies could be temporally. After a data distribution change, an anomaly possibly becomes normal in the new data environment. To this end, an anomaly detection system for streaming data should be able to 1) be initialized with only a small subset, 2) process streaming data and make prediction in real-time, 3) adapt data evolution over time.\\

Nowadays, LSTMs-based autoencoders are used for time series anomaly detection in order to capture the temporal information between data points. For example, Malhotra et al. introduced an autoencoder based anomaly detection approaches in [1],[2], and achieved good performance in multiple time series dataset. However, in those approaches, they still assume that the whole datasets are available beforehand and work on static data. Also, they didn’t consider the aforementioned online learning difficulties. Hence, we enhanced this kind of LSTMs-Autoencoder based static anomaly detection approaches with the online learning ability by implementing incremental model updating strategies with streaming data.\\

In this paper, we introduce a novel and robust incremental LSTMs-Autoencoder anomaly detection model, which designed specifically for time series data in a streaming fashion using Long Short-Term memory (LSTM) units, with also online learning ability for model updating. For each accumulated mini-batch of streaming data, the autoencoder reconstructs it with previous knowledge learned from normal data. Anomaly data (never used for training) is expected to cause significant larger reconstruction error than normal data. In addition, the model is able to update itself when detected data distribution changes.\\

Todo: Summary of experiment results…\\

The rest of this paper is organized as follow. In \autoref{chap:relatedworks}, we collected previous works on anomaly detection and their shortcomings. We also refer some works on incremental neural network. In \autoref{chap:Preliminaries}, define the problem formally. In \autoref{chap:Proposedmodel}, we propose our method for streaming data anomaly detection and discuss the advantage over previous works. In \autoref{chap:Experimentalsetup}, we describe the datasets used for experiments and the experimental set-up. In \autoref{chap:results}, we present our experimental results. And in \autoref{chap:conclusion}, we summarize the work and discuss of success and deficiency.
