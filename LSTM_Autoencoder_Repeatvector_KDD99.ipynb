{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model,Sequential\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of LSTM_Autoencoder(RepeatVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Autoencoder_RepeatVector:\n",
    "\n",
    "    def __init__(self, dataset, timesteps, latent_dim, n_epoch,n_batch):\n",
    "        self.dataset = dataset\n",
    "        self.timesteps = timesteps\n",
    "        self.latent_dim = latent_dim \n",
    "        self.n_batch = n_batch\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "    def reshape(self,data):\n",
    "        sample_num = math.floor(data.shape[0]/self.timesteps)\n",
    "        new_dataset = np.reshape(data[:sample_num*self.timesteps],(sample_num,self.timesteps,data.shape[1]))\n",
    "        return new_dataset\n",
    "\n",
    "    def lstm_autoencoder_repeatvector(self,optimizer='adadelta',loss='mse'):\n",
    "        input_dim = self.dataset.shape[1]\n",
    "        inputs = Input(shape=(self.timesteps,input_dim))\n",
    "        encoded = LSTM(self.latent_dim)(inputs)\n",
    "        decoded = RepeatVector(self.timesteps)(encoded)\n",
    "        decoded = LSTM(input_dim,return_sequences=True)(decoded)\n",
    "        autoencoder = Model(inputs,decoded)\n",
    "        encoder = Model(inputs,encoded)\n",
    "        autoencoder.compile(optimizer=optimizer,loss=loss)\n",
    "\n",
    "        reversed_dataset = np.empty([0,self.dataset.shape[-1]])\n",
    "        for i in range(int(self.dataset.shape[0]/self.timesteps)):\n",
    "            temp = self.dataset[i*self.timesteps:i*self.timesteps+self.timesteps,:]\n",
    "            temp = temp[::-1,:]\n",
    "            reversed_dataset = np.concatenate((reversed_dataset,temp))\n",
    "        new_dataset = self.reshape(self.dataset)\n",
    "        reversed_dataset = self.reshape(reversed_dataset)\n",
    "        print(\"Trianing LSTM-Autoencoder...\")\n",
    "        dt1 = datetime.now()\n",
    "        history = autoencoder.fit(new_dataset,reversed_dataset,\n",
    "                    epochs=self.n_epoch,\n",
    "                    batch_size=self.n_batch,\n",
    "                    validation_split=0.33\n",
    "                    )\n",
    "        dt2 = datetime.now()\n",
    "        print(\"time used of trianing LSTM-Autoencoder: \",(dt2-dt1),\"s\")\n",
    "        Print(\"Encoding dataset\")\n",
    "        dt3 = datetime.now()\n",
    "        encoded_dataset = encoder.predict(new_dataset)\n",
    "        dt4 = datetime.now()\n",
    "        print(\"time used of encoding dataset \",(dt2-dt1),\"s\")\n",
    "        # plot the performance of training\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load (10%)KDD99 dataset (only numeric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Bin/Documents/Datasets/KDD99/columns.txt\") as col_file:\n",
    "    line = col_file.readline()\n",
    "\n",
    "columns = line.split('.')\n",
    "col_names = []\n",
    "col_types = []\n",
    "for col in columns:\n",
    "    col_names.append(col.split(': ')[0].strip())\n",
    "    col_types.append(col.split(': ')[1])\n",
    "col_names.append(\"label\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/KDD99/kddcup.data_10_percent_corrected\",names=col_names)\n",
    "\n",
    "data = df.iloc[:,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()\n",
    "label = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling dataset to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "dataset = scaler.transform(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11032 samples, validate on 5435 samples\n",
      "Epoch 1/100\n",
      "11032/11032 [==============================] - 7s 675us/step - loss: 0.1055 - val_loss: 0.0750\n",
      "Epoch 2/100\n",
      "11032/11032 [==============================] - 4s 406us/step - loss: 0.0342 - val_loss: 0.0505\n",
      "Epoch 3/100\n",
      "11032/11032 [==============================] - 4s 407us/step - loss: 0.0194 - val_loss: 0.0391\n",
      "Epoch 4/100\n",
      "11032/11032 [==============================] - 5s 415us/step - loss: 0.0140 - val_loss: 0.0338\n",
      "Epoch 5/100\n",
      "11032/11032 [==============================] - 5s 414us/step - loss: 0.0112 - val_loss: 0.0305\n",
      "Epoch 6/100\n",
      "11032/11032 [==============================] - 5s 421us/step - loss: 0.0094 - val_loss: 0.0280\n",
      "Epoch 7/100\n",
      "11032/11032 [==============================] - 5s 425us/step - loss: 0.0082 - val_loss: 0.0261\n",
      "Epoch 8/100\n",
      "11032/11032 [==============================] - 5s 433us/step - loss: 0.0074 - val_loss: 0.0246\n",
      "Epoch 9/100\n",
      "11032/11032 [==============================] - 5s 456us/step - loss: 0.0069 - val_loss: 0.0234\n",
      "Epoch 10/100\n",
      "11032/11032 [==============================] - 5s 483us/step - loss: 0.0065 - val_loss: 0.0223\n",
      "Epoch 11/100\n",
      "11032/11032 [==============================] - 5s 459us/step - loss: 0.0062 - val_loss: 0.0213\n",
      "Epoch 12/100\n",
      "11032/11032 [==============================] - 5s 451us/step - loss: 0.0059 - val_loss: 0.0206\n",
      "Epoch 13/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0057 - val_loss: 0.0196\n",
      "Epoch 14/100\n",
      "11032/11032 [==============================] - 5s 447us/step - loss: 0.0055 - val_loss: 0.0186\n",
      "Epoch 15/100\n",
      "11032/11032 [==============================] - 5s 448us/step - loss: 0.0054 - val_loss: 0.0176\n",
      "Epoch 16/100\n",
      "11032/11032 [==============================] - 5s 444us/step - loss: 0.0052 - val_loss: 0.0166\n",
      "Epoch 17/100\n",
      "11032/11032 [==============================] - 5s 439us/step - loss: 0.0051 - val_loss: 0.0155\n",
      "Epoch 18/100\n",
      "11032/11032 [==============================] - 5s 444us/step - loss: 0.0049 - val_loss: 0.0147\n",
      "Epoch 19/100\n",
      "11032/11032 [==============================] - 5s 453us/step - loss: 0.0048 - val_loss: 0.0141\n",
      "Epoch 20/100\n",
      "11032/11032 [==============================] - 5s 444us/step - loss: 0.0047 - val_loss: 0.0137\n",
      "Epoch 21/100\n",
      "11032/11032 [==============================] - 5s 435us/step - loss: 0.0046 - val_loss: 0.0133\n",
      "Epoch 22/100\n",
      "11032/11032 [==============================] - 5s 445us/step - loss: 0.0045 - val_loss: 0.0130\n",
      "Epoch 23/100\n",
      "11032/11032 [==============================] - 5s 446us/step - loss: 0.0044 - val_loss: 0.0127\n",
      "Epoch 24/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0044 - val_loss: 0.0124\n",
      "Epoch 25/100\n",
      "11032/11032 [==============================] - 5s 448us/step - loss: 0.0043 - val_loss: 0.0121\n",
      "Epoch 26/100\n",
      "11032/11032 [==============================] - 5s 445us/step - loss: 0.0042 - val_loss: 0.0118\n",
      "Epoch 27/100\n",
      "11032/11032 [==============================] - 5s 447us/step - loss: 0.0041 - val_loss: 0.0116\n",
      "Epoch 28/100\n",
      "11032/11032 [==============================] - 5s 441us/step - loss: 0.0041 - val_loss: 0.0113\n",
      "Epoch 29/100\n",
      "11032/11032 [==============================] - 5s 444us/step - loss: 0.0040 - val_loss: 0.0111\n",
      "Epoch 30/100\n",
      "11032/11032 [==============================] - 5s 444us/step - loss: 0.0040 - val_loss: 0.0108\n",
      "Epoch 31/100\n",
      "11032/11032 [==============================] - 5s 436us/step - loss: 0.0039 - val_loss: 0.0106\n",
      "Epoch 32/100\n",
      "11032/11032 [==============================] - 5s 441us/step - loss: 0.0039 - val_loss: 0.0103\n",
      "Epoch 33/100\n",
      "11032/11032 [==============================] - 5s 440us/step - loss: 0.0038 - val_loss: 0.0101\n",
      "Epoch 34/100\n",
      "11032/11032 [==============================] - 5s 443us/step - loss: 0.0038 - val_loss: 0.0099\n",
      "Epoch 35/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0037 - val_loss: 0.0097\n",
      "Epoch 36/100\n",
      "11032/11032 [==============================] - 5s 452us/step - loss: 0.0037 - val_loss: 0.0095\n",
      "Epoch 37/100\n",
      "11032/11032 [==============================] - 5s 442us/step - loss: 0.0036 - val_loss: 0.0093\n",
      "Epoch 38/100\n",
      "11032/11032 [==============================] - 5s 442us/step - loss: 0.0036 - val_loss: 0.0092\n",
      "Epoch 39/100\n",
      "11032/11032 [==============================] - 5s 442us/step - loss: 0.0036 - val_loss: 0.0091\n",
      "Epoch 40/100\n",
      "11032/11032 [==============================] - 5s 455us/step - loss: 0.0036 - val_loss: 0.0090\n",
      "Epoch 41/100\n",
      "11032/11032 [==============================] - 5s 454us/step - loss: 0.0035 - val_loss: 0.0089\n",
      "Epoch 42/100\n",
      "11032/11032 [==============================] - 5s 457us/step - loss: 0.0035 - val_loss: 0.0088\n",
      "Epoch 43/100\n",
      "11032/11032 [==============================] - 5s 445us/step - loss: 0.0035 - val_loss: 0.0086\n",
      "Epoch 44/100\n",
      "11032/11032 [==============================] - 5s 464us/step - loss: 0.0035 - val_loss: 0.0086\n",
      "Epoch 45/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0034 - val_loss: 0.0085\n",
      "Epoch 46/100\n",
      "11032/11032 [==============================] - 5s 466us/step - loss: 0.0034 - val_loss: 0.0085\n",
      "Epoch 47/100\n",
      "11032/11032 [==============================] - 5s 466us/step - loss: 0.0034 - val_loss: 0.0083\n",
      "Epoch 48/100\n",
      "11032/11032 [==============================] - 5s 462us/step - loss: 0.0034 - val_loss: 0.0083\n",
      "Epoch 49/100\n",
      "11032/11032 [==============================] - 5s 457us/step - loss: 0.0033 - val_loss: 0.0082\n",
      "Epoch 50/100\n",
      "11032/11032 [==============================] - 5s 455us/step - loss: 0.0033 - val_loss: 0.0082\n",
      "Epoch 51/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0033 - val_loss: 0.0081\n",
      "Epoch 52/100\n",
      "11032/11032 [==============================] - 5s 462us/step - loss: 0.0033 - val_loss: 0.0080\n",
      "Epoch 53/100\n",
      "11032/11032 [==============================] - 5s 486us/step - loss: 0.0033 - val_loss: 0.0080\n",
      "Epoch 54/100\n",
      "11032/11032 [==============================] - 6s 540us/step - loss: 0.0033 - val_loss: 0.0079\n",
      "Epoch 55/100\n",
      "11032/11032 [==============================] - 5s 467us/step - loss: 0.0032 - val_loss: 0.0079\n",
      "Epoch 56/100\n",
      "11032/11032 [==============================] - 5s 453us/step - loss: 0.0032 - val_loss: 0.0079\n",
      "Epoch 57/100\n",
      "11032/11032 [==============================] - 5s 447us/step - loss: 0.0032 - val_loss: 0.0078\n",
      "Epoch 58/100\n",
      "11032/11032 [==============================] - 5s 447us/step - loss: 0.0032 - val_loss: 0.0078\n",
      "Epoch 59/100\n",
      "11032/11032 [==============================] - 5s 451us/step - loss: 0.0032 - val_loss: 0.0077\n",
      "Epoch 60/100\n",
      "11032/11032 [==============================] - 5s 457us/step - loss: 0.0032 - val_loss: 0.0077\n",
      "Epoch 61/100\n",
      "11032/11032 [==============================] - 5s 445us/step - loss: 0.0032 - val_loss: 0.0077\n",
      "Epoch 62/100\n",
      "11032/11032 [==============================] - 5s 453us/step - loss: 0.0031 - val_loss: 0.0077\n",
      "Epoch 63/100\n",
      "11032/11032 [==============================] - 5s 451us/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 64/100\n",
      "11032/11032 [==============================] - 5s 453us/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 65/100\n",
      "11032/11032 [==============================] - 5s 454us/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 66/100\n",
      "11032/11032 [==============================] - 5s 456us/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 67/100\n",
      "11032/11032 [==============================] - 5s 448us/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 68/100\n",
      "11032/11032 [==============================] - 5s 457us/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 69/100\n",
      "11032/11032 [==============================] - 5s 463us/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 70/100\n",
      "11032/11032 [==============================] - 5s 449us/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 71/100\n",
      "11032/11032 [==============================] - 5s 462us/step - loss: 0.0030 - val_loss: 0.0075\n",
      "Epoch 72/100\n",
      "11032/11032 [==============================] - 5s 457us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 73/100\n",
      "11032/11032 [==============================] - 5s 451us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 74/100\n",
      "11032/11032 [==============================] - 5s 451us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 75/100\n",
      "11032/11032 [==============================] - 5s 456us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11032/11032 [==============================] - 5s 419us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 77/100\n",
      "11032/11032 [==============================] - 5s 430us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 78/100\n",
      "11032/11032 [==============================] - 5s 421us/step - loss: 0.0030 - val_loss: 0.0074\n",
      "Epoch 79/100\n",
      "11032/11032 [==============================] - 5s 418us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 80/100\n",
      "11032/11032 [==============================] - 5s 423us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 81/100\n",
      "11032/11032 [==============================] - 5s 425us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 82/100\n",
      "11032/11032 [==============================] - 5s 427us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 83/100\n",
      "11032/11032 [==============================] - 5s 424us/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 84/100\n",
      "11032/11032 [==============================] - 5s 422us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 85/100\n",
      "11032/11032 [==============================] - 5s 424us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 86/100\n",
      "11032/11032 [==============================] - 5s 434us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 87/100\n",
      "11032/11032 [==============================] - 5s 427us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 88/100\n",
      "11032/11032 [==============================] - 5s 422us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 89/100\n",
      "11032/11032 [==============================] - 5s 424us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 90/100\n",
      "11032/11032 [==============================] - 5s 428us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 91/100\n",
      "11032/11032 [==============================] - 5s 424us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 92/100\n",
      "11032/11032 [==============================] - 5s 427us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 93/100\n",
      "11032/11032 [==============================] - 5s 437us/step - loss: 0.0029 - val_loss: 0.0073\n",
      "Epoch 94/100\n",
      "11032/11032 [==============================] - 5s 426us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 95/100\n",
      "11032/11032 [==============================] - 5s 445us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 96/100\n",
      "11032/11032 [==============================] - 5s 436us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 97/100\n",
      "11032/11032 [==============================] - 5s 438us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 98/100\n",
      "11032/11032 [==============================] - 5s 441us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 99/100\n",
      "11032/11032 [==============================] - 5s 436us/step - loss: 0.0029 - val_loss: 0.0072\n",
      "Epoch 100/100\n",
      "11032/11032 [==============================] - 5s 439us/step - loss: 0.0029 - val_loss: 0.0072\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20dc96b3940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timesteps = 30\n",
    "latent_dim = 10\n",
    "n_epoch = 100\n",
    "n_batch = 100\n",
    "\n",
    "model = LSTM_Autoencoder_RepeatVector(dataset,timesteps,latent_dim,n_epoch,n_batch)\n",
    "encoded_dataset = model.lstm_autoencoder_repeatvector()\n",
    "# generate labels for encoded_dataset\n",
    "e_label = [None]*encoded_dataset.shape[0]\n",
    "for i in range(encoded_dataset.shape[0]):\n",
    "    e_label[i] = label[(i+1)*timesteps-1] # for each batch, use the last sample label as the encoded batch label\n",
    "e_label = np.array(e_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Todo\n",
    "#inverse scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Two SVM classifier for original dataset and encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare both original and encoded data\n",
    "\n",
    "def train_test_split(dataset,label):\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    y_train, y_test = label[0:train_size],label[train_size:len(dataset)]\n",
    "    # label str2int\n",
    "    with open(\"C:/Users/Bin/Documents/Datasets/KDD99/classes.txt\") as f:\n",
    "        line = f.readline()\n",
    "    classes = line.split(\",\")\n",
    "    class_dic = {classes[i]:i for i in range(len(classes))}\n",
    "    y_train_num = [class_dic[x.strip('.')] for x in y_train]\n",
    "    y_test_num = [class_dic[x.strip('.')] for x in y_test]\n",
    "    return train,test,y_train_num,y_test_num\n",
    "xn_train, xn_test, yn_train,yn_test =  train_test_split(dataset,label)\n",
    "xe_train, xe_test, ye_train,ye_test =  train_test_split(encoded_dataset,e_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing SVM classifier with original dataset...\n",
      "time used of trianing on original dataset:  0:01:05.823061 s\n",
      "Trianing SVM classifier with encoded dataset...\n",
      "time used of training on encoded dataset:  0:00:00.118958 s\n"
     ]
    }
   ],
   "source": [
    "# train two svm classifiers\n",
    "from sklearn import svm\n",
    "\n",
    "print(\"Trianing SVM classifier with original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "clf_n = svm.SVC()\n",
    "clf_n.fit(xn_train,yn_train)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of trianing on original dataset: \",(dt2-dt1),\"s\")\n",
    "print(\"Trianing SVM classifier with encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "clf_e = svm.SVC()\n",
    "clf_e.fit(xe_train,ye_train)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of training on encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction using original dataset...\n",
      "time used of prediction using original dataset:  0:01:34.830185 s\n",
      "Making prediction using encoded dataset...\n",
      "time used of prediction using encoded dataset:  0:00:00.084652 s\n"
     ]
    }
   ],
   "source": [
    "# making prediction using both classifier and both dataset\n",
    "\n",
    "#original\n",
    "print(\"Making prediction using original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "prediction_n = clf_n.predict(xn_test)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of prediction using original dataset: \",(dt2-dt1),\"s\")\n",
    "#encoded\n",
    "print(\"Making prediction using encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "prediction_e = clf_e.predict(xe_test)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of prediction using encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on original dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       100\n",
      "          1       0.00      0.00      0.00        18\n",
      "          3       0.00      0.00      0.00         0\n",
      "          5       0.55      0.14      0.23       385\n",
      "          6       0.50      0.25      0.33         4\n",
      "          7       0.00      0.00      0.00         1\n",
      "          9       1.00      0.69      0.82     66079\n",
      "         10       0.00      0.00      0.00         0\n",
      "         11       0.98      0.97      0.98     26053\n",
      "         12       0.00      0.00      0.00         1\n",
      "         13       0.00      0.00      0.00         1\n",
      "         14       0.00      0.00      0.00       162\n",
      "         15       0.02      0.87      0.03       401\n",
      "         16       0.00      0.00      0.00         3\n",
      "         17       0.00      0.00      0.00         2\n",
      "         18       1.00      1.00      1.00     69235\n",
      "         20       1.00      0.99      1.00       582\n",
      "         21       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.99      0.87      0.92    163027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "# performance using original dataset\n",
    "print(\"Performance on original dataset\\n\")\n",
    "print(classification_report(yn_test, prediction_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on encoded dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         3\n",
      "          5       0.00      0.00      0.00        17\n",
      "          9       1.00      0.69      0.82      2204\n",
      "         11       0.94      1.00      0.97       866\n",
      "         14       0.00      0.00      0.00         4\n",
      "         15       1.00      0.54      0.70        13\n",
      "         17       0.00      0.00      0.00         0\n",
      "         18       1.00      1.00      1.00      2308\n",
      "         20       0.00      0.00      0.00        20\n",
      "\n",
      "avg / total       0.98      0.86      0.91      5435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Performance using encoded dataset\n",
    "print(\"Performance on encoded dataset\\n\")\n",
    "print(classification_report(ye_test, prediction_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
