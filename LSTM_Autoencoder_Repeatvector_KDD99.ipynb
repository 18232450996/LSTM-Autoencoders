{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model,Sequential\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of LSTM_Autoencoder(RepeatVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Autoencoder_RepeatVector:\n",
    "\n",
    "    def __init__(self, dataset, timesteps, latent_dim, n_epoch,n_batch):\n",
    "        self.dataset = dataset\n",
    "        self.timesteps = timesteps\n",
    "        self.latent_dim = latent_dim \n",
    "        self.n_batch = n_batch\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "    def reshape(self,data):\n",
    "        sample_num = math.floor(data.shape[0]/self.timesteps)\n",
    "        new_dataset = np.reshape(data[:sample_num*self.timesteps],(sample_num,self.timesteps,data.shape[1]))\n",
    "        return new_dataset\n",
    "\n",
    "    def lstm_autoencoder_repeatvector(self,optimizer='adadelta',loss='mse'):\n",
    "        input_dim = self.dataset.shape[1]\n",
    "        inputs = Input(shape=(self.timesteps,input_dim))\n",
    "        encoded = LSTM(self.latent_dim)(inputs)\n",
    "        decoded = RepeatVector(self.timesteps)(encoded)\n",
    "        decoded = LSTM(input_dim,return_sequences=True)(decoded)\n",
    "        autoencoder = Model(inputs,decoded)\n",
    "        encoder = Model(inputs,encoded)\n",
    "        autoencoder.compile(optimizer=optimizer,loss=loss)\n",
    "\n",
    "        reversed_dataset = np.empty([0,self.dataset.shape[-1]])\n",
    "        for i in range(int(self.dataset.shape[0]/self.timesteps)):\n",
    "            temp = self.dataset[i*self.timesteps:i*self.timesteps+self.timesteps,:]\n",
    "            temp = temp[::-1,:]\n",
    "            reversed_dataset = np.concatenate((reversed_dataset,temp))\n",
    "        new_dataset = self.reshape(self.dataset)\n",
    "        reversed_dataset = self.reshape(reversed_dataset)\n",
    "        print(\"Trianing LSTM-Autoencoder...\")\n",
    "        dt1 = datetime.now()\n",
    "        history = autoencoder.fit(new_dataset,reversed_dataset,\n",
    "                    epochs=self.n_epoch,\n",
    "                    batch_size=self.n_batch,\n",
    "                    validation_split=0.33\n",
    "                    )\n",
    "        dt2 = datetime.now()\n",
    "        print(\"time used of trianing LSTM-Autoencoder: \",(dt2-dt1),\"s\")\n",
    "        Print(\"Encoding dataset\")\n",
    "        dt3 = datetime.now()\n",
    "        encoded_dataset = encoder.predict(new_dataset)\n",
    "        dt4 = datetime.now()\n",
    "        print(\"time used of encoding dataset \",(dt2-dt1),\"s\")\n",
    "        # plot the performance of training\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load (10%)KDD99 dataset (only numeric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Bin/Documents/Datasets/KDD99/columns.txt\") as col_file:\n",
    "    line = col_file.readline()\n",
    "\n",
    "columns = line.split('.')\n",
    "col_names = []\n",
    "col_types = []\n",
    "for col in columns:\n",
    "    col_names.append(col.split(': ')[0].strip())\n",
    "    col_types.append(col.split(': ')[1])\n",
    "col_names.append(\"label\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/KDD99/kddcup.data_10_percent_corrected\",names=col_names)\n",
    "\n",
    "data = df.iloc[:,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()\n",
    "label = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling dataset to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "dataset = scaler.transform(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-6620effdfb30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_Autoencoder_RepeatVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mencoded_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_autoencoder_repeatvector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# generate labels for encoded_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0me_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mencoded_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-8f875aeb79c1>\u001b[0m in \u001b[0;36mlstm_autoencoder_repeatvector\u001b[1;34m(self, optimizer, loss)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mreversed_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mnew_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mreversed_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timesteps = 10\n",
    "latent_dim = 10\n",
    "n_epoch = 100\n",
    "n_batch = 100\n",
    "\n",
    "model = LSTM_Autoencoder_RepeatVector(dataset,timesteps,latent_dim,n_epoch,n_batch)\n",
    "encoded_dataset = model.lstm_autoencoder_repeatvector()\n",
    "# generate labels for encoded_dataset\n",
    "e_label = [None]*encoded_dataset.shape[0]\n",
    "for i in range(encoded_dataset.shape[0]):\n",
    "    e_label[i] = label[(i+1)*timesteps-1] # for each batch, use the last sample label as the encoded batch label\n",
    "e_label = np.array(e_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Todo\n",
    "#inverse scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Two SVM classifier for original dataset and encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare both original and encoded data\n",
    "\n",
    "def train_test_split(dataset,label):\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    y_train, y_test = label[0:train_size],label[train_size:len(dataset)]\n",
    "    # label str2int\n",
    "    with open(\"C:/Users/Bin/Documents/Datasets/KDD99/classes.txt\") as f:\n",
    "        line = f.readline()\n",
    "    classes = line.split(\",\")\n",
    "    class_dic = {classes[i]:i for i in range(len(classes))}\n",
    "    y_train_num = [class_dic[x.strip('.')] for x in y_train]\n",
    "    y_test_num = [class_dic[x.strip('.')] for x in y_test]\n",
    "    return train,test,y_train_num,y_test_num\n",
    "xn_train, xn_test, yn_train,yn_test =  train_test_split(dataset,label)\n",
    "xe_train, xe_test, ye_train,ye_test =  train_test_split(encoded_dataset,e_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing SVM classifier with original dataset...\n",
      "time used of trianing on original dataset:  0:01:05.823061 s\n",
      "Trianing SVM classifier with encoded dataset...\n",
      "time used of training on encoded dataset:  0:00:00.118958 s\n"
     ]
    }
   ],
   "source": [
    "# train two svm classifiers\n",
    "from sklearn import svm\n",
    "\n",
    "print(\"Trianing SVM classifier with original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "clf_n = svm.SVC()\n",
    "clf_n.fit(xn_train,yn_train)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of trianing on original dataset: \",(dt2-dt1),\"s\")\n",
    "print(\"Trianing SVM classifier with encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "clf_e = svm.SVC()\n",
    "clf_e.fit(xe_train,ye_train)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of training on encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction using original dataset...\n",
      "time used of prediction using original dataset:  0:01:34.830185 s\n",
      "Making prediction using encoded dataset...\n",
      "time used of prediction using encoded dataset:  0:00:00.084652 s\n"
     ]
    }
   ],
   "source": [
    "# making prediction using both classifier and both dataset\n",
    "\n",
    "#original\n",
    "print(\"Making prediction using original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "prediction_n = clf_n.predict(xn_test)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of prediction using original dataset: \",(dt2-dt1),\"s\")\n",
    "#encoded\n",
    "print(\"Making prediction using encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "prediction_e = clf_e.predict(xe_test)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of prediction using encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on original dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.00000   0.00000   0.00000       100\n",
      "          1    0.00000   0.00000   0.00000        18\n",
      "          3    0.00000   0.00000   0.00000         0\n",
      "          5    0.55000   0.14286   0.22680       385\n",
      "          6    0.50000   0.25000   0.33333         4\n",
      "          7    0.00000   0.00000   0.00000         1\n",
      "          9    0.99982   0.69034   0.81675     66079\n",
      "         10    0.00000   0.00000   0.00000         0\n",
      "         11    0.98389   0.97064   0.97722     26053\n",
      "         12    0.00000   0.00000   0.00000         1\n",
      "         13    0.00000   0.00000   0.00000         1\n",
      "         14    0.00000   0.00000   0.00000       162\n",
      "         15    0.01682   0.87282   0.03301       401\n",
      "         16    0.00000   0.00000   0.00000         3\n",
      "         17    0.00000   0.00000   0.00000         2\n",
      "         18    0.99952   0.99908   0.99930     69235\n",
      "         20    1.00000   0.99141   0.99569       582\n",
      "         21    0.00000   0.00000   0.00000         0\n",
      "\n",
      "avg / total    0.99189   0.86525   0.91578    163027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "# performance using original dataset\n",
    "print(\"Performance on original dataset\\n\")\n",
    "print(classification_report(yn_test, prediction_n,digits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on encoded dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.00000   0.00000   0.00000         3\n",
      "          5    0.00000   0.00000   0.00000        17\n",
      "          9    0.99868   0.68875   0.81525      2204\n",
      "         11    0.93900   0.99538   0.96637       866\n",
      "         14    0.00000   0.00000   0.00000         4\n",
      "         15    1.00000   0.53846   0.70000        13\n",
      "         17    0.00000   0.00000   0.00000         0\n",
      "         18    0.99913   0.99783   0.99848      2308\n",
      "         20    0.00000   0.00000   0.00000        20\n",
      "\n",
      "avg / total    0.98128   0.86293   0.91026      5435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Performance using encoded dataset\n",
    "print(\"Performance on encoded dataset\\n\")\n",
    "print(classification_report(ye_test, prediction_e,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16467, 10)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
