{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/Bin/Desktop/Thesis/code')\n",
    "from Conf_EncDecAD_KDD99 import Conf_EncDecAD_KDD99\n",
    "#from EncDecAD import EncDecAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "\n",
    "data_root = \"C:/Users/Bin/Documents/Datasets/KDD99/6_subsets_win/\"\n",
    "conf = Conf_EncDecAD_KDD99(data_root)\n",
    "#[sn_list, va_list, vn1_list, vn2_list, tn_list, ta_list] = conf.data_list\n",
    "\n",
    "#p_input = conf.p_input\n",
    "#p_inputs = conf.p_inputs\n",
    "\n",
    "\n",
    "\n",
    "batch_num = conf.batch_num\n",
    "hidden_num = conf.hidden_num\n",
    "step_num = conf.step_num\n",
    "elem_num = conf.elem_num\n",
    "\n",
    "iteration = conf.iteration\n",
    "modelpath_root = conf.modelpath_root\n",
    "modelpath = conf.modelpath\n",
    "decode_without_input = conf.decode_without_input\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: 7.51197\n",
      "iter 2: 7.24106\n",
      "iter 3: 5.83815\n",
      "iter 4: 4.83861\n",
      "iter 5: 3.96047\n",
      "iter 6: 3.4242\n",
      "iter 7: 2.83394\n",
      "iter 8: 2.23055\n",
      "iter 9: 1.86834\n",
      "iter 10: 1.58492\n",
      "iter 11: 1.39966\n",
      "iter 12: 1.18028\n",
      "iter 13: 1.0044\n",
      "iter 14: 0.943103\n",
      "iter 15: 0.888694\n",
      "iter 16: 0.81497\n",
      "iter 17: 0.840342\n",
      "iter 18: 0.674519\n",
      "iter 19: 0.556531\n",
      "iter 20: 0.557433\n",
      "iter 21: 0.508053\n",
      "iter 22: 0.439355\n",
      "iter 23: 0.431179\n",
      "iter 24: 0.459009\n",
      "iter 25: 0.363388\n",
      "iter 26: 0.387082\n",
      "iter 27: 0.311459\n",
      "iter 28: 0.302973\n",
      "iter 29: 0.321011\n",
      "iter 30: 0.254924\n",
      "iter 31: 0.268844\n",
      "iter 32: 0.245902\n",
      "iter 33: 0.267388\n",
      "iter 34: 0.271701\n",
      "iter 35: 0.248599\n",
      "iter 36: 0.198732\n",
      "iter 37: 0.220623\n",
      "iter 38: 0.175422\n",
      "iter 39: 0.167471\n",
      "iter 40: 0.178838\n",
      "iter 41: 0.180768\n",
      "iter 42: 0.169217\n",
      "iter 43: 0.185415\n",
      "iter 44: 0.159892\n",
      "iter 45: 0.1608\n",
      "iter 46: 0.137266\n",
      "iter 47: 0.127748\n",
      "iter 48: 0.123396\n",
      "iter 49: 0.120896\n",
      "iter 50: 0.12278\n",
      "iter 51: 0.111605\n",
      "iter 52: 0.118172\n",
      "iter 53: 0.107634\n",
      "iter 54: 0.124384\n",
      "iter 55: 0.112882\n",
      "iter 56: 0.102028\n",
      "iter 57: 0.130443\n",
      "iter 58: 0.147567\n",
      "iter 59: 0.106401\n",
      "iter 60: 0.0969681\n",
      "iter 61: 0.10944\n",
      "iter 62: 0.093924\n",
      "iter 63: 0.107248\n",
      "iter 64: 0.0968959\n",
      "iter 65: 0.0901023\n",
      "iter 66: 0.101177\n",
      "iter 67: 0.0865559\n",
      "iter 68: 0.0810143\n",
      "iter 69: 0.0842896\n",
      "iter 70: 0.0751477\n",
      "iter 71: 0.0778647\n",
      "iter 72: 0.0942162\n",
      "iter 73: 0.0777883\n",
      "iter 74: 0.0967629\n",
      "iter 75: 0.0735276\n",
      "iter 76: 0.0725091\n",
      "iter 77: 0.0817166\n",
      "iter 78: 0.0919413\n",
      "iter 79: 0.0683102\n",
      "iter 80: 0.079911\n",
      "iter 81: 0.0796894\n",
      "iter 82: 0.0805593\n",
      "iter 83: 0.0766167\n",
      "iter 84: 0.0664007\n",
      "iter 85: 0.0760535\n",
      "iter 86: 0.0714242\n",
      "iter 87: 0.0729446\n",
      "iter 88: 0.0766296\n",
      "iter 89: 0.068347\n",
      "iter 90: 0.0738274\n",
      "iter 91: 0.0630006\n",
      "iter 92: 0.0713475\n",
      "iter 93: 0.0654229\n",
      "iter 94: 0.0610879\n",
      "iter 95: 0.062039\n",
      "iter 96: 0.0724365\n",
      "iter 97: 0.0717779\n",
      "iter 98: 0.0611382\n",
      "iter 99: 0.0677564\n",
      "iter 100: 0.0615055\n",
      "Model saved in file: C:/Users/Bin/Desktop/Thesis/tmp/52test/LSTMAutoencoder_kdd99_v1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'    \\n    err_vec_list = []   \\n    for _ in range(len(conf.vn1_list)//batch_num):\\n        data =[]\\n        for temp in range(batch_num):\\n            ind = np.random.randint(0,len(conf.vn1_list)-1)\\n            sub = conf.vn1_list[ind]\\n            data.append(sub)\\n        data = np.array(data)\\n        (_input_, _output_) = sess.run([input_, output_], {p_input: data})\\n        err_vec_list.append(abs(_input_ - _output_))\\n    \\n    err_vec = np.mean(np.array(err_vec_list),axis=0).reshape(batch_num,-1)\\n    mu = np.mean(err_vec,axis=0)\\n    sigma = np.cov(err_vec.T)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19e90475eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    \n",
    "    p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num),name = \"p_input\")\n",
    "    p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]\n",
    "    \n",
    "    _enc_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "    _dec_cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)\n",
    "    #inputs = conf.p_inputs\n",
    "    inputs = p_inputs #...\n",
    "    \n",
    "    reverse = True\n",
    "    decode_without_input = False\n",
    "    is_training = True\n",
    "    with tf.variable_scope('encoder',reuse = tf.AUTO_REUSE):\n",
    "        (z_codes, enc_state) = tf.contrib.rnn.static_rnn(_enc_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('decoder',reuse =tf.AUTO_REUSE) as vs:\n",
    "\n",
    "        dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,elem_num], dtype=tf.float32),name=\"dec_weight_\")\n",
    "\n",
    "        dec_bias_ = tf.Variable(tf.constant(0.1,shape=[elem_num],dtype=tf.float32),name=\"dec_bias_\")\n",
    "\n",
    "\n",
    "        dec_state = enc_state\n",
    "        dec_input_ = tf.zeros(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "        dec_outputs = []\n",
    "\n",
    "        for step in range(len(inputs)):\n",
    "            if step > 0:\n",
    "                vs.reuse_variables()\n",
    "            (dec_input_, dec_state) =_dec_cell(dec_input_, dec_state)\n",
    "            dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\n",
    "            dec_outputs.append(dec_input_)\n",
    "    \n",
    "        if reverse:\n",
    "            dec_outputs = dec_outputs[::-1]\n",
    "\n",
    "        output_ = tf.transpose(tf.stack(dec_outputs), [1, 0, 2],name=\"output_\")\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    input_= tf.transpose(tf.stack(inputs), [1, 0, 2],name=\"input_\")\n",
    "#    output_ = tf.transpose(output_, [0,1,2])\n",
    "    loss_ = tf.reduce_mean(tf.square(input_ - output_),name=\"loss_\")\n",
    "\n",
    "   \n",
    "    train_ = tf.train.AdamOptimizer().minimize(loss_)\n",
    "    #train_ = tf.train.GradientDescentOptimizer(0.01).minimize(loss_)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    loss = []\n",
    "    for i in range(iteration):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.sn_list)-1)\n",
    "            sub = conf.sn_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "\n",
    "        (loss_val, _) = sess.run([loss_, train_], {p_input: data})\n",
    "        loss.append(loss_val)\n",
    "        print('iter %d:' % (i + 1), loss_val)\n",
    "    pd.Series(loss).plot(title=\"Loss\")\n",
    "    \n",
    "\n",
    "\n",
    "    save_path = saver.save(sess, modelpath)\n",
    "    print(\"Model saved in file: %s\" % save_path) \n",
    "\n",
    "\n",
    "\"\"\"    \n",
    "    err_vec_list = []   \n",
    "    for _ in range(len(conf.vn1_list)//batch_num):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.vn1_list)-1)\n",
    "            sub = conf.vn1_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "        (_input_, _output_) = sess.run([input_, output_], {p_input: data})\n",
    "        err_vec_list.append(abs(_input_ - _output_))\n",
    "    \n",
    "    err_vec = np.mean(np.array(err_vec_list),axis=0).reshape(batch_num,-1)\n",
    "    mu = np.mean(err_vec,axis=0)\n",
    "    sigma = np.cov(err_vec.T)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae = EncDecAD(hidden_num, p_inputs, is_training = True, decode_without_input=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss = []\n",
    "    for i in range(iteration):\n",
    "        data =[]\n",
    "        for temp in range(batch_num):\n",
    "            ind = np.random.randint(0,len(conf.sn_list)-1)\n",
    "            sub = conf.sn_list[ind]\n",
    "            data.append(sub)\n",
    "        data = np.array(data)\n",
    "        \n",
    "        (loss_val, _) = sess.run([ae.loss, ae.train], {p_input: data})\n",
    "        loss.append(loss_val)\n",
    "        print('iter %d:' % (i + 1), loss_val)\n",
    "    pd.Series(loss).plot(title=\"Loss\")\n",
    "\n",
    "    save_path = saver.save(sess, modelpath)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate parameters using Vn1 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Bin/Desktop/Thesis/tmp/52test/LSTMAutoencoder_kdd99_v1.ckpt\n",
      "Model restored.\n",
      "Initialized\n",
      "Got parameters mu and sigma.\n",
      "[ 0.19463608  0.11765845  0.13154957  0.18010619  0.11181988  0.05767112\n",
      "  0.12715456  0.06118212  0.24614303  0.07082351  0.1498466   0.21913306\n",
      "  0.09148553  0.08496854  0.2224772   0.08920401  0.10600094  0.27662054\n",
      "  0.22896647  0.06222694  0.07404834  0.21031396  0.10301597  0.28576559\n",
      "  0.48060146  0.40352878  0.51187122  0.1618385   0.1194734   0.16127291\n",
      "  0.1798491   0.07099917  0.05087043  0.07606138  0.18938069  0.11362384\n",
      "  0.13247427  0.15461209  0.1103431   0.04980225  0.10199855  0.05496701\n",
      "  0.2119652   0.07680407  0.14027238  0.20153554  0.07812548  0.06630364\n",
      "  0.18447158  0.0939533   0.08540108  0.27285659  0.2393146   0.0620846\n",
      "  0.08067043  0.18169461  0.09672682  0.27894354  0.47332352  0.40689787\n",
      "  0.50511062  0.13631585  0.11195876  0.12283494  0.15848689  0.05407965\n",
      "  0.03996972  0.06563514  0.17618403  0.10851087  0.13291372  0.13212857\n",
      "  0.10718755  0.05151384  0.07770088  0.0456512   0.18330278  0.07496937\n",
      "  0.13649665  0.18653403  0.05867921  0.05848246  0.13417606  0.10492732\n",
      "  0.06455077  0.25809476  0.24717483  0.06960902  0.08695837  0.16007796\n",
      "  0.10568551  0.25133908  0.46553031  0.41897464  0.49295568  0.12869254\n",
      "  0.09669749  0.1004598   0.13298871  0.0543044   0.04639949  0.06217164\n",
      "  0.15387031  0.09832387  0.12540567  0.11519881  0.100347    0.05871571\n",
      "  0.05737986  0.03419928  0.16881764  0.06471868  0.13396579  0.17688647\n",
      "  0.04490869  0.0548084   0.08363204  0.12784415  0.05106814  0.2352207\n",
      "  0.25682217  0.08305256  0.08939883  0.14923996  0.10262543  0.25867841\n",
      "  0.45493668  0.42678079  0.50604177  0.12943015  0.09864896  0.09180273\n",
      "  0.11064608  0.06238582  0.06318103  0.05735952  0.12673852  0.08313037\n",
      "  0.10709274  0.1067784   0.09076367  0.06267523  0.04664593  0.02538713\n",
      "  0.16806859  0.04619082  0.12314469  0.17316724  0.06086789  0.04917258\n",
      "  0.04799309  0.16392799  0.05017668  0.20589499  0.26817593  0.09145264\n",
      "  0.09148349  0.14991815  0.09855694  0.27100867  0.457555    0.42838961\n",
      "  0.52443355  0.13461478  0.10235003  0.09153955  0.10032044  0.07165083\n",
      "  0.07832263  0.05739319  0.10427877  0.06669378  0.08308335  0.10707094\n",
      "  0.08142921  0.06860364  0.04723138  0.0392906   0.18352763  0.04673677\n",
      "  0.10550491  0.17648818  0.09159888  0.04421131  0.04739747  0.19609118\n",
      "  0.05367582  0.18219665  0.27929288  0.09199657  0.09805997  0.15856114\n",
      "  0.0826561   0.27825263  0.45549893  0.42126912  0.54895782  0.13790874\n",
      "  0.14538641  0.09933208  0.10018207  0.07443275  0.08923078  0.06090126\n",
      "  0.07728805  0.05279064  0.06676713  0.10884526  0.07188264  0.08025529\n",
      "  0.0557889   0.05972461  0.19405952  0.05251314  0.08704081  0.18499152\n",
      "  0.11956333  0.03861686  0.05763208  0.21349053  0.05285741  0.16796155\n",
      "  0.29005945  0.0809246   0.10037734  0.18167396  0.08651574  0.30150133\n",
      "  0.4653151   0.41603667  0.57203424  0.13817392  0.16837487  0.11126713\n",
      "  0.11569645  0.07241676  0.0875008   0.06725226  0.05539326  0.04266488\n",
      "  0.06081674  0.11181523  0.06116981  0.09595098  0.06767091  0.0702938\n",
      "  0.19863251  0.05388261  0.07408737  0.19941357  0.13990165  0.03757933\n",
      "  0.07171706  0.21878763  0.05273791  0.16559881  0.30132705  0.07108997\n",
      "  0.09856991  0.20675424  0.07997854  0.29560858  0.46582016  0.40181193\n",
      "  0.59109271  0.13944878  0.18769267  0.12087605  0.1440042   0.07517035\n",
      "  0.08433022  0.07919452  0.04192574  0.03524147  0.05822037  0.11799318\n",
      "  0.0511983   0.11079899  0.07641074  0.06545289  0.19643146  0.0563179\n",
      "  0.06676395  0.2083993   0.15114991  0.0505049   0.08477315  0.21903415\n",
      "  0.05815332  0.17733087  0.3011505   0.05793377  0.10599784  0.22541609\n",
      "  0.09363995  0.3057287   0.46613464  0.37449235  0.58770245  0.13629277\n",
      "  0.2040128   0.1256897   0.17516279  0.06900883  0.07178738  0.08837499\n",
      "  0.04381912  0.03234324  0.05237687  0.11662577  0.0515493   0.11557021\n",
      "  0.08334283  0.05484226  0.18900606  0.07229908  0.06325896  0.20543198\n",
      "  0.15104988  0.07356497  0.09123334  0.2129703   0.05406274  0.19646683\n",
      "  0.30639726  0.04833429  0.1325855   0.22779374  0.09742545  0.28884608\n",
      "  0.46805778  0.35378295  0.56733423  0.1224222   0.20197077  0.11702964\n",
      "  0.20357013  0.06146177  0.07000842  0.09411221  0.0532709   0.03710579\n",
      "  0.0478393   0.09743094  0.0672771   0.1104105   0.09617029  0.03817689\n",
      "  0.17148873  0.09877199  0.05984111  0.19597499  0.14538927  0.11313665\n",
      "  0.08553831  0.2075206   0.05314922  0.21460135  0.31166893  0.05445529\n",
      "  0.16406031  0.2212528   0.08500642  0.2834745   0.46563679  0.36753002\n",
      "  0.54674798  0.1127607   0.19388428  0.10297029  0.23278907  0.06200171\n",
      "  0.07700594  0.09165151  0.05689443  0.04308288  0.05109941  0.07059337\n",
      "  0.08877752  0.10605776  0.11306749  0.03128939  0.14059608  0.12081567\n",
      "  0.05800606  0.1960929   0.13638741  0.15706353  0.07261361  0.20577212\n",
      "  0.05846693  0.21745047  0.29637033  0.06447709  0.17306894  0.21991841\n",
      "  0.07122689  0.27118668  0.46423513  0.42667991  0.5271849   0.12245838\n",
      "  0.16024536  0.08430053  0.26856613  0.07269109  0.08235995  0.07109635\n",
      "  0.04695851  0.04301492  0.07832511  0.065536    0.09919205  0.10297147\n",
      "  0.11834773  0.05490468  0.11183839  0.13403958  0.06881513  0.20103614\n",
      "  0.12627573  0.18782964  0.06782286  0.21619737  0.06633435  0.22537673\n",
      "  0.26509377  0.06489804  0.15444806  0.22641353  0.07626743  0.23990062\n",
      "  0.45620185  0.48275742  0.5026778   0.14626019  0.12979794  0.08020459\n",
      "  0.29818836  0.10223218  0.07766254  0.04918551  0.04806975  0.05909782\n",
      "  0.1077631   0.08630029  0.09763326  0.09520215  0.10580079  0.10062402\n",
      "  0.11565147  0.15508984  0.08868489  0.18875672  0.1078904   0.18646899\n",
      "  0.07773864  0.22989371  0.07585607  0.25101289  0.2454197   0.06073732\n",
      "  0.13864197  0.24051289  0.09701838  0.23473191  0.43981585  0.49761361\n",
      "  0.48302346  0.16733858  0.12027172  0.08146747  0.30286247  0.13248631\n",
      "  0.06568113  0.03509752  0.06537725  0.08340208  0.1316704   0.12034478\n",
      "  0.08857329  0.10334835  0.09072932  0.11962058  0.1660482   0.19826823\n",
      "  0.09819502  0.16390559  0.07583387  0.16658168  0.08893155  0.24040392\n",
      "  0.09345067  0.28955632  0.2461741   0.07773302  0.13054827  0.27264819\n",
      "  0.12057139  0.21449539  0.43823606  0.50387633  0.50510293  0.19404213\n",
      "  0.14481999  0.07413805  0.29121676  0.16021976  0.06100387  0.05327141\n",
      "  0.0759037   0.10094589  0.13645105  0.14143905  0.04326679  0.07553396\n",
      "  0.08951749  0.11672765  0.24988253  0.25110096  0.10618538  0.16788861\n",
      "  0.04631831  0.17315635  0.06830918  0.23297794  0.11989164  0.32217717\n",
      "  0.27519101  0.07711857  0.1041356   0.33660525  0.1419072   0.23521736\n",
      "  0.45108971  0.5265438   0.55193377  0.26224402  0.17262629  0.07770105\n",
      "  0.28705221  0.22657128  0.08472537  0.09095982  0.07504937  0.13991272\n",
      "  0.12234579  0.15316576  0.0530148   0.03013225  0.09601762  0.14270887\n",
      "  0.35034913  0.28519803  0.12123217  0.21868034  0.08952712  0.21665876\n",
      "  0.08070821  0.20389536  0.10190842  0.34730253  0.30585858  0.05670816\n",
      "  0.0689628   0.39568585  0.15260729  0.27558178  0.44918609  0.52412927\n",
      "  0.60594463  0.38208216  0.21272011  0.07781984  0.28872272  0.27659249\n",
      "  0.13471082  0.15820791  0.06303398  0.22828591  0.09753025  0.14175825\n",
      "  0.12851958  0.08439558  0.083505    0.18390347  0.46134552  0.29979938\n",
      "  0.14619842  0.26850697  0.14804174  0.2393347   0.13282457  0.15472089\n",
      "  0.07890779  0.33812171  0.35248998  0.05597211  0.08216324  0.36299178\n",
      "  0.13994762  0.30884379  0.4353601   0.40421095  0.62021863  0.48779622\n",
      "  0.30284056  0.05826962  0.26739338  0.24564001  0.17330049  0.25693652\n",
      "  0.0603236   0.35809699  0.09429247  0.1045969   0.10601525  0.18569605\n",
      "  0.0702475   0.13633417  0.51463741  0.33089671  0.22556229  0.28679055\n",
      "  0.18292603  0.20803995  0.21362254  0.06539007  0.04603272  0.30045462\n",
      "  0.44691664  0.0952317   0.08581497  0.24194875  0.12831695  0.268246\n",
      "  0.38962665  0.23687431  0.67245334  0.52225673  0.45861071  0.06378215\n",
      "  0.22772405  0.15804128  0.1700522   0.22522703  0.15481359  0.31050235\n",
      "  0.08177862  0.08036298  0.13429585  0.15435605  0.1667577   0.30191153\n",
      "  0.52493924  0.21732     0.46403342  0.3096517   0.29303679  0.17535332\n",
      "  0.37992346  0.17625347  0.12630342  0.37204388  0.39299411  0.38887382\n",
      "  0.38831681  0.20157775  0.15547618  0.31897396  0.30012301  0.31144354\n",
      "  0.92199308  0.43599224  0.75880224  0.15235227  0.1293864   0.3302362\n",
      "  0.24755843  0.1806958 ]\n"
     ]
    }
   ],
   "source": [
    "from Parameter_helper import Parameter_Helper\n",
    "\n",
    "para = Parameter_Helper(conf)\n",
    "\n",
    "mu, sigma = para.mu_and_sigma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate anomaly score, get threshold t using Vn2 and Va dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = para.get_threshold(mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, modelpath)  # decode_without_input=True, iter=5000\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    \n",
    "    normal_score = []\n",
    "    n_in = []\n",
    "    n_out = []\n",
    "    a_in = []\n",
    "    a_out = []\n",
    "    \n",
    "    for count in range(len(tn_list)//batch_num):\n",
    "        normal_sub = np.array(tn_list[count*batch_num:(count+1)*batch_num]) \n",
    "        (input_n, output_n) = sess.run([ae.input_, ae.output_], {p_input: normal_sub})\n",
    "        n_in.append(input_n)\n",
    "        n_out.append(output_n)\n",
    "        err_n = abs(input_n-output_n).reshape(-1,step_num)\n",
    "        err_n = err_n.reshape(batch_num,-1)\n",
    "        for batch in range(batch_num):\n",
    "           temp = np.dot( (err_n[batch] - mu ).reshape(1,-1)  , sigma.T)\n",
    "           s = np.dot(temp,(err_n[batch] - mu ))\n",
    "           normal_score.append(s[0])\n",
    "           \n",
    "    abnormal_score = []\n",
    "    for count in range(len(ta_list)//batch_num):\n",
    "        abnormal_sub = np.array(ta_list[count*batch_num:(count+1)*batch_num]) \n",
    "        (input_a, output_a) = sess.run([ae.input_, ae.output_], {p_input: abnormal_sub})\n",
    "        a_in.append(input_a)\n",
    "        a_out.append(output_a)\n",
    "        err_a = abs(input_a-output_a).reshape(-1,step_num)\n",
    "        err_a = err_a.reshape(batch_num,-1)\n",
    "        for batch in range(batch_num):\n",
    "           temp = np.dot( (err_a[batch] - mu ).reshape(1,-1)  , sigma.T)\n",
    "           s = np.dot(temp,(err_a[batch] - mu ))\n",
    "           abnormal_score.append(s[0])\n",
    "             \n",
    "\n",
    "    print('Predict result :')\n",
    "\n",
    "    pd.Series(normal_score).plot(label=\"normal_score\",figsize=(18,5))\n",
    "    pd.Series(abnormal_score).plot(label=\"abnormal_score\")\n",
    "    bar = threshold*np.ones(len(normal_score)+len(abnormal_score))\n",
    "    pd.Series(bar).plot(label=\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "tp = np.array(abnormal_score)[np.array(abnormal_score)>threshold].size\n",
    "fp = len(abnormal_score)-tp\n",
    "fn = np.array(normal_score)[np.array(normal_score)>threshold].size\n",
    "tn = len(normal_score)- fn\n",
    "P = tp/(tp+fp)\n",
    "R = tp/(tp+fn)\n",
    "fbeta= (1+beta*beta)*P*R/(beta*beta*P+R)\n",
    "fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tp,fp,tn,fn,P,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp/fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
