{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model,Sequential\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of LSTM_Autoencoder(RepeatVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Autoencoder_RepeatVector:\n",
    "\n",
    "    def __init__(self, dataset, timesteps, latent_dim, n_epoch,n_batch):\n",
    "        self.dataset = dataset\n",
    "        self.timesteps = timesteps\n",
    "        self.latent_dim = latent_dim \n",
    "        self.n_batch = n_batch\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "    def reshape(self,data):\n",
    "        sample_num = math.floor(data.shape[0]/self.timesteps)\n",
    "        new_dataset = np.reshape(data[:sample_num*self.timesteps],(sample_num,self.timesteps,data.shape[1]))\n",
    "        return new_dataset\n",
    "\n",
    "    def lstm_autoencoder_repeatvector(self,optimizer='adadelta',loss='mse'):\n",
    "        input_dim = self.dataset.shape[1]\n",
    "        inputs = Input(shape=(self.timesteps,input_dim))\n",
    "        encoded = LSTM(self.latent_dim)(inputs)\n",
    "        decoded = RepeatVector(self.timesteps)(encoded)\n",
    "        decoded = LSTM(input_dim,return_sequences=True)(decoded)\n",
    "        autoencoder = Model(inputs,decoded)\n",
    "        encoder = Model(inputs,encoded)\n",
    "        autoencoder.compile(optimizer=optimizer,loss=loss)\n",
    "\n",
    "        reversed_dataset = np.empty([0,self.dataset.shape[-1]])\n",
    "        for i in range(int(self.dataset.shape[0]/self.timesteps)):\n",
    "            temp = self.dataset[i*self.timesteps:i*self.timesteps+self.timesteps,:]\n",
    "            temp = temp[::-1,:]\n",
    "            reversed_dataset = np.concatenate((reversed_dataset,temp))\n",
    "        new_dataset = self.reshape(self.dataset)\n",
    "        reversed_dataset = self.reshape(reversed_dataset)\n",
    "        print(\"Trianing LSTM-Autoencoder...\")\n",
    "        dt1 = datetime.now()\n",
    "        history = autoencoder.fit(new_dataset,reversed_dataset,\n",
    "                    epochs=self.n_epoch,\n",
    "                    batch_size=self.n_batch,\n",
    "                    validation_split=0.33\n",
    "                    )\n",
    "        dt2 = datetime.now()\n",
    "        print(\"time used of trianing LSTM-Autoencoder: \",(dt2-dt1),\"s\")\n",
    "        print(\"Encoding dataset\")\n",
    "        dt3 = datetime.now()\n",
    "        encoded_dataset = encoder.predict(new_dataset)\n",
    "        dt4 = datetime.now()\n",
    "        print(\"time used of encoding dataset \",(dt2-dt1),\"s\")\n",
    "\n",
    "        \n",
    "        # plot the performance of training\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.plot(history.history[\"val_loss\"])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        # get decoder output\n",
    "        decoder_output = autoencoder.predict(new_dataset)\n",
    "        \n",
    "        return encoded_dataset, decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load (10%)KDD99 dataset (only numeric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Bin/Documents/Datasets/KDD99/columns.txt\") as col_file:\n",
    "    line = col_file.readline()\n",
    "\n",
    "columns = line.split('.')\n",
    "col_names = []\n",
    "col_types = []\n",
    "for col in columns:\n",
    "    col_names.append(col.split(': ')[0].strip())\n",
    "    col_types.append(col.split(': ')[1])\n",
    "col_names.append(\"label\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/KDD99/kddcup.data_10_percent_corrected\",names=col_names)\n",
    "\n",
    "data = df.iloc[:,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()\n",
    "label = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling dataset to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "dataset = scaler.transform(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updata: Load power demand dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bin\\Desktop\\Thesis\\code\\Loaddata.py:56: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  sub_power = sub_power.reshape(-1, 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "%run Loaddata\n",
    "from Loaddata import Loaddata\n",
    "\n",
    "ld = Loaddata(\"power_demand\")\n",
    "datasets = ld.read()\n",
    "[training_normal, validation_1, validation_2, test_normal, validation_anomaly, test_anomaly,whole_normal,whole_anormaly] = datasets\n",
    "dataset = np.reshape(whole_normal,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing LSTM-Autoencoder...\n",
      "Train on 30 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 4s 120ms/step - loss: 0.1461 - val_loss: 0.1413\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.1057 - val_loss: 0.1017\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0711 - val_loss: 0.0661\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0459 - val_loss: 0.0478\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0384 - val_loss: 0.0449\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0378 - val_loss: 0.0446\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0377 - val_loss: 0.0443\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0376 - val_loss: 0.0445\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0375 - val_loss: 0.0441\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0374 - val_loss: 0.0444\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0374 - val_loss: 0.0439\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0373 - val_loss: 0.0439\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0372 - val_loss: 0.0437\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0371 - val_loss: 0.0438\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0371 - val_loss: 0.0439\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0370 - val_loss: 0.0434\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0369 - val_loss: 0.0435\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0368 - val_loss: 0.0435\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0367 - val_loss: 0.0435\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0365 - val_loss: 0.0434\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0365 - val_loss: 0.0431\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0363 - val_loss: 0.0435\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0363 - val_loss: 0.0428\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0360 - val_loss: 0.0426\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0360 - val_loss: 0.0424\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0358 - val_loss: 0.0429\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0356 - val_loss: 0.0428\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0353 - val_loss: 0.0417\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0350 - val_loss: 0.0414\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0346 - val_loss: 0.0409\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0347 - val_loss: 0.0404\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0337 - val_loss: 0.0426\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0346 - val_loss: 0.0397\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.0345 - val_loss: 0.0426\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.0347 - val_loss: 0.0451\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0339 - val_loss: 0.0444\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0348 - val_loss: 0.0392\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0342 - val_loss: 0.0386\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0327 - val_loss: 0.0386\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0340 - val_loss: 0.0389\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0342 - val_loss: 0.0401\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0344 - val_loss: 0.0391\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0324 - val_loss: 0.0380\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0338 - val_loss: 0.0380\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0319 - val_loss: 0.0379\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0320 - val_loss: 0.0389\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0344 - val_loss: 0.0382\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0323 - val_loss: 0.0378\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0322 - val_loss: 0.0384\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0329 - val_loss: 0.0378\n",
      "time used of trianing LSTM-Autoencoder:  0:00:58.009793 s\n",
      "Encoding dataset\n",
      "time used of encoding dataset  0:00:58.009793 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npLAQEZEh/T96eLaJMe725gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2305579ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# power demand training\n",
    "timesteps = 84\n",
    "latent_dim = 40\n",
    "n_epoch = 50\n",
    "n_batch = 3\n",
    "model = LSTM_Autoencoder_RepeatVector(dataset,timesteps,latent_dim,n_epoch,n_batch)\n",
    "encoded_dataset,output = model.lstm_autoencoder_repeatvector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDD99 training\n",
    "timesteps = 10\n",
    "latent_dim = 10\n",
    "n_epoch = 100\n",
    "n_batch = 100\n",
    "\n",
    "model = LSTM_Autoencoder_RepeatVector(dataset,timesteps,latent_dim,n_epoch,n_batch)\n",
    "encoded_dataset = model.lstm_autoencoder_repeatvector()\n",
    "# generate labels for encoded_dataset\n",
    "e_label = [None]*encoded_dataset.shape[0]\n",
    "for i in range(encoded_dataset.shape[0]):\n",
    "    e_label[i] = label[(i+1)*timesteps-1] # for each batch, use the last sample label as the encoded batch label\n",
    "e_label = np.array(e_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Todo\n",
    "#inverse scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Two SVM classifier for original dataset and encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare both original and encoded data\n",
    "\n",
    "def train_test_split(dataset,label):\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    y_train, y_test = label[0:train_size],label[train_size:len(dataset)]\n",
    "    # label str2int\n",
    "    with open(\"C:/Users/Bin/Documents/Datasets/KDD99/classes.txt\") as f:\n",
    "        line = f.readline()\n",
    "    classes = line.split(\",\")\n",
    "    class_dic = {classes[i]:i for i in range(len(classes))}\n",
    "    y_train_num = [class_dic[x.strip('.')] for x in y_train]\n",
    "    y_test_num = [class_dic[x.strip('.')] for x in y_test]\n",
    "    return train,test,y_train_num,y_test_num\n",
    "xn_train, xn_test, yn_train,yn_test =  train_test_split(dataset,label)\n",
    "xe_train, xe_test, ye_train,ye_test =  train_test_split(encoded_dataset,e_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing SVM classifier with original dataset...\n",
      "time used of trianing on original dataset:  0:01:05.823061 s\n",
      "Trianing SVM classifier with encoded dataset...\n",
      "time used of training on encoded dataset:  0:00:00.118958 s\n"
     ]
    }
   ],
   "source": [
    "# train two svm classifiers\n",
    "from sklearn import svm\n",
    "\n",
    "print(\"Trianing SVM classifier with original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "clf_n = svm.SVC()\n",
    "clf_n.fit(xn_train,yn_train)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of trianing on original dataset: \",(dt2-dt1),\"s\")\n",
    "print(\"Trianing SVM classifier with encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "clf_e = svm.SVC()\n",
    "clf_e.fit(xe_train,ye_train)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of training on encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction using original dataset...\n",
      "time used of prediction using original dataset:  0:01:34.830185 s\n",
      "Making prediction using encoded dataset...\n",
      "time used of prediction using encoded dataset:  0:00:00.084652 s\n"
     ]
    }
   ],
   "source": [
    "# making prediction using both classifier and both dataset\n",
    "\n",
    "#original\n",
    "print(\"Making prediction using original dataset...\")\n",
    "dt1 = datetime.now()\n",
    "prediction_n = clf_n.predict(xn_test)\n",
    "dt2 = datetime.now()\n",
    "print(\"time used of prediction using original dataset: \",(dt2-dt1),\"s\")\n",
    "#encoded\n",
    "print(\"Making prediction using encoded dataset...\")\n",
    "dt3 = datetime.now()\n",
    "prediction_e = clf_e.predict(xe_test)\n",
    "dt4 = datetime.now()\n",
    "print(\"time used of prediction using encoded dataset: \",(dt4-dt3),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on original dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.00000   0.00000   0.00000       100\n",
      "          1    0.00000   0.00000   0.00000        18\n",
      "          3    0.00000   0.00000   0.00000         0\n",
      "          5    0.55000   0.14286   0.22680       385\n",
      "          6    0.50000   0.25000   0.33333         4\n",
      "          7    0.00000   0.00000   0.00000         1\n",
      "          9    0.99982   0.69034   0.81675     66079\n",
      "         10    0.00000   0.00000   0.00000         0\n",
      "         11    0.98389   0.97064   0.97722     26053\n",
      "         12    0.00000   0.00000   0.00000         1\n",
      "         13    0.00000   0.00000   0.00000         1\n",
      "         14    0.00000   0.00000   0.00000       162\n",
      "         15    0.01682   0.87282   0.03301       401\n",
      "         16    0.00000   0.00000   0.00000         3\n",
      "         17    0.00000   0.00000   0.00000         2\n",
      "         18    0.99952   0.99908   0.99930     69235\n",
      "         20    1.00000   0.99141   0.99569       582\n",
      "         21    0.00000   0.00000   0.00000         0\n",
      "\n",
      "avg / total    0.99189   0.86525   0.91578    163027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "# performance using original dataset\n",
    "print(\"Performance on original dataset\\n\")\n",
    "print(classification_report(yn_test, prediction_n,digits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on encoded dataset\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0    0.00000   0.00000   0.00000         3\n",
      "          5    0.00000   0.00000   0.00000        17\n",
      "          9    0.99868   0.68875   0.81525      2204\n",
      "         11    0.93900   0.99538   0.96637       866\n",
      "         14    0.00000   0.00000   0.00000         4\n",
      "         15    1.00000   0.53846   0.70000        13\n",
      "         17    0.00000   0.00000   0.00000         0\n",
      "         18    0.99913   0.99783   0.99848      2308\n",
      "         20    0.00000   0.00000   0.00000        20\n",
      "\n",
      "avg / total    0.98128   0.86293   0.91026      5435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Performance using encoded dataset\n",
    "print(\"Performance on encoded dataset\\n\")\n",
    "print(classification_report(ye_test, prediction_e,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16467, 10)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
