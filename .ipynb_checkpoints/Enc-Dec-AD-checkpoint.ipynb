{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the implementation of the EncDec-AD model based on the paper \"LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection\". And a slight variantion \"TimeNet\" based on the paper \"TimeNet: Pre-trained deep recurrent neural network for time series classiﬁcation\", which has been shown that achives better performance in the time series anomaly detection scenario. The only difference is the second model feeds constants to its decoder as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the EncDec-AD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecAD(object):\n",
    "    def __init__(self,latent_dim,inputs):\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.input_dim = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self.enc_cell = tf.nn.rnn_cell.LSTMCell(latent_dim)\n",
    "        self.dec_cell = tf.nn.rnn_cell.LSTMCell(latent_dim)\n",
    "        \n",
    "        with tf.variable_scope('encoder1'):\n",
    "            self.enc_outputs, self.enc_state = tf.nn.static_rnn(self.enc_cell, inputs, \n",
    "                                                         dtype=tf.float32)\n",
    "        with tf.variable_scope('decoder1') as scope_dec:\n",
    "            w = tf.Variable(tf.truncated_normal([latent_dim,self.input_dim],\n",
    "                                               dtype=tf.float32),name=\"w\")\n",
    "            b = tf.Variable(tf.constant(0.1,shape=[self.input_dim],\n",
    "                                               dtype=tf.float32),name=\"b\")\n",
    "            dec_state = self.enc_state\n",
    "            dec_input = tf.zeros(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "            dec_outputs = []\n",
    "            for time_steps in range(len(inputs)):\n",
    "                if time_steps>0:\n",
    "                    scope_dec.reuse_variables()\n",
    "                dec_input, dec_state = self.dec_cell(dec_input,dec_state)\n",
    "                dec_input = tf.matmul(dec_input,w) + b\n",
    "                dec_outputs.append(dec_input)\n",
    "            dec_outputs = dec_outputs[::-1]\n",
    "            # convert output to the form [batch_size, time_steps, input_dim]\n",
    "            self.output = tf.transpose(tf.stack(dec_outputs),[1,0,2]) \n",
    "        #Calculate loss\n",
    "        self.input = tf.transpose(tf.stack(inputs),[1,0,2])\n",
    "        self.loss = tf.reduce_mean(tf.square(self.input - self.output))\n",
    "        \n",
    "        #Optimization\n",
    "        self.train = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the TimeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeNet(object):\n",
    "    def __init__(self,latent_dim,inputs):\n",
    "        self.batch_num = inputs[0].get_shape().as_list()[0]\n",
    "        self.input_dim = inputs[0].get_shape().as_list()[1]\n",
    "        \n",
    "        self.enc_cell = tf.nn.rnn_cell.LSTMCell(latent_dim)\n",
    "        self.dec_cell = tf.nn.rnn_cell.LSTMCell(latent_dim)\n",
    "        \n",
    "        with tf.variable_scope('encoder6'):\n",
    "            self.enc_outputs, self.enc_state = tf.nn.static_rnn(self.enc_cell, inputs, \n",
    "                                                         dtype=tf.float32)\n",
    "        with tf.variable_scope('decoder6') as scope_dec:\n",
    "            w = tf.Variable(tf.truncated_normal([latent_dim,self.input_dim],\n",
    "                                               dtype=tf.float32),name=\"w\")\n",
    "            b = tf.Variable(tf.constant(0.1,shape=[self.input_dim],\n",
    "                                               dtype=tf.float32),name=\"b\")\n",
    "            \n",
    "            dec_inputs = [tf.zeros(tf.shape(inputs[0]),dtype=tf.float32)\n",
    "                                  for _ in range(len(inputs))]\n",
    "            dec_outputs, dec_state = tf.nn.static_rnn(self.dec_cell, dec_inputs,\n",
    "                                                     initial_state=self.enc_state,\n",
    "                                                     dtype=tf.float32)\n",
    "            dec_outputs = dec_outputs[::-1]\n",
    "            dec_output = tf.transpose(tf.stack(dec_outputs),[1,0,2])\n",
    "            w = tf.tile(tf.expand_dims(w,0),[self.batch_num,1,1])\n",
    "            self.output = tf.matmul(dec_output,w)+b\n",
    "            \n",
    "        #Calculate loss\n",
    "        self.input = tf.transpose(tf.stack(inputs),[1,0,2])\n",
    "        self.loss = tf.reduce_mean(tf.square(self.input - self.output))\n",
    "        \n",
    "        #Optimization\n",
    "        self.train = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data \n",
    "def read():\n",
    "    folder = \"C:/Users/Bin/Documents/Datasets/KDD99/\"\n",
    "    col_name_suffix = \"columns.txt\"\n",
    "    dataset_suffix = \"kddcup.data_10_percent_corrected\"\n",
    "    \n",
    "    with open(folder+col_name_suffix) as col_file:\n",
    "        line = col_file.readline()\n",
    "    columns = line.split('.')\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    for col in columns:\n",
    "        col_names.append(col.split(': ')[0].strip())\n",
    "        col_types.append(col.split(': ')[1])\n",
    "    col_names.append(\"label\")\n",
    "    df = pd.read_csv(folder+dataset_suffix,names=col_names)\n",
    "    data = df.iloc[:,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()   #Select only numeric features\n",
    "    label = df.iloc[:,-1]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    dataset = scaler.transform(data) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = read()\n",
    "\n",
    "time_steps = 10\n",
    "latent_dim = 10\n",
    "n_epoch = 100\n",
    "batch_size = 100\n",
    "input_dim = dataset.shape[1]\n",
    "\n",
    "# reshape the dataset to a list, each element with the shape [batch_size, time_steps, input_dim]\n",
    "size = dataset.shape[0]//(batch_size*time_steps)\n",
    "data_input = np.reshape(dataset[:size*batch_size*time_steps],(size,batch_size,time_steps,input_dim))\n",
    "array = [t for t in data_input]\n",
    "del dataset, data_input\n",
    "\n",
    "# inputs placeholder\n",
    "p_input = tf.placeholder(tf.float32, [batch_size, time_steps, input_dim])\n",
    "p_inputs = [t for t in tf.split(p_input, time_steps, 1)]\n",
    "p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, time_steps, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  0.220792\n",
      "Epoch 2:  0.148115\n",
      "Epoch 3:  0.139054\n",
      "Epoch 4:  0.126811\n",
      "Epoch 5:  0.116594\n",
      "Epoch 6:  0.109886\n",
      "Epoch 7:  0.10469\n",
      "Epoch 8:  0.100545\n",
      "Epoch 9:  0.0970631\n",
      "Epoch 10:  0.0939343\n",
      "Epoch 11:  0.0911488\n",
      "Epoch 12:  0.0887058\n",
      "Epoch 13:  0.0865162\n",
      "Epoch 14:  0.0845019\n",
      "Epoch 15:  0.0826256\n",
      "Epoch 16:  0.0808598\n",
      "Epoch 17:  0.0791878\n",
      "Epoch 18:  0.0776027\n",
      "Epoch 19:  0.0760942\n",
      "Epoch 20:  0.0746492\n",
      "Epoch 21:  0.0732577\n",
      "Epoch 22:  0.0719138\n",
      "Epoch 23:  0.0706126\n",
      "Epoch 24:  0.0693487\n",
      "Epoch 25:  0.0681176\n",
      "Epoch 26:  0.0669148\n",
      "Epoch 27:  0.0657378\n",
      "Epoch 28:  0.0645837\n",
      "Epoch 29:  0.0634507\n",
      "Epoch 30:  0.0623367\n",
      "Epoch 31:  0.0612397\n",
      "Epoch 32:  0.0601579\n",
      "Epoch 33:  0.0590899\n",
      "Epoch 34:  0.0580344\n",
      "Epoch 35:  0.0569903\n",
      "Epoch 36:  0.0559569\n",
      "Epoch 37:  0.0549332\n",
      "Epoch 38:  0.0539187\n",
      "Epoch 39:  0.0529128\n",
      "Epoch 40:  0.051915\n",
      "Epoch 41:  0.0509249\n",
      "Epoch 42:  0.0499422\n",
      "Epoch 43:  0.0489666\n",
      "Epoch 44:  0.047998\n",
      "Epoch 45:  0.0470361\n",
      "Epoch 46:  0.046081\n",
      "Epoch 47:  0.0451324\n",
      "Epoch 48:  0.0441903\n",
      "Epoch 49:  0.0432547\n",
      "Epoch 50:  0.0423256\n",
      "Epoch 51:  0.0414031\n",
      "Epoch 52:  0.0404871\n",
      "Epoch 53:  0.039578\n",
      "Epoch 54:  0.0386756\n",
      "Epoch 55:  0.0377803\n",
      "Epoch 56:  0.0368921\n",
      "Epoch 57:  0.0360114\n",
      "Epoch 58:  0.0351383\n",
      "Epoch 59:  0.034273\n",
      "Epoch 60:  0.033416\n",
      "Epoch 61:  0.0325675\n",
      "Epoch 62:  0.0317279\n",
      "Epoch 63:  0.0308974\n",
      "Epoch 64:  0.0300765\n",
      "Epoch 65:  0.0292657\n",
      "Epoch 66:  0.0284653\n",
      "Epoch 67:  0.0276758\n",
      "Epoch 68:  0.0268978\n",
      "Epoch 69:  0.0261315\n",
      "Epoch 70:  0.0253776\n",
      "Epoch 71:  0.0246366\n",
      "Epoch 72:  0.023909\n",
      "Epoch 73:  0.0231953\n",
      "Epoch 74:  0.022496\n",
      "Epoch 75:  0.0218116\n",
      "Epoch 76:  0.0211427\n",
      "Epoch 77:  0.0204898\n",
      "Epoch 78:  0.0198533\n",
      "Epoch 79:  0.0192337\n",
      "Epoch 80:  0.0186314\n",
      "Epoch 81:  0.0180468\n",
      "Epoch 82:  0.0174803\n",
      "Epoch 83:  0.0169321\n",
      "Epoch 84:  0.0164025\n",
      "Epoch 85:  0.0158916\n",
      "Epoch 86:  0.0153996\n",
      "Epoch 87:  0.0149266\n",
      "Epoch 88:  0.0144726\n",
      "Epoch 89:  0.0140375\n",
      "Epoch 90:  0.0136213\n",
      "Epoch 91:  0.0132237\n",
      "Epoch 92:  0.0128447\n",
      "Epoch 93:  0.0124839\n",
      "Epoch 94:  0.012141\n",
      "Epoch 95:  0.0118157\n",
      "Epoch 96:  0.0115077\n",
      "Epoch 97:  0.0112164\n",
      "Epoch 98:  0.0109415\n",
      "Epoch 99:  0.0106824\n",
      "Epoch 100:  0.0104386\n",
      "Example: \n",
      "embedding:  [[-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]\n",
      " [-0.32527694  0.45770967 -0.69965613  0.41347975  0.08003657 -0.29567188\n",
      "   1.04753804 -0.31709445 -0.10584471  0.06801358]]\n"
     ]
    }
   ],
   "source": [
    "# EncDec-AD model\n",
    "\n",
    "encdecad = EncDecAD(latent_dim, p_inputs)\n",
    "with tf.Session() as sess1:\n",
    "    sess1.run(tf.global_variables_initializer())\n",
    "    batches = len(array)//batch_size\n",
    "    for i in range(n_epoch):\n",
    "        for j in range(batches):\n",
    "            loss_val, _ = sess1.run([encdecad.loss,encdecad.train],{p_input:array[j]})\n",
    "        print(\"Epoch %d: \" % (i+1),loss_val)\n",
    "    \n",
    "    #example\n",
    "    input_, output_, encoded_, enc_state = sess1.run([encdecad.input, encdecad.output, \n",
    "                                                    encdecad.enc_outputs, encdecad.enc_state], {p_input:array[101]})\n",
    "    embedding = enc_state[0]\n",
    "    print(\"Example: \")\n",
    "    print(\"embedding: \",embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  0.110341\n",
      "Epoch 2:  0.0986995\n",
      "Epoch 3:  0.0903961\n",
      "Epoch 4:  0.0841947\n",
      "Epoch 5:  0.0793006\n",
      "Epoch 6:  0.075198\n",
      "Epoch 7:  0.0715471\n",
      "Epoch 8:  0.0681798\n",
      "Epoch 9:  0.0650505\n",
      "Epoch 10:  0.0621422\n",
      "Epoch 11:  0.0594265\n",
      "Epoch 12:  0.0568731\n",
      "Epoch 13:  0.05446\n",
      "Epoch 14:  0.052171\n",
      "Epoch 15:  0.0499934\n",
      "Epoch 16:  0.0479174\n",
      "Epoch 17:  0.0459354\n",
      "Epoch 18:  0.0440397\n",
      "Epoch 19:  0.0422223\n",
      "Epoch 20:  0.0404768\n",
      "Epoch 21:  0.0387987\n",
      "Epoch 22:  0.0371844\n",
      "Epoch 23:  0.035631\n",
      "Epoch 24:  0.034135\n",
      "Epoch 25:  0.032694\n",
      "Epoch 26:  0.0313058\n",
      "Epoch 27:  0.0299687\n",
      "Epoch 28:  0.0286814\n",
      "Epoch 29:  0.0274427\n",
      "Epoch 30:  0.0262516\n",
      "Epoch 31:  0.0251072\n",
      "Epoch 32:  0.0240086\n",
      "Epoch 33:  0.0229551\n",
      "Epoch 34:  0.0219461\n",
      "Epoch 35:  0.0209808\n",
      "Epoch 36:  0.0200585\n",
      "Epoch 37:  0.0191785\n",
      "Epoch 38:  0.01834\n",
      "Epoch 39:  0.0175424\n",
      "Epoch 40:  0.016785\n",
      "Epoch 41:  0.016067\n",
      "Epoch 42:  0.015388\n",
      "Epoch 43:  0.0147471\n",
      "Epoch 44:  0.0141437\n",
      "Epoch 45:  0.0135772\n",
      "Epoch 46:  0.0130467\n",
      "Epoch 47:  0.0125515\n",
      "Epoch 48:  0.0120908\n",
      "Epoch 49:  0.0116635\n",
      "Epoch 50:  0.0112686\n",
      "Epoch 51:  0.010905\n",
      "Epoch 52:  0.0105715\n",
      "Epoch 53:  0.0102667\n",
      "Epoch 54:  0.0099892\n",
      "Epoch 55:  0.00973744\n",
      "Epoch 56:  0.00950987\n",
      "Epoch 57:  0.00930488\n",
      "Epoch 58:  0.00912084\n",
      "Epoch 59:  0.00895607\n",
      "Epoch 60:  0.00880893\n",
      "Epoch 61:  0.00867782\n",
      "Epoch 62:  0.00856118\n",
      "Epoch 63:  0.00845754\n",
      "Epoch 64:  0.00836548\n",
      "Epoch 65:  0.00828371\n",
      "Epoch 66:  0.00821098\n",
      "Epoch 67:  0.00814623\n",
      "Epoch 68:  0.00808842\n",
      "Epoch 69:  0.00803665\n",
      "Epoch 70:  0.00799013\n",
      "Epoch 71:  0.00794814\n",
      "Epoch 72:  0.00791006\n",
      "Epoch 73:  0.00787536\n",
      "Epoch 74:  0.00784354\n",
      "Epoch 75:  0.00781424\n",
      "Epoch 76:  0.00778704\n",
      "Epoch 77:  0.00776169\n",
      "Epoch 78:  0.00773795\n",
      "Epoch 79:  0.00771556\n",
      "Epoch 80:  0.00769436\n",
      "Epoch 81:  0.00767419\n",
      "Epoch 82:  0.00765491\n",
      "Epoch 83:  0.00763639\n",
      "Epoch 84:  0.00761858\n",
      "Epoch 85:  0.00760135\n",
      "Epoch 86:  0.00758466\n",
      "Epoch 87:  0.00756843\n",
      "Epoch 88:  0.00755263\n",
      "Epoch 89:  0.00753723\n",
      "Epoch 90:  0.00752213\n",
      "Epoch 91:  0.00750737\n",
      "Epoch 92:  0.00749288\n",
      "Epoch 93:  0.00747866\n",
      "Epoch 94:  0.00746466\n",
      "Epoch 95:  0.00745086\n",
      "Epoch 96:  0.0074373\n",
      "Epoch 97:  0.00742389\n",
      "Epoch 98:  0.00741068\n",
      "Epoch 99:  0.00739761\n",
      "Epoch 100:  0.00738471\n"
     ]
    }
   ],
   "source": [
    "# TimeNet\n",
    "timenet = TimeNet(latent_dim, p_inputs)\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    batches = len(array)//batch_size\n",
    "    for i in range(n_epoch):\n",
    "        for j in range(batches):\n",
    "            loss_val, _ = sess2.run([timenet.loss,timenet.train],{p_input:array[j]})\n",
    "        print(\"Epoch %d: \" % (i+1),loss_val)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
