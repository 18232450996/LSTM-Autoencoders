{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Bin/Documents/Datasets/KDD99/columns.txt\") as col_file:\n",
    "    line = col_file.readline()\n",
    "\n",
    "columns = line.split('.')\n",
    "col_names = []\n",
    "col_types = []\n",
    "for col in columns:\n",
    "    col_names.append(col.split(': ')[0].strip())\n",
    "    col_types.append(col.split(': ')[1])\n",
    "col_names.append(\"label\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Bin/Documents/Datasets/KDD99/kddcup.data_10_percent_corrected\",names=col_names)\n",
    "\n",
    "data = df.iloc[:5000,np.array(pd.Series(col_types)==\"continuous\")].as_matrix()\n",
    "label = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "dataset = scaler.transform(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 34)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "latent_dim = 10\n",
    "n_epoch = 100  #num_steps\n",
    "n_batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data.shape[0])\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len,:],\n",
    "                      [batch_size, batch_len,raw_data.shape[1]])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps,:]\n",
    "    x.set_shape([batch_size, num_steps,raw_data.shape[1]])\n",
    "    #y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "    #y.set_shape([batch_size, num_steps])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "    inputs =     \n",
    "    if is_training and dropout < 1:\n",
    "    inputs = tf.nn.dropout(inputs, dropout)\n",
    "    \n",
    "    # set up the state storage / extraction\n",
    "    self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "    \n",
    "    state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "    rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    # create an LSTM cell to be unrolled\n",
    "    cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "    # add a dropout wrapper if training\n",
    "    if is_training and dropout < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "    if num_layers > 1:\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True) #expect the state variables in the form of aÂ LSTMStateTuple \n",
    "   \n",
    "    output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n",
    "    # output size:  (batch_size, num_steps, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_lr(self, session, lr_value):\n",
    "    session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "    \n",
    "def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93):\n",
    "    # setup data and models\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "        m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "        current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "        for step in range(training_input.epoch_size):\n",
    "            if step % 50 != 0:\n",
    "                cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "                                                                 feed_dict={m.init_state: current_state})\n",
    "            else:\n",
    "                cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "                                                          feed_dict={m.init_state: current_state})\n",
    "                print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}\".format(epoch, step, cost, acc))\n",
    "        # save a model checkpoint\n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "    # do a final save\n",
    "    saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "    # close threads\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] run_opt\n",
      "ipykernel_launcher.py: error: argument run_opt: invalid int value: 'C:\\\\Users\\\\Bin\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-78a79ad2-fa2d-482c-a768-b46d26cbc945.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"To run this code, you'll need to first download and extract the text dataset\n",
    "\n",
    "    from here: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz. Change the\n",
    "\n",
    "    data_path variable below to your local exraction path\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"C:/Users/Bin/Desktop/exp/simple-examples/data\"\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('run_opt', type=int, default=1, help='An integer: 1 to train, 2 to test')\n",
    "\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='The full path of the training data')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "def read_words(filename):\n",
    "\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "\n",
    "    data = read_words(filename)\n",
    "\n",
    "\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "\n",
    "    data = read_words(filename)\n",
    "\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    # get the data paths\n",
    "\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "\n",
    "    word_to_id = build_vocab(train_path)\n",
    "\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "\n",
    "    vocabulary = len(word_to_id)\n",
    "\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    print(train_data[:5])\n",
    "\n",
    "    print(word_to_id)\n",
    "\n",
    "    print(vocabulary)\n",
    "\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len],\n",
    "\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps]\n",
    "\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the main model\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.input_obj = input\n",
    "\n",
    "        self.batch_size = input.batch_size\n",
    "\n",
    "        self.num_steps = input.num_steps\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "\n",
    "        # create the word embeddings\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "\n",
    "\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "\n",
    "\n",
    "\n",
    "        # set up the state storage / extraction\n",
    "\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "\n",
    "\n",
    "\n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "\n",
    "        rnn_tuple_state = tuple(\n",
    "\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "\n",
    "             for idx in range(num_layers)]\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # create an LSTM cell to be unrolled\n",
    "\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "\n",
    "        # add a dropout wrapper if training\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "        if num_layers > 1:\n",
    "\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n",
    "\n",
    "\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n",
    "\n",
    "        # reshape to (batch_size * num_steps, hidden_size)\n",
    "\n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "\n",
    "            logits,\n",
    "\n",
    "            self.input_obj.targets,\n",
    "\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "\n",
    "            average_across_timesteps=False,\n",
    "\n",
    "            average_across_batch=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Update the cost\n",
    "\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "\n",
    "\n",
    "        # get the prediction accuracy\n",
    "\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "        if not is_training:\n",
    "\n",
    "           return\n",
    "\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "\n",
    "            zip(grads, tvars),\n",
    "\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "\n",
    "\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,\n",
    "\n",
    "          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n",
    "\n",
    "    # setup data and models\n",
    "\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "\n",
    "    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "\n",
    "              num_layers=num_layers)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    orig_decay = lr_decay\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # start threads\n",
    "\n",
    "        sess.run([init_op])\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "\n",
    "            # m.assign_lr(sess, learning_rate)\n",
    "\n",
    "            # print(m.learning_rate.eval(), new_lr_decay)\n",
    "\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "\n",
    "            curr_time = dt.datetime.now()\n",
    "\n",
    "            for step in range(training_input.epoch_size):\n",
    "\n",
    "                # cost, _ = sess.run([m.cost, m.optimizer])\n",
    "\n",
    "                if step % print_iter != 0:\n",
    "\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "\n",
    "                                                      feed_dict={m.init_state: current_state})\n",
    "\n",
    "                else:\n",
    "\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "\n",
    "                    curr_time = dt.datetime.now()\n",
    "\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "\n",
    "                                                           feed_dict={m.init_state: current_state})\n",
    "\n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch,\n",
    "\n",
    "                            step, cost, acc, seconds))\n",
    "\n",
    "\n",
    "\n",
    "            # save a model checkpoint\n",
    "\n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "\n",
    "        # do a final save\n",
    "\n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "\n",
    "        # close threads\n",
    "\n",
    "        coord.request_stop()\n",
    "\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model_path, test_data, reversed_dictionary):\n",
    "\n",
    "    test_input = Input(batch_size=20, num_steps=35, data=test_data)\n",
    "\n",
    "    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,\n",
    "\n",
    "              num_layers=2)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # start threads\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))\n",
    "\n",
    "        # restore the trained model\n",
    "\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        # get an average accuracy over num_acc_batches\n",
    "\n",
    "        num_acc_batches = 30\n",
    "\n",
    "        check_batch_idx = 25\n",
    "\n",
    "        acc_check_thresh = 5\n",
    "\n",
    "        accuracy = 0\n",
    "\n",
    "        for batch in range(num_acc_batches):\n",
    "\n",
    "            if batch == check_batch_idx:\n",
    "\n",
    "                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],\n",
    "\n",
    "                                                               feed_dict={m.init_state: current_state})\n",
    "\n",
    "                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]\n",
    "\n",
    "                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]\n",
    "\n",
    "                print(\"True values (1st line) vs predicted values (2nd line):\")\n",
    "\n",
    "                print(\" \".join(true_vals_string))\n",
    "\n",
    "                print(\" \".join(pred_string))\n",
    "\n",
    "            else:\n",
    "\n",
    "                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})\n",
    "\n",
    "            if batch >= acc_check_thresh:\n",
    "\n",
    "                accuracy += acc\n",
    "\n",
    "        print(\"Average accuracy: {:.3f}\".format(accuracy / (num_acc_batches-acc_check_thresh)))\n",
    "\n",
    "        # close threads\n",
    "\n",
    "        coord.request_stop()\n",
    "\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if args.data_path:\n",
    "\n",
    "    data_path = args.data_path\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "\n",
    "if args.run_opt == 1:\n",
    "\n",
    "    train(train_data, vocabulary, num_layers=2, num_epochs=60, batch_size=20,\n",
    "\n",
    "          model_save_name='two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr')\n",
    "\n",
    "else:\n",
    "\n",
    "    trained_model = args.data_path + \"\\\\two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr-38\"\n",
    "\n",
    "    test(trained_model, test_data, reversed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
